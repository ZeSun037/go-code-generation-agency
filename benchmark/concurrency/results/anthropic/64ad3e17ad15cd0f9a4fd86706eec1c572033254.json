{
    "task": "64ad3e17ad15cd0f9a4fd86706eec1c572033254.txt",
    "provider": "anthropic",
    "model": "claude-haiku-4-5",
    "rounds": [
        {
            "code": "package main\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype TaskStatus struct {\n\tName     string\n\tStatus   string\n\tMetadata map[string]interface{}\n\tUpdatedAt time.Time\n}\n\ntype StatusUpdate struct {\n\tName   string\n\tStatus string\n}\n\ntype TaskManager struct {\n\ttasks          map[string]*TaskStatus\n\ttasksMutex     sync.RWMutex\n\tstatusChannel  chan StatusUpdate\n\tdone           chan struct{}\n\twg             sync.WaitGroup\n\tsyncedCount    int\n\tfailedCount    int\n\tstatsLock      sync.Mutex\n}\n\nfunc NewTaskManager(channelCapacity int) *TaskManager {\n\ttm := &TaskManager{\n\t\ttasks:         make(map[string]*TaskStatus),\n\t\tstatusChannel: make(chan StatusUpdate, channelCapacity),\n\t\tdone:          make(chan struct{}),\n\t\tsyncedCount:   0,\n\t\tfailedCount:   0,\n\t}\n\treturn tm\n}\n\nfunc (tm *TaskManager) Start() {\n\ttm.wg.Add(1)\n\tgo tm.syncWorker()\n}\n\nfunc (tm *TaskManager) SetTaskStatus(taskName string, newStatus string) {\n\ttm.tasksMutex.Lock()\n\t\n\texistingTask, exists := tm.tasks[taskName]\n\tif exists && existingTask.Status == newStatus {\n\t\ttm.tasksMutex.Unlock()\n\t\treturn\n\t}\n\t\n\ttask := &TaskStatus{\n\t\tName:      taskName,\n\t\tStatus:    newStatus,\n\t\tMetadata:  make(map[string]interface{}),\n\t\tUpdatedAt: time.Now(),\n\t}\n\t\n\ttm.tasks[taskName] = task\n\ttm.tasksMutex.Unlock()\n\t\n\tselect {\n\tcase tm.statusChannel <- StatusUpdate{Name: taskName, Status: newStatus}:\n\tcase <-tm.done:\n\t}\n}\n\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.tasksMutex.Lock()\n\tdelete(tm.tasks, taskName)\n\ttm.tasksMutex.Unlock()\n}\n\nfunc (tm *TaskManager) SyncBatch() {\n\tbatch := make([]StatusUpdate, 0, 5)\n\ttimeout := time.After(100 * time.Millisecond)\n\t\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusChannel:\n\t\t\tbatch = append(batch, update)\n\t\t\tif len(batch) >= 5 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-timeout:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\tcase <-tm.done:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) processBatch(batch []StatusUpdate) {\n\tsyncFailed := false\n\tfailureThreshold := 0.2\n\t\n\tif rand.Float64() < failureThreshold {\n\t\tsyncFailed = true\n\t}\n\t\n\tif syncFailed {\n\t\ttm.statsLock.Lock()\n\t\ttm.failedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\t\t\n\t\tfmt.Printf(\"[SYNC FAILED] Failed to sync %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t\ttm.DeleteTaskStatus(update.Name)\n\t\t}\n\t\tfmt.Println()\n\t} else {\n\t\ttm.statsLock.Lock()\n\t\ttm.syncedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\t\t\n\t\tfmt.Printf(\"[SYNC SUCCESS] Synced %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t}\n\t\tfmt.Println()\n\t}\n}\n\nfunc (tm *TaskManager) syncWorker() {\n\tdefer tm.wg.Done()\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-tm.done:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusChannel:\n\t\t\t\t\tbatch := []StatusUpdate{update}\n\t\t\t\t\ttm.processBatch(batch)\n\t\t\t\tdefault:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\ttm.SyncBatch()\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) GetStats() (synced int, failed int) {\n\ttm.statsLock.Lock()\n\tdefer tm.statsLock.Unlock()\n\treturn tm.syncedCount, tm.failedCount\n}\n\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (*TaskStatus, bool) {\n\ttm.tasksMutex.RLock()\n\tdefer tm.tasksMutex.RUnlock()\n\ttask, exists := tm.tasks[taskName]\n\treturn task, exists\n}\n\nfunc (tm *TaskManager) Shutdown() {\n\tclose(tm.done)\n\ttm.wg.Wait()\n}\n\nfunc worker(tm *TaskManager, workerID int, duration time.Duration) {\n\ttaskNames := []string{\n\t\tfmt.Sprintf(\"task_%d_1\", workerID),\n\t\tfmt.Sprintf(\"task_%d_2\", workerID),\n\t\tfmt.Sprintf(\"task_%d_3\", workerID),\n\t\tfmt.Sprintf(\"task_%d_4\", workerID),\n\t\tfmt.Sprintf(\"task_%d_5\", workerID),\n\t}\n\t\n\tstatuses := []string{\"pending\", \"running\", \"completed\"}\n\t\n\tticker := time.NewTicker(time.Duration(100+rand.Intn(200)) * time.Millisecond)\n\tdefer ticker.Stop()\n\t\n\ttimeout := time.After(duration)\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-timeout:\n\t\t\tfmt.Printf(\"Worker %d finished\\n\", workerID)\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\ttaskName := taskNames[rand.Intn(len(taskNames))]\n\t\t\tstatus := statuses[rand.Intn(len(statuses))]\n\t\t\ttm.SetTaskStatus(taskName, status)\n\t\t}\n\t}\n}\n\nfunc statsReporter(tm *TaskManager, ticker *time.Ticker) {\n\tfor range ticker.C {\n\t\tsynced, failed := tm.GetStats()\n\t\ttotal := synced + failed\n\t\tif total > 0 {\n\t\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\t\tfmt.Printf(\"[STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\t\ttotal, synced, failed, successRate)\n\t\t}\n\t}\n}\n\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\t\n\ttm := NewTaskManager(5)\n\ttm.Start()\n\t\n\tduration := 10 * time.Second\n\tnumWorkers := 10\n\t\n\tstatsTicker := time.NewTicker(2 * time.Second)\n\tdefer statsTicker.Stop()\n\t\n\tgo statsReporter(tm, statsTicker)\n\t\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\tworker(tm, id, duration)\n\t\t}(i)\n\t}\n\t\n\twg.Wait()\n\t\n\ttime.Sleep(1 * time.Second)\n\t\n\ttm.Shutdown()\n\t\n\tsynced, failed := tm.GetStats()\n\ttotal := synced + failed\n\tif total > 0 {\n\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\tfmt.Printf(\"\\n[FINAL STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\ttotal, synced, failed, successRate)\n\t}\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:200\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    199: \t\t\ttaskName := taskNames[rand.Intn(len(taskNames))]\n  > 200: \t\t\tstatus := statuses[rand.Intn(len(statuses))]\n    201: \t\t\ttm.SetTaskStatus(taskName, status)\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:199\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    198: \t\tcase <-ticker.C:\n  > 199: \t\t\ttaskName := taskNames[rand.Intn(len(taskNames))]\n    200: \t\t\tstatus := statuses[rand.Intn(len(statuses))]\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:188\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    187: \t\n  > 188: \tticker := time.NewTicker(time.Duration(100+rand.Intn(200)) * time.Millisecond)\n    189: \tdefer ticker.Stop()\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:110\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    109: \t\n  > 110: \tif rand.Float64() < failureThreshold {\n    111: \t\tsyncFailed = true\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 254\n  Nosec  : 0\n  Issues : \u001b[1;31m4\u001b[0m\n\n[gosec] 2025/12/01 13:49:34 Including rules: default\n[gosec] 2025/12/01 13:49:34 Excluding rules: default\n[gosec] 2025/12/01 13:49:34 Including analyzers: default\n[gosec] 2025/12/01 13:49:34 Excluding analyzers: default\n[gosec] 2025/12/01 13:49:34 Import directory: /tmp/go_synthesis_0icqfxj1\n[gosec] 2025/12/01 13:49:34 Checking package: main\n[gosec] 2025/12/01 13:49:34 Checking file: /tmp/go_synthesis_0icqfxj1/main.go"
                },
                {
                    "verifier": "staticcheck",
                    "error": "main.go:219:2: rand.Seed has been deprecated since Go 1.20 and an alternative has been available since Go 1.0: As of Go 1.20 there is no reason to call Seed with a random value. Programs that call Seed with a known value to get a specific sequence of results should use New(NewSource(seed)) to obtain a local random generator. (SA1019)"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype TaskStatus struct {\n\tName      string\n\tStatus    string\n\tMetadata  map[string]interface{}\n\tUpdatedAt time.Time\n}\n\ntype StatusUpdate struct {\n\tName   string\n\tStatus string\n}\n\ntype TaskManager struct {\n\ttasks         map[string]*TaskStatus\n\ttasksMutex    sync.RWMutex\n\tstatusChannel chan StatusUpdate\n\tdone          chan struct{}\n\twg            sync.WaitGroup\n\tsyncedCount   int\n\tfailedCount   int\n\tstatsLock     sync.Mutex\n\trng           *rand.Rand\n}\n\nfunc NewTaskManager(channelCapacity int) *TaskManager {\n\ttm := &TaskManager{\n\t\ttasks:         make(map[string]*TaskStatus),\n\t\tstatusChannel: make(chan StatusUpdate, channelCapacity),\n\t\tdone:          make(chan struct{}),\n\t\tsyncedCount:   0,\n\t\tfailedCount:   0,\n\t\trng:           rand.New(rand.NewPCG(uint64(time.Now().UnixNano()), uint64(time.Now().UnixNano()+1))),\n\t}\n\treturn tm\n}\n\nfunc (tm *TaskManager) Start() {\n\ttm.wg.Add(1)\n\tgo tm.syncWorker()\n}\n\nfunc (tm *TaskManager) SetTaskStatus(taskName string, newStatus string) {\n\ttm.tasksMutex.Lock()\n\n\texistingTask, exists := tm.tasks[taskName]\n\tif exists && existingTask.Status == newStatus {\n\t\ttm.tasksMutex.Unlock()\n\t\treturn\n\t}\n\n\ttask := &TaskStatus{\n\t\tName:      taskName,\n\t\tStatus:    newStatus,\n\t\tMetadata:  make(map[string]interface{}),\n\t\tUpdatedAt: time.Now(),\n\t}\n\n\ttm.tasks[taskName] = task\n\ttm.tasksMutex.Unlock()\n\n\tselect {\n\tcase tm.statusChannel <- StatusUpdate{Name: taskName, Status: newStatus}:\n\tcase <-tm.done:\n\t}\n}\n\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.tasksMutex.Lock()\n\tdelete(tm.tasks, taskName)\n\ttm.tasksMutex.Unlock()\n}\n\nfunc (tm *TaskManager) SyncBatch() {\n\tbatch := make([]StatusUpdate, 0, 5)\n\ttimeout := time.After(100 * time.Millisecond)\n\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusChannel:\n\t\t\tbatch = append(batch, update)\n\t\t\tif len(batch) >= 5 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-timeout:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\tcase <-tm.done:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) processBatch(batch []StatusUpdate) {\n\tsyncFailed := false\n\tfailureThreshold := 0.2\n\n\tif tm.rng.Float64() < failureThreshold {\n\t\tsyncFailed = true\n\t}\n\n\tif syncFailed {\n\t\ttm.statsLock.Lock()\n\t\ttm.failedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\n\t\tfmt.Printf(\"[SYNC FAILED] Failed to sync %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t\ttm.DeleteTaskStatus(update.Name)\n\t\t}\n\t\tfmt.Println()\n\t} else {\n\t\ttm.statsLock.Lock()\n\t\ttm.syncedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\n\t\tfmt.Printf(\"[SYNC SUCCESS] Synced %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t}\n\t\tfmt.Println()\n\t}\n}\n\nfunc (tm *TaskManager) syncWorker() {\n\tdefer tm.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-tm.done:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusChannel:\n\t\t\t\t\tbatch := []StatusUpdate{update}\n\t\t\t\t\ttm.processBatch(batch)\n\t\t\t\tdefault:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\ttm.SyncBatch()\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) GetStats() (synced int, failed int) {\n\ttm.statsLock.Lock()\n\tdefer tm.statsLock.Unlock()\n\treturn tm.syncedCount, tm.failedCount\n}\n\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (*TaskStatus, bool) {\n\ttm.tasksMutex.RLock()\n\tdefer tm.tasksMutex.RUnlock()\n\ttask, exists := tm.tasks[taskName]\n\treturn task, exists\n}\n\nfunc (tm *TaskManager) Shutdown() {\n\tclose(tm.done)\n\ttm.wg.Wait()\n}\n\nfunc worker(tm *TaskManager, workerID int, duration time.Duration) {\n\ttaskNames := []string{\n\t\tfmt.Sprintf(\"task_%d_1\", workerID),\n\t\tfmt.Sprintf(\"task_%d_2\", workerID),\n\t\tfmt.Sprintf(\"task_%d_3\", workerID),\n\t\tfmt.Sprintf(\"task_%d_4\", workerID),\n\t\tfmt.Sprintf(\"task_%d_5\", workerID),\n\t}\n\n\tstatuses := []string{\"pending\", \"running\", \"completed\"}\n\n\tticker := time.NewTicker(time.Duration(100+tm.rng.IntN(200)) * time.Millisecond)\n\tdefer ticker.Stop()\n\n\ttimeout := time.After(duration)\n\n\tfor {\n\t\tselect {\n\t\tcase <-timeout:\n\t\t\tfmt.Printf(\"Worker %d finished\\n\", workerID)\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\ttaskName := taskNames[tm.rng.IntN(len(taskNames))]\n\t\t\tstatus := statuses[tm.rng.IntN(len(statuses))]\n\t\t\ttm.SetTaskStatus(taskName, status)\n\t\t}\n\t}\n}\n\nfunc statsReporter(tm *TaskManager, ticker *time.Ticker) {\n\tfor range ticker.C {\n\t\tsynced, failed := tm.GetStats()\n\t\ttotal := synced + failed\n\t\tif total > 0 {\n\t\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\t\tfmt.Printf(\"[STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\t\ttotal, synced, failed, successRate)\n\t\t}\n\t}\n}\n\nfunc main() {\n\ttm := NewTaskManager(5)\n\ttm.Start()\n\n\tduration := 10 * time.Second\n\tnumWorkers := 10\n\n\tstatsTicker := time.NewTicker(2 * time.Second)\n\tdefer statsTicker.Stop()\n\n\tgo statsReporter(tm, statsTicker)\n\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\tworker(tm, id, duration)\n\t\t}(i)\n\t}\n\n\twg.Wait()\n\n\ttime.Sleep(1 * time.Second)\n\n\ttm.Shutdown()\n\n\tsynced, failed := tm.GetStats()\n\ttotal := synced + failed\n\tif total > 0 {\n\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\tfmt.Printf(\"\\n[FINAL STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\ttotal, synced, failed, successRate)\n\t}\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:41\u001b[0m] - G115 (CWE-190): integer overflow conversion int64 -> uint64 (Confidence: MEDIUM, Severity: HIGH)\n    40: \t\tfailedCount:   0,\n  > 41: \t\trng:           rand.New(rand.NewPCG(uint64(time.Now().UnixNano()), uint64(time.Now().UnixNano()+1))),\n    42: \t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:41\u001b[0m] - G115 (CWE-190): integer overflow conversion int64 -> uint64 (Confidence: MEDIUM, Severity: HIGH)\n    40: \t\tfailedCount:   0,\n  > 41: \t\trng:           rand.New(rand.NewPCG(uint64(time.Now().UnixNano()), uint64(time.Now().UnixNano()+1))),\n    42: \t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:41\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    40: \t\tfailedCount:   0,\n  > 41: \t\trng:           rand.New(rand.NewPCG(uint64(time.Now().UnixNano()), uint64(time.Now().UnixNano()+1))),\n    42: \t}\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 254\n  Nosec  : 0\n  Issues : \u001b[1;31m3\u001b[0m\n\n[gosec] 2025/12/01 13:49:46 Including rules: default\n[gosec] 2025/12/01 13:49:46 Excluding rules: default\n[gosec] 2025/12/01 13:49:46 Including analyzers: default\n[gosec] 2025/12/01 13:49:46 Excluding analyzers: default\n[gosec] 2025/12/01 13:49:46 Import directory: /tmp/go_synthesis_0icqfxj1\n[gosec] 2025/12/01 13:49:46 Checking package: main\n[gosec] 2025/12/01 13:49:46 Checking file: /tmp/go_synthesis_0icqfxj1/main.go"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"fmt\"\n\t\"math/rand/v2\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype TaskStatus struct {\n\tName      string\n\tStatus    string\n\tMetadata  map[string]interface{}\n\tUpdatedAt time.Time\n}\n\ntype StatusUpdate struct {\n\tName   string\n\tStatus string\n}\n\ntype TaskManager struct {\n\ttasks         map[string]*TaskStatus\n\ttasksMutex    sync.RWMutex\n\tstatusChannel chan StatusUpdate\n\tdone          chan struct{}\n\twg            sync.WaitGroup\n\tsyncedCount   int\n\tfailedCount   int\n\tstatsLock     sync.Mutex\n\trng           *rand.Rand\n}\n\nfunc NewTaskManager(channelCapacity int) *TaskManager {\n\tsource := rand.NewPCG(1, 2)\n\ttm := &TaskManager{\n\t\ttasks:         make(map[string]*TaskStatus),\n\t\tstatusChannel: make(chan StatusUpdate, channelCapacity),\n\t\tdone:          make(chan struct{}),\n\t\tsyncedCount:   0,\n\t\tfailedCount:   0,\n\t\trng:           rand.New(source),\n\t}\n\treturn tm\n}\n\nfunc (tm *TaskManager) Start() {\n\ttm.wg.Add(1)\n\tgo tm.syncWorker()\n}\n\nfunc (tm *TaskManager) SetTaskStatus(taskName string, newStatus string) {\n\ttm.tasksMutex.Lock()\n\n\texistingTask, exists := tm.tasks[taskName]\n\tif exists && existingTask.Status == newStatus {\n\t\ttm.tasksMutex.Unlock()\n\t\treturn\n\t}\n\n\ttask := &TaskStatus{\n\t\tName:      taskName,\n\t\tStatus:    newStatus,\n\t\tMetadata:  make(map[string]interface{}),\n\t\tUpdatedAt: time.Now(),\n\t}\n\n\ttm.tasks[taskName] = task\n\ttm.tasksMutex.Unlock()\n\n\tselect {\n\tcase tm.statusChannel <- StatusUpdate{Name: taskName, Status: newStatus}:\n\tcase <-tm.done:\n\t}\n}\n\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.tasksMutex.Lock()\n\tdelete(tm.tasks, taskName)\n\ttm.tasksMutex.Unlock()\n}\n\nfunc (tm *TaskManager) SyncBatch() {\n\tbatch := make([]StatusUpdate, 0, 5)\n\ttimeout := time.After(100 * time.Millisecond)\n\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusChannel:\n\t\t\tbatch = append(batch, update)\n\t\t\tif len(batch) >= 5 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-timeout:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\tcase <-tm.done:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) processBatch(batch []StatusUpdate) {\n\tsyncFailed := false\n\tfailureThreshold := 0.2\n\n\tif tm.rng.Float64() < failureThreshold {\n\t\tsyncFailed = true\n\t}\n\n\tif syncFailed {\n\t\ttm.statsLock.Lock()\n\t\ttm.failedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\n\t\tfmt.Printf(\"[SYNC FAILED] Failed to sync %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t\ttm.DeleteTaskStatus(update.Name)\n\t\t}\n\t\tfmt.Println()\n\t} else {\n\t\ttm.statsLock.Lock()\n\t\ttm.syncedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\n\t\tfmt.Printf(\"[SYNC SUCCESS] Synced %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t}\n\t\tfmt.Println()\n\t}\n}\n\nfunc (tm *TaskManager) syncWorker() {\n\tdefer tm.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-tm.done:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusChannel:\n\t\t\t\t\tbatch := []StatusUpdate{update}\n\t\t\t\t\ttm.processBatch(batch)\n\t\t\t\tdefault:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\ttm.SyncBatch()\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) GetStats() (synced int, failed int) {\n\ttm.statsLock.Lock()\n\tdefer tm.statsLock.Unlock()\n\treturn tm.syncedCount, tm.failedCount\n}\n\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (*TaskStatus, bool) {\n\ttm.tasksMutex.RLock()\n\tdefer tm.tasksMutex.RUnlock()\n\ttask, exists := tm.tasks[taskName]\n\treturn task, exists\n}\n\nfunc (tm *TaskManager) Shutdown() {\n\tclose(tm.done)\n\ttm.wg.Wait()\n}\n\nfunc worker(tm *TaskManager, workerID int, duration time.Duration) {\n\ttaskNames := []string{\n\t\tfmt.Sprintf(\"task_%d_1\", workerID),\n\t\tfmt.Sprintf(\"task_%d_2\", workerID),\n\t\tfmt.Sprintf(\"task_%d_3\", workerID),\n\t\tfmt.Sprintf(\"task_%d_4\", workerID),\n\t\tfmt.Sprintf(\"task_%d_5\", workerID),\n\t}\n\n\tstatuses := []string{\"pending\", \"running\", \"completed\"}\n\n\tticker := time.NewTicker(time.Duration(100+tm.rng.IntN(200)) * time.Millisecond)\n\tdefer ticker.Stop()\n\n\ttimeout := time.After(duration)\n\n\tfor {\n\t\tselect {\n\t\tcase <-timeout:\n\t\t\tfmt.Printf(\"Worker %d finished\\n\", workerID)\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\ttaskName := taskNames[tm.rng.IntN(len(taskNames))]\n\t\t\tstatus := statuses[tm.rng.IntN(len(statuses))]\n\t\t\ttm.SetTaskStatus(taskName, status)\n\t\t}\n\t}\n}\n\nfunc statsReporter(tm *TaskManager, ticker *time.Ticker) {\n\tfor range ticker.C {\n\t\tsynced, failed := tm.GetStats()\n\t\ttotal := synced + failed\n\t\tif total > 0 {\n\t\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\t\tfmt.Printf(\"[STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\t\ttotal, synced, failed, successRate)\n\t\t}\n\t}\n}\n\nfunc main() {\n\ttm := NewTaskManager(5)\n\ttm.Start()\n\n\tduration := 10 * time.Second\n\tnumWorkers := 10\n\n\tstatsTicker := time.NewTicker(2 * time.Second)\n\tdefer statsTicker.Stop()\n\n\tgo statsReporter(tm, statsTicker)\n\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\tworker(tm, id, duration)\n\t\t}(i)\n\t}\n\n\twg.Wait()\n\n\ttime.Sleep(1 * time.Second)\n\n\ttm.Shutdown()\n\n\tsynced, failed := tm.GetStats()\n\ttotal := synced + failed\n\tif total > 0 {\n\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\tfmt.Printf(\"\\n[FINAL STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\ttotal, synced, failed, successRate)\n\t}\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_0icqfxj1/main.go:42\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    41: \t\tfailedCount:   0,\n  > 42: \t\trng:           rand.New(source),\n    43: \t}\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 255\n  Nosec  : 0\n  Issues : \u001b[1;31m1\u001b[0m\n\n[gosec] 2025/12/01 13:49:59 Including rules: default\n[gosec] 2025/12/01 13:49:59 Excluding rules: default\n[gosec] 2025/12/01 13:49:59 Including analyzers: default\n[gosec] 2025/12/01 13:49:59 Excluding analyzers: default\n[gosec] 2025/12/01 13:49:59 Import directory: /tmp/go_synthesis_0icqfxj1\n[gosec] 2025/12/01 13:49:59 Checking package: main\n[gosec] 2025/12/01 13:49:59 Checking file: /tmp/go_synthesis_0icqfxj1/main.go"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"crypto/rand\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype TaskStatus struct {\n\tName      string\n\tStatus    string\n\tMetadata  map[string]interface{}\n\tUpdatedAt time.Time\n}\n\ntype StatusUpdate struct {\n\tName   string\n\tStatus string\n}\n\ntype TaskManager struct {\n\ttasks         map[string]*TaskStatus\n\ttasksMutex    sync.RWMutex\n\tstatusChannel chan StatusUpdate\n\tdone          chan struct{}\n\twg            sync.WaitGroup\n\tsyncedCount   int\n\tfailedCount   int\n\tstatsLock     sync.Mutex\n}\n\nfunc NewTaskManager(channelCapacity int) *TaskManager {\n\ttm := &TaskManager{\n\t\ttasks:         make(map[string]*TaskStatus),\n\t\tstatusChannel: make(chan StatusUpdate, channelCapacity),\n\t\tdone:          make(chan struct{}),\n\t\tsyncedCount:   0,\n\t\tfailedCount:   0,\n\t}\n\treturn tm\n}\n\nfunc (tm *TaskManager) Start() {\n\ttm.wg.Add(1)\n\tgo tm.syncWorker()\n}\n\nfunc (tm *TaskManager) SetTaskStatus(taskName string, newStatus string) {\n\ttm.tasksMutex.Lock()\n\n\texistingTask, exists := tm.tasks[taskName]\n\tif exists && existingTask.Status == newStatus {\n\t\ttm.tasksMutex.Unlock()\n\t\treturn\n\t}\n\n\ttask := &TaskStatus{\n\t\tName:      taskName,\n\t\tStatus:    newStatus,\n\t\tMetadata:  make(map[string]interface{}),\n\t\tUpdatedAt: time.Now(),\n\t}\n\n\ttm.tasks[taskName] = task\n\ttm.tasksMutex.Unlock()\n\n\tselect {\n\tcase tm.statusChannel <- StatusUpdate{Name: taskName, Status: newStatus}:\n\tcase <-tm.done:\n\t}\n}\n\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.tasksMutex.Lock()\n\tdelete(tm.tasks, taskName)\n\ttm.tasksMutex.Unlock()\n}\n\nfunc (tm *TaskManager) SyncBatch() {\n\tbatch := make([]StatusUpdate, 0, 5)\n\ttimeout := time.After(100 * time.Millisecond)\n\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusChannel:\n\t\t\tbatch = append(batch, update)\n\t\t\tif len(batch) >= 5 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-timeout:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\tcase <-tm.done:\n\t\t\tif len(batch) > 0 {\n\t\t\t\ttm.processBatch(batch)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) processBatch(batch []StatusUpdate) {\n\tsyncFailed := false\n\tfailureThreshold := 0.2\n\n\trandomBytes := make([]byte, 8)\n\t_, err := rand.Read(randomBytes)\n\tif err == nil {\n\t\trandomValue := float64(binary.BigEndian.Uint64(randomBytes)) / float64(^uint64(0))\n\t\tif randomValue < failureThreshold {\n\t\t\tsyncFailed = true\n\t\t}\n\t}\n\n\tif syncFailed {\n\t\ttm.statsLock.Lock()\n\t\ttm.failedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\n\t\tfmt.Printf(\"[SYNC FAILED] Failed to sync %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t\ttm.DeleteTaskStatus(update.Name)\n\t\t}\n\t\tfmt.Println()\n\t} else {\n\t\ttm.statsLock.Lock()\n\t\ttm.syncedCount += len(batch)\n\t\ttm.statsLock.Unlock()\n\n\t\tfmt.Printf(\"[SYNC SUCCESS] Synced %d updates: \", len(batch))\n\t\tfor _, update := range batch {\n\t\t\tfmt.Printf(\"%s(%s) \", update.Name, update.Status)\n\t\t}\n\t\tfmt.Println()\n\t}\n}\n\nfunc (tm *TaskManager) syncWorker() {\n\tdefer tm.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-tm.done:\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusChannel:\n\t\t\t\t\tbatch := []StatusUpdate{update}\n\t\t\t\t\ttm.processBatch(batch)\n\t\t\t\tdefault:\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\ttm.SyncBatch()\n\t\t}\n\t}\n}\n\nfunc (tm *TaskManager) GetStats() (synced int, failed int) {\n\ttm.statsLock.Lock()\n\tdefer tm.statsLock.Unlock()\n\treturn tm.syncedCount, tm.failedCount\n}\n\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (*TaskStatus, bool) {\n\ttm.tasksMutex.RLock()\n\tdefer tm.tasksMutex.RUnlock()\n\ttask, exists := tm.tasks[taskName]\n\treturn task, exists\n}\n\nfunc (tm *TaskManager) Shutdown() {\n\tclose(tm.done)\n\ttm.wg.Wait()\n}\n\nfunc randomIntN(n int) int {\n\trandomBytes := make([]byte, 4)\n\t_, err := rand.Read(randomBytes)\n\tif err != nil {\n\t\treturn 0\n\t}\n\treturn int(binary.BigEndian.Uint32(randomBytes)) % n\n}\n\nfunc randomDuration(min, max int) time.Duration {\n\trandomBytes := make([]byte, 4)\n\t_, err := rand.Read(randomBytes)\n\tif err != nil {\n\t\treturn time.Duration(min) * time.Millisecond\n\t}\n\trandomValue := int(binary.BigEndian.Uint32(randomBytes)) % (max - min)\n\treturn time.Duration(min+randomValue) * time.Millisecond\n}\n\nfunc worker(tm *TaskManager, workerID int, duration time.Duration) {\n\ttaskNames := []string{\n\t\tfmt.Sprintf(\"task_%d_1\", workerID),\n\t\tfmt.Sprintf(\"task_%d_2\", workerID),\n\t\tfmt.Sprintf(\"task_%d_3\", workerID),\n\t\tfmt.Sprintf(\"task_%d_4\", workerID),\n\t\tfmt.Sprintf(\"task_%d_5\", workerID),\n\t}\n\n\tstatuses := []string{\"pending\", \"running\", \"completed\"}\n\n\tticker := time.NewTicker(randomDuration(100, 300))\n\tdefer ticker.Stop()\n\n\ttimeout := time.After(duration)\n\n\tfor {\n\t\tselect {\n\t\tcase <-timeout:\n\t\t\tfmt.Printf(\"Worker %d finished\\n\", workerID)\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\ttaskName := taskNames[randomIntN(len(taskNames))]\n\t\t\tstatus := statuses[randomIntN(len(statuses))]\n\t\t\ttm.SetTaskStatus(taskName, status)\n\t\t}\n\t}\n}\n\nfunc statsReporter(tm *TaskManager, ticker *time.Ticker) {\n\tfor range ticker.C {\n\t\tsynced, failed := tm.GetStats()\n\t\ttotal := synced + failed\n\t\tif total > 0 {\n\t\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\t\tfmt.Printf(\"[STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\t\ttotal, synced, failed, successRate)\n\t\t}\n\t}\n}\n\nfunc main() {\n\ttm := NewTaskManager(5)\n\ttm.Start()\n\n\tduration := 10 * time.Second\n\tnumWorkers := 10\n\n\tstatsTicker := time.NewTicker(2 * time.Second)\n\tdefer statsTicker.Stop()\n\n\tgo statsReporter(tm, statsTicker)\n\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < numWorkers; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\tworker(tm, id, duration)\n\t\t}(i)\n\t}\n\n\twg.Wait()\n\n\ttime.Sleep(1 * time.Second)\n\n\ttm.Shutdown()\n\n\tsynced, failed := tm.GetStats()\n\ttotal := synced + failed\n\tif total > 0 {\n\t\tsuccessRate := float64(synced) / float64(total) * 100\n\t\tfmt.Printf(\"\\n[FINAL STATS] Total: %d | Synced: %d | Failed: %d | Success Rate: %.1f%%\\n\",\n\t\t\ttotal, synced, failed, successRate)\n\t}\n}",
            "passed": true,
            "errors": []
        }
    ],
    "passed": true
}