{
    "task": "task1.txt",
    "provider": "gemini",
    "model": "gemini-2.5-flash",
    "rounds": [
        {
            "code": "package main\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\t\"unsafe\"\n)\n\nconst (\n\t// DefaultPageSize is the size of a page in the database file.\n\tDefaultPageSize = 4096 // 4KB\n\n\t// MetaPageSize is the size of a metadata page.\n\tMetaPageSize = DefaultPageSize\n\n\t// InitialDBSize is the initial size of the database file.\n\t// It must be large enough to hold at least two meta pages and some initial data.\n\tInitialDBSize = 4 * DefaultPageSize // 2 meta pages + 2 data pages initially\n\n\t// MetaPage0Offset is the offset of the first meta page.\n\tMetaPage0Offset = 0\n\n\t// MetaPage1Offset is the offset of the second meta page.\n\tMetaPage1Offset = MetaPageSize\n\n\t// DataOffset is the offset where actual data pages begin.\n\tDataOffset = 2 * MetaPageSize\n)\n\nvar (\n\tErrTxClosed           = errors.New(\"transaction closed\")\n\tErrDatabaseClosed     = errors.New(\"database closed\")\n\tErrReadOnlyTx         = errors.New(\"cannot write in a read-only transaction\")\n\tErrPageNotFound       = errors.New(\"page not found\")\n\tErrInvalidMeta        = errors.New(\"invalid metadata page\")\n\tErrCommitFailed       = errors.New(\"transaction commit failed\")\n\tErrInsufficientSpace  = errors.New(\"insufficient space for data\")\n\tErrFileSizeTooSmall   = errors.New(\"database file size is too small for metadata\")\n)\n\n// Meta represents the database's metadata.\n// It's a fixed-size structure that will be directly mapped to the database file.\n// Fields are ordered for direct binary serialization.\ntype Meta struct {\n\tVersion        uint64 // Database version\n\tPageSize       uint32 // Size of a data page\n\tFreeListHead   uint64 // Pointer to the head of the free list (page ID)\n\tFileSize       uint64 // Current total size of the database file\n\tNumPages       uint64 // Total number of data pages\n\tChecksum       uint64 // Checksum of the Meta data\n\tCurrentMetaIdx uint32 // Index of the currently active meta page (0 or 1)\n\tTxID           uint64 // Last committed transaction ID\n\t_              [4]byte // Padding for alignment, ensure consistent size (e.g., 64 bytes)\n}\n\n// metaMarshalSize is the size of the Meta struct in bytes.\nvar metaMarshalSize = uint64(unsafe.Sizeof(Meta{}))\n\n// CalculateChecksum calculates the CRC32 checksum of the Meta struct.\n// The Checksum field itself is set to 0 during calculation.\nfunc (m *Meta) CalculateChecksum() uint64 {\n\tm.Checksum = 0 // Exclude checksum field itself from calculation\n\tbuf := make([]byte, metaMarshalSize)\n\t// Manually serialize the fields to a buffer for checksum calculation\n\tbinary.BigEndian.PutUint64(buf[0:8], m.Version)\n\tbinary.BigEndian.PutUint32(buf[8:12], m.PageSize)\n\tbinary.BigEndian.PutUint64(buf[12:20], m.FreeListHead)\n\tbinary.BigEndian.PutUint64(buf[20:28], m.FileSize)\n\tbinary.BigEndian.PutUint64(buf[28:36], m.NumPages)\n\tbinary.BigEndian.PutUint64(buf[36:44], m.Checksum) // This will be 0\n\tbinary.BigEndian.PutUint32(buf[44:48], m.CurrentMetaIdx)\n\tbinary.BigEndian.PutUint64(buf[48:56], m.TxID)\n\n\treturn uint64(crc32.ChecksumIEEE(buf))\n}\n\n// ValidateChecksum validates the checksum of the Meta struct.\nfunc (m *Meta) ValidateChecksum() bool {\n\tchecksum := m.Checksum\n\tcalculated := m.CalculateChecksum()\n\tm.Checksum = checksum // Restore original checksum\n\treturn checksum == calculated\n}\n\n// DB represents the memory-mapped database.\ntype DB struct {\n\tpath string\n\tfile *os.File\n\n\t// mmapLock protects access to mmapData and its associated file operations (remap).\n\t// Read-locks are held by transactions to ensure they work with a stable mmap.\n\t// Write-locks are held during remap operations.\n\tmmapLock sync.RWMutex\n\tmmapData []byte // The current memory-mapped region\n\n\t// meta represents the current database metadata.\n\t// It is a *copy* of the on-disk meta, updated during write transactions.\n\t// Access to this field is protected by mmapLock.\n\tmeta *Meta\n\n\t// activeTxns tracks the number of active transactions.\n\t// Used during remap to wait for old transactions to complete.\n\tactiveTxns sync.WaitGroup\n\n\t// txnIDCounter generates unique IDs for transactions.\n\ttxnIDCounter uint64\n\n\t// remapCond is used to signal and wait for remap operations.\n\t// It's associated with mmapLock.\n\tremapCond *sync.Cond\n\n\t// isRemapping is a flag indicating if a remap operation is in progress.\n\t// Protected by mmapLock.\n\tisRemapping bool\n\n\t// closed indicates if the database is closed.\n\tclosed bool\n\tmu     sync.Mutex // Protects 'closed' field.\n}\n\n// Open opens a database file at the given path. If the file does not exist, it is created.\nfunc Open(path string) (*DB, error) {\n\tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to open database file: %w\", err)\n\t}\n\n\tdb := &DB{\n\t\tpath: file.Name(),\n\t\tfile: file,\n\t}\n\tdb.remapCond = sync.NewCond(db.mmapLock.Writer()) // Condition associated with the write-part of RWMutex\n\n\t// Initialize database file if it's new/empty\n\tfi, err := file.Stat()\n\tif err != nil {\n\t\t_ = db.Close() // Attempt to clean up\n\t\treturn nil, fmt.Errorf(\"failed to stat database file: %w\", err)\n\t}\n\n\tcurrentFileSize := uint64(fi.Size())\n\tif currentFileSize < DataOffset { // Ensure enough space for meta pages + some initial data\n\t\tif err := db.initFile(); err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, fmt.Errorf(\"failed to initialize database file: %w\", err)\n\t\t}\n\t\tcurrentFileSize = InitialDBSize\n\t} else if currentFileSize < InitialDBSize {\n\t\t// If the file exists but is smaller than our initial requirement, expand it.\n\t\t// This handles cases where it was previously initialized but then truncated.\n\t\tdb.mmapLock.Lock() // Acquire lock for expansion\n\t\tdefer db.mmapLock.Unlock()\n\t\tif err := db.expandFile(InitialDBSize); err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, fmt.Errorf(\"failed to expand database file to initial size: %w\", err)\n\t\t}\n\t\tcurrentFileSize = InitialDBSize\n\t}\n\n\t// Initial mmap\n\tdb.mmapLock.Lock() // Need a write lock to initialize mmapData\n\tdefer db.mmapLock.Unlock()\n\tif err := db.mmap(int(currentFileSize)); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, fmt.Errorf(\"failed to mmap database file: %w\", err)\n\t}\n\n\t// Load metadata\n\tif err := db.loadMeta(); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, fmt.Errorf(\"failed to load metadata: %w\", err)\n\t}\n\n\treturn db, nil\n}\n\n// initFile initializes a new database file with default metadata.\n// This function must acquire the db.mmapLock (write lock) as it modifies the file system and mmapData.\nfunc (db *DB) initFile() error {\n\tdb.mmapLock.Lock()\n\tdefer db.mmapLock.Unlock()\n\n\t// Truncate to initial size\n\tif err := db.file.Truncate(InitialDBSize); err != nil {\n\t\treturn fmt.Errorf(\"failed to truncate file for initialization: %w\", err)\n\t}\n\n\t// Mmap the initial file size\n\tif db.mmapData != nil { // Unmap existing if any, though unlikely on initFile\n\t\tif err := syscall.Munmap(db.mmapData); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to munmap old region during init: %w\", err)\n\t\t}\n\t}\n\tvar err error\n\tdb.mmapData, err = syscall.Mmap(int(db.file.Fd()), 0, InitialDBSize, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to mmap file for initialization: %w\", err)\n\t}\n\n\tdb.meta = &Meta{\n\t\tVersion:        1,\n\t\tPageSize:       DefaultPageSize,\n\t\tFreeListHead:   0, // No free pages initially\n\t\tFileSize:       InitialDBSize,\n\t\tNumPages:       (InitialDBSize - DataOffset) / DefaultPageSize,\n\t\tTxID:           0,\n\t\tCurrentMetaIdx: 0, // Start with meta page 0\n\t}\n\n\t// Write initial metadata to both meta pages for redundancy\n\t// We increment TxID to differentiate the two meta pages if both are valid\n\tif err := db.writeMeta(db.meta, 0); err != nil {\n\t\treturn fmt.Errorf(\"failed to write initial meta page 0: %w\", err)\n\t}\n\tdb.meta.CurrentMetaIdx = 1 // Switch to next meta page for next write\n\tdb.meta.TxID++             // Increment TxID for the second meta page\n\tif err := db.writeMeta(db.meta, 1); err != nil {\n\t\treturn fmt.Errorf(\"failed to write initial meta page 1: %w\", err)\n\t}\n\n\t// Sync to disk\n\tif err := syscall.Msync(db.mmapData, syscall.MS_SYNC); err != nil {\n\t\treturn fmt.Errorf(\"failed to sync initial metadata: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// loadMeta loads the metadata from the database file.\n// It reads both meta pages, validates their checksums, and uses the most recent valid one.\n// Assumes db.mmapLock (write lock) is held.\nfunc (db *DB) loadMeta() error {\n\tvar meta0, meta1 Meta\n\tif len(db.mmapData) < int(MetaPage1Offset+metaMarshalSize) {\n\t\treturn ErrFileSizeTooSmall\n\t}\n\n\t// Read meta page 0\n\tbuf0 := db.mmapData[MetaPage0Offset : MetaPage0Offset+metaMarshalSize]\n\t// Use unsafe.Pointer for direct struct interpretation, which is fast but requires care.\n\t// For production-grade, consider using `encoding/binary` for portability.\n\tmeta0Ptr := (*Meta)(unsafe.Pointer(&buf0[0]))\n\tmeta0 = *meta0Ptr // Copy the meta data\n\n\t// Read meta page 1\n\tbuf1 := db.mmapData[MetaPage1Offset : MetaPage1Offset+metaMarshalSize]\n\tmeta1Ptr := (*Meta)(unsafe.Pointer(&buf1[0]))\n\tmeta1 = *meta1Ptr // Copy the meta data\n\n\tvar valid0, valid1 bool\n\tif meta0.ValidateChecksum() {\n\t\tvalid0 = true\n\t} else {\n\t\tfmt.Printf(\"Warning: Meta page 0 checksum invalid (txid %d). File: %s\\n\", meta0.TxID, db.path)\n\t}\n\tif meta1.ValidateChecksum() {\n\t\tvalid1 = true\n\t} else {\n\t\tfmt.Printf(\"Warning: Meta page 1 checksum invalid (txid %d). File: %s\\n\", meta1.TxID, db.path)\n\t}\n\n\tswitch {\n\tcase valid0 && valid1:\n\t\t// Both are valid, pick the one with the higher TxID (most recent)\n\t\tif meta0.TxID > meta1.TxID {\n\t\t\tdb.meta = &meta0\n\t\t} else {\n\t\t\tdb.meta = &meta1\n\t\t}\n\tcase valid0:\n\t\tdb.meta = &meta0\n\tcase valid1:\n\t\tdb.meta = &meta1\n\tdefault:\n\t\treturn ErrInvalidMeta // Neither meta page is valid\n\t}\n\n\t// Ensure the database file size is consistent with the loaded meta.\n\tif uint64(len(db.mmapData)) < db.meta.FileSize {\n\t\treturn fmt.Errorf(\"mmap data size (%d) is smaller than meta.FileSize (%d)\", len(db.mmapData), db.meta.FileSize)\n\t}\n\treturn nil\n}\n\n// writeMeta writes the given metadata to the specified meta page index (0 or 1).\n// Assumes db.mmapLock.Lock() is held.\nfunc (db *DB) writeMeta(meta *Meta, idx int) error {\n\tmeta.Checksum = meta.CalculateChecksum() // Update checksum before writing\n\n\toffset := MetaPage0Offset\n\tif idx == 1 {\n\t\toffset = MetaPage1Offset\n\t}\n\n\tif len(db.mmapData) < int(offset+metaMarshalSize) {\n\t\treturn fmt.Errorf(\"mmap region too small to write meta page %d\", idx)\n\t}\n\n\t// Directly write the meta struct to the mmapData.\n\tsrcPtr := unsafe.Pointer(meta)\n\tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n\tfor i := 0; i < int(metaMarshalSize); i++ {\n\t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n\t}\n\treturn nil\n}\n\n// Close closes the database.\nfunc (db *DB) Close() error {\n\tdb.mu.Lock()\n\tif db.closed {\n\t\tdb.mu.Unlock()\n\t\treturn ErrDatabaseClosed\n\t}\n\tdb.closed = true\n\tdb.mu.Unlock()\n\n\t// Wait for all active transactions to complete before unmapping/closing\n\tfmt.Println(\"Waiting for active transactions to complete before closing DB...\")\n\tdb.activeTxns.Wait()\n\tfmt.Println(\"All active transactions completed, proceeding with DB close.\")\n\n\tdb.mmapLock.Lock() // Acquire write lock to ensure no new mmap operations or transactions start\n\tdefer db.mmapLock.Unlock()\n\n\tvar err error\n\t// Flush changes to disk before unmapping\n\tif db.mmapData != nil {\n\t\tif syncErr := syscall.Msync(db.mmapData, syscall.MS_SYNC); syncErr != nil {\n\t\t\terr = errors.Join(err, fmt.Errorf(\"failed to sync mmap data: %w\", syncErr))\n\t\t}\n\t\tif munmapErr := syscall.Munmap(db.mmapData); munmapErr != nil {\n\t\t\terr = errors.Join(err, fmt.Errorf(\"failed to unmap data: %w\", munmapErr))\n\t\t}\n\t\tdb.mmapData = nil\n\t}\n\n\tif closeErr := db.file.Close(); closeErr != nil {\n\t\terr = errors.Join(err, fmt.Errorf(\"failed to close file: %w\", closeErr))\n\t}\n\n\treturn err\n}\n\n// mmap maps the database file into memory.\n// Assumes db.mmapLock is held (either RLock for initial mmap or Lock for remap).\nfunc (db *DB) mmap(size int) error {\n\tif db.mmapData != nil {\n\t\tif err := syscall.Munmap(db.mmapData); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to munmap old region: %w\", err)\n\t\t}\n\t}\n\n\tvar err error\n\tdb.mmapData, err = syscall.Mmap(int(db.file.Fd()), 0, size, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to mmap file: %w\", err)\n\t}\n\treturn nil\n}\n\n// expandFile expands the database file to a new size and remaps it.\n// This function requires an exclusive write lock on db.mmapLock.\n// Assumes db.mmapLock is already held.\nfunc (db *DB) expandFile(newSize uint64) error {\n\tif newSize <= uint64(len(db.mmapData)) {\n\t\treturn nil // No expansion needed or shrinking is not supported here.\n\t}\n\n\t// Round up to nearest page size\n\tif newSize%uint64(db.meta.PageSize) != 0 {\n\t\tnewSize = (newSize/uint64(db.meta.PageSize) + 1) * uint64(db.meta.PageSize)\n\t}\n\n\tfmt.Printf(\"Remapping DB: Old size %d, New size %d, File: %s\\n\", len(db.mmapData), newSize, db.path)\n\n\tdb.isRemapping = true\n\t// Wake up any new transactions waiting on remap, so they retry after it's done.\n\tdb.remapCond.Broadcast()\n\n\t// Wait for all existing transactions to complete.\n\t// This is the crucial step to ensure no one is accessing the old mmapData.\n\tfmt.Println(\"Waiting for all active transactions to complete before remap...\")\n\tdb.activeTxns.Wait()\n\tfmt.Println(\"All active transactions completed, proceeding with remap.\")\n\n\t// Truncate the file to the new size\n\tif err := db.file.Truncate(int64(newSize)); err != nil {\n\t\tdb.isRemapping = false\n\t\tdb.remapCond.Broadcast() // Re-broadcast in case of error\n\t\treturn fmt.Errorf(\"failed to truncate file to new size %d: %w\", newSize, err)\n\t}\n\n\t// Mmap the new region\n\tif err := db.mmap(int(newSize)); err != nil {\n\t\tdb.isRemapping = false\n\t\tdb.remapCond.Broadcast() // Re-broadcast in case of error\n\t\treturn fmt.Errorf(\"failed to mmap new file region of size %d: %w\", newSize, err)\n\t}\n\n\tdb.meta.FileSize = newSize\n\tdb.meta.NumPages = (newSize - DataOffset) / db.meta.PageSize\n\tdb.isRemapping = false\n\tdb.remapCond.Broadcast() // Signal that remap is complete and new map is ready\n\n\tfmt.Printf(\"DB remapped successfully to size %d, File: %s\\n\", newSize, db.path)\n\treturn nil\n}\n\n// Begin starts a new transaction.\n// If writable is true, a read-write transaction is started. Otherwise, a read-only transaction.\nfunc (db *DB) Begin(writable bool) (Transaction, error) {\n\tdb.mu.Lock()\n\tif db.closed {\n\t\tdb.mu.Unlock()\n\t\treturn nil, ErrDatabaseClosed\n\t}\n\tdb.mu.Unlock()\n\n\ttxID := atomic.AddUint64(&db.txnIDCounter, 1)\n\tdb.activeTxns.Add(1) // Increment active transaction counter\n\n\tif writable {\n\t\tdb.mmapLock.Lock() // Acquire exclusive write lock for R/W transaction\n\t\t// A read-write transaction holds the database-wide write lock for its entire duration\n\t\t// (from Begin to Commit/Rollback). This ensures only one R/W transaction at a time.\n\t\t// It also protects against concurrent remap operations.\n\n\t\t// If a remap is in progress when a R/W transaction tries to begin, it must wait.\n\t\tfor db.isRemapping {\n\t\t\tdb.mmapLock.Unlock() // Release lock to allow remap to proceed\n\t\t\tdb.remapCond.L.Lock()\n\t\t\tif db.isRemapping { // Double check after acquiring cond mutex\n\t\t\t\tdb.remapCond.Wait() // Wait for remap to finish\n\t\t\t}\n\t\t\tdb.remapCond.L.Unlock()\n\t\t\tdb.mmapLock.Lock() // Re-acquire write lock\n\t\t}\n\n\t\t// After acquiring write lock and ensuring no remap, take a snapshot of meta and mmap.\n\t\tmetaCopy := *db.meta\n\t\tmmapSnapshot := db.mmapData\n\n\t\treturn &ReadWriteTx{\n\t\t\tbaseTx: baseTx{\n\t\t\t\tid:       txID,\n\t\t\t\tdb:       db,\n\t\t\t\tmmapData: mmapSnapshot, // Points to current mmap\n\t\t\t\tdone:     false,\n\t\t\t},\n\t\t\tmeta:          metaCopy,\n\t\t\tpendingWrites: make(map[uint64][]byte),\n\t\t\tnewFileSize:   db.meta.FileSize,\n\t\t}, nil\n\t} else {\n\t\tdb.mmapLock.RLock() // Acquire read lock for R/O transaction\n\n\t\t// If a remap is in progress, release read lock and wait.\n\t\tfor db.isRemapping {\n\t\t\tdb.mmapLock.RUnlock()\n\t\t\tdb.remapCond.L.Lock()\n\t\t\tif db.isRemapping { // Double check after acquiring cond mutex\n\t\t\t\tdb.remapCond.Wait() // Wait for remap to finish\n\t\t\t}\n\t\t\tdb.remapCond.L.Unlock()\n\t\t\tdb.mmapLock.RLock() // Re-acquire read lock for a new attempt\n\t\t}\n\t\t// After acquiring read lock and ensuring no remap, take a snapshot of meta and mmap.\n\t\tmetaCopy := *db.meta\n\t\tmmapSnapshot := db.mmapData\n\t\t\n\t\tdefer db.mmapLock.RUnlock() // Release read lock after transaction creation.\n\n\t\treturn &ReadOnlyTx{\n\t\t\tbaseTx: baseTx{\n\t\t\t\tid:       txID,\n\t\t\t\tdb:       db,\n\t\t\t\tmmapData: mmapSnapshot, // Points to current mmap slice\n\t\t\t\tdone:     false,\n\t\t\t},\n\t\t\tsnapshotMeta: metaCopy,\n\t\t}, nil\n\t}\n}\n\n// Transaction interface for database operations.\ntype Transaction interface {\n\tID() uint64\n\tRead(offset, size uint64) ([]byte, error)\n\tPage(pageID uint64) ([]byte, error)\n\tCommit() error\n\tRollback() error\n\tIsWritable() bool\n}\n\n// baseTx provides common fields and methods for both transaction types.\ntype baseTx struct {\n\tid       uint64\n\tdb       *DB\n\tmmapData []byte // A snapshot of the mmapData when the transaction started\n\tdone     bool   // True if the transaction is committed or rolled back\n\tmu       sync.Mutex // Protects 'done'\n}\n\nfunc (b *baseTx) ID() uint64 {\n\treturn b.id\n}\n\n// doneTx cleans up common transaction resources.\nfunc (b *baseTx) doneTx() {\n\tb.mu.Lock()\n\tif !b.done {\n\t\tb.done = true\n\t\tb.db.activeTxns.Done() // Decrement active transaction count\n\n\t\t// If this was a ReadWriteTx, its db.mmapLock.Lock() needs to be released.\n\t\t// This is handled by a defer in ReadWriteTx.Commit/Rollback.\n\t}\n\tb.mu.Unlock()\n}\n\nfunc (b *baseTx) Read(offset, size uint64) ([]byte, error) {\n\tb.mu.Lock()\n\tif b.done {\n\t\tb.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\tb.mu.Unlock()\n\n\tif offset+size > uint64(len(b.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"read out of bounds: offset %d, size %d, mmap size %d\", offset, size, len(b.mmapData))\n\t}\n\treturn b.mmapData[offset : offset+size], nil\n}\n\n// Page returns a byte slice representing the data for a given page ID.\nfunc (b *baseTx) Page(pageID uint64) ([]byte, error) {\n\tb.mu.Lock()\n\tif b.done {\n\t\tb.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\tb.mu.Unlock()\n\n\t// For read-only transactions, use snapshotMeta.\n\t// For read-write transactions, use tx.meta.\n\t// For baseTx, we use a snapshot for consistency.\n\tmeta := b.db.meta // A ReadOnlyTx has its own snapshotMeta, an RW Tx has tx.meta.\n\t// This baseTx method is generic, so it refers to the *current* DB meta for page structure.\n\t// This is safe because meta.PageSize is fixed after init.\n\tif meta == nil {\n\t\treturn nil, fmt.Errorf(\"database metadata not loaded\")\n\t}\n\tpageSize := uint64(meta.PageSize)\n\n\t// Note: For ReadOnlyTx, if the DB expands and new pages are added,\n\t// this transaction will not see them beyond its initial NumPages.\n\t// This is consistent with transactional snapshot isolation for RO.\n\tif pageID >= meta.NumPages {\n\t\treturn nil, ErrPageNotFound\n\t}\n\n\toffset := DataOffset + pageID*pageSize\n\tif offset+pageSize > uint64(len(b.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"page %d (offset %d) out of mmap bounds\", pageID, offset)\n\t}\n\treturn b.mmapData[offset : offset+pageSize], nil\n}\n\n// ReadOnlyTx represents a read-only database transaction.\ntype ReadOnlyTx struct {\n\tbaseTx\n\tsnapshotMeta Meta // A copy of the metadata at the time of transaction creation.\n}\n\n// IsWritable returns false for a read-only transaction.\nfunc (tx *ReadOnlyTx) IsWritable() bool {\n\treturn false\n}\n\n// Commit attempts to commit the read-only transaction.\n// For read-only transactions, this effectively just marks it as done.\nfunc (tx *ReadOnlyTx) Commit() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\ttx.doneTx()\n\treturn nil\n}\n\n// Rollback rolls back the read-only transaction.\n// For read-only transactions, this effectively just marks it as done.\nfunc (tx *ReadOnlyTx) Rollback() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\ttx.doneTx()\n\treturn nil\n}\n\n// ReadWriteTx represents a read-write database transaction.\ntype ReadWriteTx struct {\n\tbaseTx\n\tmeta          Meta              // A mutable copy of the metadata for this transaction\n\tpendingWrites map[uint64][]byte // Offset -> Data to write\n\tnewFileSize   uint64            // Tracks the required file size\n}\n\n// IsWritable returns true for a read-write transaction.\nfunc (tx *ReadWriteTx) IsWritable() bool {\n\treturn true\n}\n\n// Write writes data to the specified offset.\n// Data is buffered until Commit.\nfunc (tx *ReadWriteTx) Write(offset uint64, data []byte) error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tif offset+uint64(len(data)) > tx.newFileSize {\n\t\ttx.newFileSize = offset + uint64(len(data))\n\t}\n\ttx.pendingWrites[offset] = data\n\treturn nil\n}\n\n// AllocatePage allocates a new page and returns its ID and a byte slice to its data.\n// It will update `tx.newFileSize` and `tx.meta.NumPages` accordingly.\n// The returned slice is a temporary buffer for the caller to fill; actual write happens on Commit.\nfunc (tx *ReadWriteTx) AllocatePage(size uint64) (uint64, []byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn 0, nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.meta.PageSize)\n\tif size > pageSize {\n\t\treturn 0, nil, ErrInsufficientSpace // Can't allocate a page larger than PageSize\n\t}\n\n\tpageID := tx.meta.NumPages // New page ID is current NumPages\n\trequiredSize := DataOffset + (pageID+1)*pageSize\n\n\tif requiredSize > tx.newFileSize {\n\t\ttx.newFileSize = requiredSize\n\t}\n\n\ttx.meta.NumPages++ // Update transaction's meta for new page count\n\n\t// Return a temporary buffer for the caller to write into.\n\t// On commit, this data will be transferred from `pendingWrites` to `mmapData`.\n\ttempBuf := make([]byte, pageSize)\n\toffset := DataOffset + pageID*pageSize\n\ttx.pendingWrites[offset] = tempBuf // Buffer for the page data\n\treturn pageID, tempBuf, nil\n}\n\n// Commit commits the read-write transaction.\nfunc (tx *ReadWriteTx) Commit() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tdefer tx.db.mmapLock.Unlock() // Release the DB's write lock that was acquired in Begin\n\tdefer tx.doneTx()             // Ensure doneTx is called even on error\n\n\t// The mmapLock.Lock() was acquired by db.Begin(true) and is held throughout the R/W transaction.\n\t// This ensures that for the commit, no other transactions are active and no remapping is happening.\n\n\t// Step 1: Handle file expansion if needed\n\tif tx.newFileSize > tx.db.meta.FileSize {\n\t\tif err := tx.db.expandFile(tx.newFileSize); err != nil {\n\t\t\treturn fmt.Errorf(\"%w: failed to expand file during commit: %v\", ErrCommitFailed, err)\n\t\t}\n\t\t// Update our internal mmapData reference after expansion\n\t\ttx.mmapData = tx.db.mmapData\n\t} else if tx.newFileSize < tx.db.meta.FileSize {\n\t\t// If tx.newFileSize is smaller, it means we might have released pages.\n\t\t// For now, we only support growth, not shrinking the file on commit.\n\t\t// A full implementation would handle file shrinking (carefully).\n\t\ttx.newFileSize = tx.db.meta.FileSize // Revert to current file size if attempting to shrink\n\t}\n\n\t// Step 2: Apply pending writes to the mmapData\n\tfor offset, data := range tx.pendingWrites {\n\t\tif offset+uint64(len(data)) > uint64(len(tx.db.mmapData)) {\n\t\t\treturn fmt.Errorf(\"%w: write out of mmap bounds after expansion (offset %d, len %d, mmap size %d)\",\n\t\t\t\tErrCommitFailed, offset, len(data), len(tx.db.mmapData))\n\t\t}\n\t\tcopy(tx.db.mmapData[offset:], data)\n\t}\n\n\t// Step 3: Update database metadata\n\ttx.db.meta.Version = tx.meta.Version\n\ttx.db.meta.PageSize = tx.meta.PageSize\n\ttx.db.meta.FreeListHead = tx.meta.FreeListHead\n\ttx.db.meta.FileSize = tx.newFileSize // This should now be consistent\n\ttx.db.meta.NumPages = tx.meta.NumPages\n\ttx.db.meta.TxID = tx.id // Set the database's last committed TxID\n\n\t// Write the updated metadata to the *next* meta page.\n\t// This provides atomic metadata updates (two-phase commit for meta).\n\ttargetMetaIdx := (tx.db.meta.CurrentMetaIdx + 1) % 2\n\ttx.db.meta.CurrentMetaIdx = targetMetaIdx // Update the in-memory meta\n\tif err := tx.db.writeMeta(tx.db.meta, int(targetMetaIdx)); err != nil {\n\t\treturn fmt.Errorf(\"%w: failed to write meta page %d: %v\", ErrCommitFailed, targetMetaIdx, err)\n\t}\n\n\t// Step 4: Flush changes to disk\n\tif err := syscall.Msync(tx.db.mmapData, syscall.MS_SYNC); err != nil {\n\t\treturn fmt.Errorf(\"%w: failed to sync mmap data to disk: %v\", ErrCommitFailed, err)\n\t}\n\n\tfmt.Printf(\"Transaction %d committed successfully. DB FileSize: %d, NumPages: %d, File: %s\\n\", tx.id, tx.db.meta.FileSize, tx.db.meta.NumPages, tx.db.path)\n\n\treturn nil\n}\n\n// Rollback rolls back the read-write transaction.\nfunc (tx *ReadWriteTx) Rollback() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tdefer tx.db.mmapLock.Unlock() // Release the DB's write lock\n\tdefer tx.doneTx()             // Ensure doneTx is called\n\n\t// For a read-write transaction, rollback just discards pending writes.\n\ttx.pendingWrites = nil\n\tfmt.Printf(\"Transaction %d rolled back. File: %s\\n\", tx.id, tx.db.path)\n\treturn nil\n}\n\n// Example usage and demonstration of concurrency.\nfunc main() {\n\tdbPath := filepath.Join(os.TempDir(), \"mydata.db\")\n\t_ = os.Remove(dbPath) // Clean up any previous database file\n\n\tfmt.Printf(\"Database file: %s\\n\", dbPath)\n\n\tdb, err := Open(dbPath)\n\tif err != nil {\n\t\tfmt.Printf(\"Error opening database: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif closeErr := db.Close(); closeErr != nil {\n\t\t\tfmt.Printf(\"Error closing database: %v\\n\", closeErr)\n\t\t}\n\t}()\n\n\t// --- Test 1: Initial write and read ---\n\tfmt.Println(\"\\n--- Test 1: Initial write and read ---\")\n\trwTx, err := db.Begin(true)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RW transaction: %v\\n\", err)\n\t\treturn\n\t}\n\n\t// Allocate a page and write some data\n\tpageID, pageDataBuf, err := rwTx.(*ReadWriteTx).AllocatePage(DefaultPageSize)\n\tif err != nil {\n\t\tfmt.Printf(\"Error allocating page: %v\\n\", err)\n\t\trwTx.Rollback()\n\t\treturn\n\t}\n\tcopy(pageDataBuf, []byte(\"Hello, DB World from Page 0!\"))\n\n\t// Note: With AllocatePage buffering, `tx.Write` call for `pageDataBuf` is implicit.\n\t// If AllocatePage returns a direct slice to mmap, then `tx.Write` would be explicit.\n\n\tif err := rwTx.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RW transaction: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Initial data committed.\")\n\n\t// Read data back\n\troTx, err := db.Begin(false)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RO transaction: %v\\n\", err)\n\t\treturn\n\t}\n\treadPage0Data, err := roTx.Page(0)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading page 0: %v\\n\", err)\n\t\troTx.Rollback()\n\t\treturn\n\t}\n\tfmt.Printf(\"Read from Page 0: %s\\n\", string(readPage0Data[:len(\"Hello, DB World from Page 0!\")]))\n\troTx.Commit()\n\n\t// --- Test 2: Concurrent read-only transactions ---\n\tfmt.Println(\"\\n--- Test 2: Concurrent read-only transactions ---\")\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 5; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\ttx, err := db.Begin(false)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"RO Tx %d: Error beginning transaction: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\t\tfmt.Printf(\"RO Tx %d: Error committing: %v\\n\", id, err)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\treadData, err := tx.Page(0)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"RO Tx %d: Error reading page 0: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfmt.Printf(\"RO Tx %d: Read data: %s\\n\", id, string(readData[:len(\"Hello, DB World from Page 0!\")]))\n\t\t\ttime.Sleep(100 * time.Millisecond) // Simulate work\n\t\t}(i)\n\t}\n\twg.Wait()\n\tfmt.Println(\"All concurrent RO transactions completed.\")\n\n\t// --- Test 3: Read-write transaction with expansion and concurrent RO access ---\n\tfmt.Println(\"\\n--- Test 3: Read-write transaction with expansion and concurrent RO access ---\")\n\n\t// Start some background RO transactions that will be active during the RW commit.\n\tvar concurrentROwg sync.WaitGroup\n\tfor i := 0; i < 3; i++ {\n\t\tconcurrentROwg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer concurrentROwg.Done()\n\t\t\ttx, err := db.Begin(false)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error beginning transaction: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error committing: %v\\n\", id, err)\n\t\t\t\t}\n\t\t\t}()\n\t\t\t// Keep transaction active for a bit to ensure it is running during remap.\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t\treadData, err := tx.Page(0)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error reading page 0: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfmt.Printf(\"Background RO Tx %d: Read data: %s\\n\", id, string(readData[:len(\"Hello, DB World from Page 0!\")]))\n\t\t}(i)\n\t}\n\n\t// Give a moment for background RO transactions to start\n\ttime.Sleep(100 * time.Millisecond)\n\n\t// Start a R/W transaction that expands the file\n\tfmt.Println(\"Starting R/W transaction that will expand the database.\")\n\trwTx2, err := db.Begin(true)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RW transaction 2: %v\\n\", err)\n\t\treturn\n\t}\n\n\t// Allocate a new page, which should trigger file expansion\n\tnewPageID := rwTx2.(*ReadWriteTx).meta.NumPages // Get current number of pages for the new page ID from TX's meta\n\tnewPageDataBuf, _, err := rwTx2.(*ReadWriteTx).AllocatePage(DefaultPageSize)\n\tif err != nil {\n\t\tfmt.Printf(\"Error allocating new page for expansion: %v\\n\", err)\n\t\trwTx2.Rollback()\n\t\treturn\n\t}\n\tcopy(newPageDataBuf, []byte(\"This is new data in an expanded page!\"))\n\n\t// Update page 0 as well to show a write to an existing page\n\tupdatedPage0Data := []byte(\"UPDATED: Hello, DB World from Page 0!\")\n\tif err := rwTx2.(*ReadWriteTx).Write(DataOffset, updatedPage0Data); err != nil {\n\t\tfmt.Printf(\"Error writing updated page 0 data: %v\\n\", err)\n\t\trwTx2.Rollback()\n\t\treturn\n\t}\n\n\tfmt.Println(\"R/W transaction 2 attempting to commit and expand file...\")\n\tif err := rwTx2.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RW transaction 2: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"RW transaction 2 committed successfully, file expanded.\")\n\n\t// Wait for background RO transactions to finish\n\tconcurrentROwg.Wait()\n\tfmt.Println(\"All background RO transactions finished.\")\n\n\t// --- Test 4: Verify expanded data and new RO txns use new map ---\n\tfmt.Println(\"\\n--- Test 4: Verify expanded data and new RO txns use new map ---\")\n\tfinalRoTx, err := db.Begin(false)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning final RO transaction: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif err := finalRoTx.Commit(); err != nil {\n\t\t\tfmt.Printf(\"Error committing final RO: %v\\n\", err)\n\t\t}\n\t}()\n\n\treadUpdatedPage0Data, err := finalRoTx.Page(0)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading updated page 0: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"Final RO Tx: Read from Page 0 (updated): %s\\n\", string(readUpdatedPage0Data[:len(updatedPage0Data)]))\n\n\treadNewPageData, err := finalRoTx.Page(newPageID)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading new page %d: %v\\n\", newPageID, err)\n\t\treturn\n\t}\n\tfmt.Printf(\"Final RO Tx: Read from new Page %d: %s\\n\", newPageID, string(readNewPageData[:len(\"This is new data in an expanded page!\")]))\n\n\t// Demonstrate a new transaction trying to start during a (simulated) remap\n\tfmt.Println(\"\\n--- Test 5: New transaction waiting on remap ---\")\n\t// Simulate a remap by manually locking and setting isRemapping\n\tdb.mmapLock.Lock() // Acquire DB write lock\n\tdb.isRemapping = true\n\tfmt.Println(\"Simulating remapping in progress...\")\n\n\tvar waitingTxErr error\n\tvar waitingTxWg sync.WaitGroup\n\twaitingTxWg.Add(1)\n\tgo func() {\n\t\tdefer waitingTxWg.Done()\n\t\tfmt.Println(\"Attempting to begin a new RO transaction during simulated remap...\")\n\t\ttx, err := db.Begin(false) // This call should block\n\t\tif err != nil {\n\t\t\twaitingTxErr = err\n\t\t\tfmt.Printf(\"Waiting RO Tx: Error beginning transaction: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer func() {\n\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\tfmt.Printf(\"Waiting RO Tx: Error committing: %v\\n\", err)\n\t\t\t}\n\t\t}()\n\t\tfmt.Println(\"Waiting RO Tx: Successfully started after remap completion.\")\n\t}()\n\n\ttime.Sleep(500 * time.Millisecond) // Give the waiting goroutine a chance to hit the wait condition\n\tfmt.Println(\"Releasing simulated remap...\")\n\tdb.isRemapping = false\n\tdb.remapCond.Broadcast() // Signal that remap is done\n\tdb.mmapLock.Unlock()     // Release the lock so the waiting tx can proceed\n\n\twaitingTxWg.Wait()\n\tif waitingTxErr != nil {\n\t\tfmt.Printf(\"Waiting transaction failed: %v\\n\", waitingTxErr)\n\t} else {\n\t\tfmt.Println(\"Waiting transaction successfully proceeded after remap.\")\n\t}\n\n\tfmt.Println(\"\\nDatabase operations complete.\")\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "go build -race",
                    "error": "# command-line-arguments\n./main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n./main.go:230:20: undefined: syscall.Msync\n./main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n./main.go:337:25: undefined: syscall.Msync\n./main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n./main.go:735:20: undefined: syscall.Msync\n./main.go:789:2: declared and not used: pageID\n./main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)"
                },
                {
                    "verifier": "gosec",
                    "error": "Results:\n\nGolang errors in file: []:\n\n  > [line 0 : column 0] - # command-line-arguments\n./main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n./main.go:230:20: undefined: syscall.Msync\n./main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n./main.go:337:25: undefined: syscall.Msync\n./main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n./main.go:735:20: undefined: syscall.Msync\n./main.go:789:2: declared and not used: pageID\n./main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n\n\nGolang errors in file: [/tmp/go_synthesis_4z44__2e/main.go]:\n\n  > [line 140 : column 42] - db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n\n  > [line 230 : column 20] - undefined: syscall.Msync\n\n  > [line 303 : column 28] - invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n\n  > [line 337 : column 25] - undefined: syscall.Msync\n\n  > [line 410 : column 21] - invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n\n  > [line 735 : column 20] - undefined: syscall.Msync\n\n  > [line 789 : column 2] - declared and not used: pageID\n\n  > [line 900 : column 7] - invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n\n\n\n[\u001b[30;43m/tmp/go_synthesis_4z44__2e/main.go:131\u001b[0m] - G304 (CWE-22): Potential file inclusion via variable (Confidence: HIGH, Severity: MEDIUM)\n    130: func Open(path string) (*DB, error) {\n  > 131: \tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n    132: \tif err != nil {\n\nAutofix: Consider using os.Root to scope file access under a fixed root (Go >=1.24). Prefer root.Open/root.Stat over os.Open/os.Stat to prevent directory traversal.\n\n[\u001b[30;43m/tmp/go_synthesis_4z44__2e/main.go:131\u001b[0m] - G302 (CWE-276): Expect file permissions to be 0600 or less (Confidence: HIGH, Severity: MEDIUM)\n    130: func Open(path string) (*DB, error) {\n  > 131: \tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n    132: \tif err != nil {\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:311\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    310: \tfor i := 0; i < int(metaMarshalSize); i++ {\n  > 311: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n    312: \t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:311\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    310: \tfor i := 0; i < int(metaMarshalSize); i++ {\n  > 311: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n    312: \t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:309\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    308: \tsrcPtr := unsafe.Pointer(meta)\n  > 309: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n    310: \tfor i := 0; i < int(metaMarshalSize); i++ {\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:308\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    307: \t// Directly write the meta struct to the mmapData.\n  > 308: \tsrcPtr := unsafe.Pointer(meta)\n    309: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:255\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    254: \tbuf1 := db.mmapData[MetaPage1Offset : MetaPage1Offset+metaMarshalSize]\n  > 255: \tmeta1Ptr := (*Meta)(unsafe.Pointer(&buf1[0]))\n    256: \tmeta1 = *meta1Ptr // Copy the meta data\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:250\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    249: \t// For production-grade, consider using `encoding/binary` for portability.\n  > 250: \tmeta0Ptr := (*Meta)(unsafe.Pointer(&buf0[0]))\n    251: \tmeta0 = *meta0Ptr // Copy the meta data\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:906\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    905: \t\tfmt.Printf(\"Error writing updated page 0 data: %v\\n\", err)\n  > 906: \t\trwTx2.Rollback()\n    907: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:897\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    896: \t\tfmt.Printf(\"Error allocating new page for expansion: %v\\n\", err)\n  > 897: \t\trwTx2.Rollback()\n    898: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:819\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    818: \tfmt.Printf(\"Read from Page 0: %s\\n\", string(readPage0Data[:len(\"Hello, DB World from Page 0!\")]))\n  > 819: \troTx.Commit()\n    820: \n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:815\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    814: \t\tfmt.Printf(\"Error reading page 0: %v\\n\", err)\n  > 815: \t\troTx.Rollback()\n    816: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:792\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    791: \t\tfmt.Printf(\"Error allocating page: %v\\n\", err)\n  > 792: \t\trwTx.Rollback()\n    793: \t\treturn\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 989\n  Nosec  : 0\n  Issues : \u001b[1;31m13\u001b[0m\n\n[gosec] 2025/12/01 13:21:42 Including rules: default\n[gosec] 2025/12/01 13:21:42 Excluding rules: default\n[gosec] 2025/12/01 13:21:42 Including analyzers: default\n[gosec] 2025/12/01 13:21:42 Excluding analyzers: default\n[gosec] 2025/12/01 13:21:42 Import directory: /tmp/go_synthesis_4z44__2e\n[gosec] 2025/12/01 13:21:42 Checking package: main\n[gosec] 2025/12/01 13:21:42 Checking file: /tmp/go_synthesis_4z44__2e/main.go\n[gosec] 2025/12/01 13:21:44 Panic when running SSA analyzer on package: main. Panic: runtime error: invalid memory address or nil pointer dereference\nStack trace:\ngoroutine 1 [running]:\nruntime/debug.Stack()\n\t/usr/local/go/src/runtime/debug/stack.go:26 +0x5e\ngithub.com/securego/gosec/v2.(*Analyzer).buildSSA.func1()\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:501 +0x58\npanic({0xecea40?, 0x1998730?})\n\t/usr/local/go/src/runtime/panic.go:783 +0x132\ngolang.org/x/tools/go/ssa.(*Program).packageLevelMember(...)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/source.go:187\ngolang.org/x/tools/go/ssa.(*builder).expr0(0xc001313080, 0xc0014ca400, {0x11e2648?, 0xc00105d300}, {0x0, {0x0, 0x0}, {0x0, 0x0}})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:820 +0x1b2b\ngolang.org/x/tools/go/ssa.(*builder).expr(0xc001313080, 0xc0014ca400, {0x11e2648, 0xc00105d300})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:641 +0x17b\ngolang.org/x/tools/go/ssa.(*builder).expr0(0xc001313080, 0xc0014ca400, {0x11e26d8?, 0xc001056cc0}, {0x0, {0x0, 0x0}, {0x0, 0x0}})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:842 +0x1031\ngolang.org/x/tools/go/ssa.(*builder).expr(0xc001313080, 0xc0014ca400, {0x11e26d8, 0xc001056cc0})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:641 +0x17b\ngolang.org/x/tools/go/ssa.(*builder).setCallFunc(0xc001313080?, 0xc0014ca400?, 0xc001058f40?, 0xc0014cdcc0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1066 +0x2b6\ngolang.org/x/tools/go/ssa.(*builder).setCall(0xc001313080, 0xc0014ca400, 0xc001058f40, 0xc0014cdcc0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1144 +0x2b\ngolang.org/x/tools/go/ssa.(*builder).expr0(0xc001313080, 0xc0014ca400, {0x11e2708?, 0xc001058f40}, {0x0, {0x0, 0x0}, {0x0, 0x0}})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:724 +0x2105\ngolang.org/x/tools/go/ssa.(*builder).expr(0xc001313080, 0xc0014ca400, {0x11e2708, 0xc001058f40})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:641 +0x17b\ngolang.org/x/tools/go/ssa.(*builder).emitCallArgs(0xc001313080, 0xc0014ca400, 0xc000205940, 0xc001058f80, {0x0?, 0x0, 0x78?})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1090 +0x105\ngolang.org/x/tools/go/ssa.(*builder).setCall(0xc001313080, 0xc0014ca400, 0xc001058f80, 0xc0014cdc40)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1151 +0x85\ngolang.org/x/tools/go/ssa.(*builder).expr0(0xc001313080, 0xc0014ca400, {0x11e2708?, 0xc001058f80}, {0x7, {0x11de848, 0xc000b9faf0}, {0x0, 0x0}})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:724 +0x2105\ngolang.org/x/tools/go/ssa.(*builder).expr(0xc001313080, 0xc0014ca400, {0x11e2708, 0xc001058f80})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:641 +0x17b\ngolang.org/x/tools/go/ssa.(*builder).assign(0xc001313080?, 0xc0014ca400, {0x11e5ec8, 0xc0014d07e0}, {0x11e2708?, 0xc001058f80?}, 0xbe?, 0xc000057078)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:614 +0x3a5\ngolang.org/x/tools/go/ssa.(*builder).assignStmt(0xc001313080, 0xc0014ca400, {0xc001048830, 0x1, 0xc00087e8f0?}, {0xc001048850, 0x1, 0x0?}, 0x0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1229 +0x468\ngolang.org/x/tools/go/ssa.(*builder).stmt(0xc001313080, 0xc0014ca400, {0x11e2d90?, 0xc001058fc0?})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2720 +0x5b5\ngolang.org/x/tools/go/ssa.(*builder).stmtList(...)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:972\ngolang.org/x/tools/go/ssa.(*builder).stmt(0xc001313080, 0xc0014ca400, {0x11e2eb0?, 0xc0010622d0?})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2753 +0x1109\ngolang.org/x/tools/go/ssa.(*builder).buildFromSyntax(0xc001313080, 0xc0014ca400)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2962 +0x25a\ngolang.org/x/tools/go/ssa.(*builder).buildFunction(0xf73000?, 0xc0014ca400)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2912 +0x183\ngolang.org/x/tools/go/ssa.(*builder).iterate(0xc001313080)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2897 +0x25\ngolang.org/x/tools/go/ssa.(*Package).build(0xc000929f00)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:3183 +0xce\nsync.(*Once).doSlow(0xc000f1c480?, 0xc00104b740?)\n\t/usr/local/go/src/sync/once.go:78 +0xac\nsync.(*Once).Do(...)\n\t/usr/local/go/src/sync/once.go:69\ngolang.org/x/tools/go/ssa.(*Package).Build(...)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:3172\ngolang.org/x/tools/go/analysis/passes/buildssa.run(0xc000ddc0e0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/analysis/passes/buildssa/buildssa.go:59 +0x21d\ngithub.com/securego/gosec/v2.(*Analyzer).buildSSA(0x71a07b?, 0xc0007f9a00)\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:524 +0x1d3\ngithub.com/securego/gosec/v2.(*Analyzer).CheckAnalyzers(0xc000a50980, 0xc0007f9a00)\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:415 +0x32\ngithub.com/securego/gosec/v2.(*Analyzer).Process(0xc000a50980, {0x0, 0x0, 0x0}, {0xc000053530, 0x1, 0x3d?})\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:321 +0x487\nmain.main()\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/cmd/gosec/main.go:474 +0xde7\n[gosec] 2025/12/01 13:21:44 Error building the SSA representation of the package main: no ssa result"
                },
                {
                    "verifier": "govulncheck",
                    "error": "govulncheck: loading packages: \nThere are errors with the provided package patterns:\n\n/tmp/go_synthesis_4z44__2e/main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n/tmp/go_synthesis_4z44__2e/main.go:230:20: undefined: syscall.Msync\n/tmp/go_synthesis_4z44__2e/main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n/tmp/go_synthesis_4z44__2e/main.go:337:25: undefined: syscall.Msync\n/tmp/go_synthesis_4z44__2e/main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n/tmp/go_synthesis_4z44__2e/main.go:735:20: undefined: syscall.Msync\n/tmp/go_synthesis_4z44__2e/main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n/tmp/go_synthesis_4z44__2e/main.go:789:2: declared and not used: pageID\n\nFor details on package patterns, see https://pkg.go.dev/cmd/go#hdr-Package_lists_and_patterns."
                },
                {
                    "verifier": "errcheck",
                    "error": "error: failed to check packages: errors while loading package command-line-arguments: [-: # command-line-arguments\n./main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n./main.go:230:20: undefined: syscall.Msync\n./main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n./main.go:337:25: undefined: syscall.Msync\n./main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n./main.go:735:20: undefined: syscall.Msync\n./main.go:789:2: declared and not used: pageID\n./main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64) /tmp/go_synthesis_4z44__2e/main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer) /tmp/go_synthesis_4z44__2e/main.go:230:20: undefined: syscall.Msync /tmp/go_synthesis_4z44__2e/main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64) /tmp/go_synthesis_4z44__2e/main.go:337:25: undefined: syscall.Msync /tmp/go_synthesis_4z44__2e/main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32) /tmp/go_synthesis_4z44__2e/main.go:735:20: undefined: syscall.Msync /tmp/go_synthesis_4z44__2e/main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64) /tmp/go_synthesis_4z44__2e/main.go:789:2: declared and not used: pageID]"
                },
                {
                    "verifier": "go-errorlint",
                    "error": "-: # command-line-arguments\n./main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n./main.go:230:20: undefined: syscall.Msync\n./main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n./main.go:337:25: undefined: syscall.Msync\n./main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n./main.go:735:20: undefined: syscall.Msync\n./main.go:789:2: declared and not used: pageID\n./main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n/tmp/go_synthesis_4z44__2e/main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n/tmp/go_synthesis_4z44__2e/main.go:230:20: undefined: syscall.Msync\n/tmp/go_synthesis_4z44__2e/main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n/tmp/go_synthesis_4z44__2e/main.go:337:25: undefined: syscall.Msync\n/tmp/go_synthesis_4z44__2e/main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n/tmp/go_synthesis_4z44__2e/main.go:735:20: undefined: syscall.Msync\n/tmp/go_synthesis_4z44__2e/main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n/tmp/go_synthesis_4z44__2e/main.go:789:2: declared and not used: pageID\nerrorlint: analysis skipped due to errors in package"
                },
                {
                    "verifier": "staticcheck",
                    "error": "-: # command-line-arguments\n./main.go:140:42: db.mmapLock.Writer undefined (type \"sync\".RWMutex has no field or method Writer)\n./main.go:230:20: undefined: syscall.Msync\n./main.go:303:28: invalid operation: offset + metaMarshalSize (mismatched types int and uint64)\n./main.go:337:25: undefined: syscall.Msync\n./main.go:410:21: invalid operation: (newSize - DataOffset) / db.meta.PageSize (mismatched types uint64 and uint32)\n./main.go:735:20: undefined: syscall.Msync\n./main.go:789:2: declared and not used: pageID\n./main.go:900:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64) (compile)"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n\n\tunix \"golang.org/x/sys/unix\" // For mmap, munmap, msync\n)\n\nconst (\n\t// DefaultPageSize is the size of a page in the database file.\n\tDefaultPageSize = 4096 // 4KB\n\n\t// MetaPageSize is the size of a metadata page.\n\tMetaPageSize = DefaultPageSize\n\n\t// InitialDBSize is the initial size of the database file.\n\t// It must be large enough to hold at least two meta pages and some initial data.\n\tInitialDBSize = 4 * DefaultPageSize // 2 meta pages + 2 data pages initially\n\n\t// MetaPage0Offset is the offset of the first meta page.\n\tMetaPage0Offset = 0\n\n\t// MetaPage1Offset is the offset of the second meta page.\n\tMetaPage1Offset = MetaPageSize\n\n\t// DataOffset is the offset where actual data pages begin.\n\tDataOffset = 2 * MetaPageSize\n)\n\nvar (\n\tErrTxClosed          = errors.New(\"transaction closed\")\n\tErrDatabaseClosed    = errors.New(\"database closed\")\n\tErrReadOnlyTx        = errors.New(\"cannot write in a read-only transaction\")\n\tErrPageNotFound      = errors.New(\"page not found\")\n\tErrInvalidMeta       = errors.New(\"invalid metadata page\")\n\tErrCommitFailed      = errors.New(\"transaction commit failed\")\n\tErrInsufficientSpace = errors.New(\"insufficient space for data\")\n\tErrFileSizeTooSmall  = errors.New(\"database file size is too small for metadata\")\n)\n\n// Meta represents the database's metadata.\n// It's a fixed-size structure that will be directly mapped to the database file.\n// Fields are ordered for direct binary serialization.\ntype Meta struct {\n\tVersion        uint64 // Database version\n\tPageSize       uint32 // Size of a data page\n\tFreeListHead   uint64 // Pointer to the head of the free list (page ID)\n\tFileSize       uint64 // Current total size of the database file\n\tNumPages       uint64 // Total number of data pages\n\tChecksum       uint64 // Checksum of the Meta data\n\tCurrentMetaIdx uint32 // Index of the currently active meta page (0 or 1)\n\tTxID           uint64 // Last committed transaction ID\n\t_              [4]byte // Padding for alignment, ensure consistent size (e.g., 64 bytes)\n}\n\n// metaMarshalSize is the size of the Meta struct in bytes.\nvar metaMarshalSize = uint64(unsafe.Sizeof(Meta{}))\n\n// CalculateChecksum calculates the CRC32 checksum of the Meta struct.\n// The Checksum field itself is set to 0 during calculation.\nfunc (m *Meta) CalculateChecksum() uint64 {\n\tm.Checksum = 0 // Exclude checksum field itself from calculation\n\tbuf := make([]byte, metaMarshalSize)\n\t// Manually serialize the fields to a buffer for checksum calculation\n\tbinary.BigEndian.PutUint64(buf[0:8], m.Version)\n\tbinary.BigEndian.PutUint32(buf[8:12], m.PageSize)\n\tbinary.BigEndian.PutUint64(buf[12:20], m.FreeListHead)\n\tbinary.BigEndian.PutUint64(buf[20:28], m.FileSize)\n\tbinary.BigEndian.PutUint64(buf[28:36], m.NumPages)\n\tbinary.BigEndian.PutUint64(buf[36:44], m.Checksum) // This will be 0\n\tbinary.BigEndian.PutUint32(buf[44:48], m.CurrentMetaIdx)\n\tbinary.BigEndian.PutUint64(buf[48:56], m.TxID)\n\n\treturn uint64(crc32.ChecksumIEEE(buf))\n}\n\n// ValidateChecksum validates the checksum of the Meta struct.\nfunc (m *Meta) ValidateChecksum() bool {\n\tchecksum := m.Checksum\n\tcalculated := m.CalculateChecksum()\n\tm.Checksum = checksum // Restore original checksum\n\treturn checksum == calculated\n}\n\n// DB represents the memory-mapped database.\ntype DB struct {\n\tpath string\n\tfile *os.File\n\n\t// mmapLock protects access to mmapData and its associated file operations (remap).\n\t// Read-locks are held by transactions to ensure they work with a stable mmap.\n\t// Write-locks are held during remap operations.\n\tmmapLock sync.RWMutex\n\tmmapData []byte // The current memory-mapped region\n\n\t// meta represents the current database metadata.\n\t// It is a *copy* of the on-disk meta, updated during write transactions.\n\t// Access to this field is protected by mmapLock.\n\tmeta *Meta\n\n\t// activeTxns tracks the number of active transactions.\n\t// Used during remap to wait for old transactions to complete.\n\tactiveTxns sync.WaitGroup\n\n\t// txnIDCounter generates unique IDs for transactions.\n\ttxnIDCounter uint64\n\n\t// remapCond is used to signal and wait for remap operations.\n\t// It's associated with mmapLock.\n\tremapCond *sync.Cond\n\n\t// isRemapping is a flag indicating if a remap operation is in progress.\n\t// Protected by mmapLock.\n\tisRemapping bool\n\n\t// closed indicates if the database is closed.\n\tclosed bool\n\tmu     sync.Mutex // Protects 'closed' field.\n}\n\n// Open opens a database file at the given path. If the file does not exist, it is created.\nfunc Open(path string) (*DB, error) {\n\tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to open database file: %w\", err)\n\t}\n\n\tdb := &DB{\n\t\tpath: file.Name(),\n\t\tfile: file,\n\t}\n\tdb.remapCond = sync.NewCond(&db.mmapLock) // Condition associated with the write-part of RWMutex\n\n\t// Initialize database file if it's new/empty\n\tfi, err := file.Stat()\n\tif err != nil {\n\t\t_ = db.Close() // Attempt to clean up\n\t\treturn nil, fmt.Errorf(\"failed to stat database file: %w\", err)\n\t}\n\n\tcurrentFileSize := uint64(fi.Size())\n\tif currentFileSize < uint64(DataOffset) { // Ensure enough space for meta pages + some initial data\n\t\tif err := db.initFile(); err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, fmt.Errorf(\"failed to initialize database file: %w\", err)\n\t\t}\n\t\tcurrentFileSize = InitialDBSize\n\t} else if currentFileSize < InitialDBSize {\n\t\t// If the file exists but is smaller than our initial requirement, expand it.\n\t\t// This handles cases where it was previously initialized but then truncated.\n\t\tdb.mmapLock.Lock() // Acquire lock for expansion\n\t\tdefer db.mmapLock.Unlock()\n\t\tif err := db.expandFile(InitialDBSize); err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, fmt.Errorf(\"failed to expand database file to initial size: %w\", err)\n\t\t}\n\t\tcurrentFileSize = InitialDBSize\n\t}\n\n\t// Initial mmap\n\tdb.mmapLock.Lock() // Need a write lock to initialize mmapData\n\tdefer db.mmapLock.Unlock()\n\tif err := db.mmap(int(currentFileSize)); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, fmt.Errorf(\"failed to mmap database file: %w\", err)\n\t}\n\n\t// Load metadata\n\tif err := db.loadMeta(); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, fmt.Errorf(\"failed to load metadata: %w\", err)\n\t}\n\n\treturn db, nil\n}\n\n// initFile initializes a new database file with default metadata.\n// This function must acquire the db.mmapLock (write lock) as it modifies the file system and mmapData.\nfunc (db *DB) initFile() error {\n\tdb.mmapLock.Lock()\n\tdefer db.mmapLock.Unlock()\n\n\t// Truncate to initial size\n\tif err := db.file.Truncate(InitialDBSize); err != nil {\n\t\treturn fmt.Errorf(\"failed to truncate file for initialization: %w\", err)\n\t}\n\n\t// Mmap the initial file size\n\tif db.mmapData != nil { // Unmap existing if any, though unlikely on initFile\n\t\tif err := unix.Munmap(db.mmapData); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to munmap old region during init: %w\", err)\n\t\t}\n\t}\n\tvar err error\n\tdb.mmapData, err = unix.Mmap(int(db.file.Fd()), 0, InitialDBSize, unix.PROT_READ|unix.PROT_WRITE, unix.MAP_SHARED)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to mmap file for initialization: %w\", err)\n\t}\n\n\tdb.meta = &Meta{\n\t\tVersion:        1,\n\t\tPageSize:       DefaultPageSize,\n\t\tFreeListHead:   0, // No free pages initially\n\t\tFileSize:       InitialDBSize,\n\t\tNumPages:       uint64(InitialDBSize-DataOffset) / uint64(DefaultPageSize), // Fix type mismatch\n\t\tTxID:           0,\n\t\tCurrentMetaIdx: 0, // Start with meta page 0\n\t}\n\n\t// Write initial metadata to both meta pages for redundancy\n\t// We increment TxID to differentiate the two meta pages if both are valid\n\tif err := db.writeMeta(db.meta, 0); err != nil {\n\t\treturn fmt.Errorf(\"failed to write initial meta page 0: %w\", err)\n\t}\n\tdb.meta.CurrentMetaIdx = 1 // Switch to next meta page for next write\n\tdb.meta.TxID++             // Increment TxID for the second meta page\n\tif err := db.writeMeta(db.meta, 1); err != nil {\n\t\treturn fmt.Errorf(\"failed to write initial meta page 1: %w\", err)\n\t}\n\n\t// Sync to disk\n\tif err := unix.Msync(db.mmapData, unix.MS_SYNC); err != nil {\n\t\treturn fmt.Errorf(\"failed to sync initial metadata: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// loadMeta loads the metadata from the database file.\n// It reads both meta pages, validates their checksums, and uses the most recent valid one.\n// Assumes db.mmapLock (write lock) is held.\nfunc (db *DB) loadMeta() error {\n\tvar meta0, meta1 Meta\n\t// Fix type mismatch: `MetaPage1Offset` is int, `metaMarshalSize` is uint64\n\tif len(db.mmapData) < int(uint64(MetaPage1Offset)+metaMarshalSize) {\n\t\treturn ErrFileSizeTooSmall\n\t}\n\n\t// Read meta page 0\n\tbuf0 := db.mmapData[MetaPage0Offset : MetaPage0Offset+metaMarshalSize]\n\t// Use unsafe.Pointer for direct struct interpretation, which is fast but requires care.\n\t// For production-grade, consider using `encoding/binary` for portability.\n\tmeta0Ptr := (*Meta)(unsafe.Pointer(&buf0[0]))\n\tmeta0 = *meta0Ptr // Copy the meta data\n\n\t// Read meta page 1\n\tbuf1 := db.mmapData[MetaPage1Offset : MetaPage1Offset+metaMarshalSize]\n\tmeta1Ptr := (*Meta)(unsafe.Pointer(&buf1[0]))\n\tmeta1 = *meta1Ptr // Copy the meta data\n\n\tvar valid0, valid1 bool\n\tif meta0.ValidateChecksum() {\n\t\tvalid0 = true\n\t} else {\n\t\tfmt.Printf(\"Warning: Meta page 0 checksum invalid (txid %d). File: %s\\n\", meta0.TxID, db.path)\n\t}\n\tif meta1.ValidateChecksum() {\n\t\tvalid1 = true\n\t} else {\n\t\tfmt.Printf(\"Warning: Meta page 1 checksum invalid (txid %d). File: %s\\n\", meta1.TxID, db.path)\n\t}\n\n\tswitch {\n\tcase valid0 && valid1:\n\t\t// Both are valid, pick the one with the higher TxID (most recent)\n\t\tif meta0.TxID > meta1.TxID {\n\t\t\tdb.meta = &meta0\n\t\t} else {\n\t\t\tdb.meta = &meta1\n\t\t}\n\tcase valid0:\n\t\tdb.meta = &meta0\n\tcase valid1:\n\t\tdb.meta = &meta1\n\tdefault:\n\t\treturn ErrInvalidMeta // Neither meta page is valid\n\t}\n\n\t// Ensure the database file size is consistent with the loaded meta.\n\tif uint64(len(db.mmapData)) < db.meta.FileSize {\n\t\treturn fmt.Errorf(\"mmap data size (%d) is smaller than meta.FileSize (%d)\", len(db.mmapData), db.meta.FileSize)\n\t}\n\treturn nil\n}\n\n// writeMeta writes the given metadata to the specified meta page index (0 or 1).\n// Assumes db.mmapLock.Lock() is held.\nfunc (db *DB) writeMeta(meta *Meta, idx int) error {\n\tmeta.Checksum = meta.CalculateChecksum() // Update checksum before writing\n\n\toffset := MetaPage0Offset\n\tif idx == 1 {\n\t\toffset = MetaPage1Offset\n\t}\n\n\t// Fix type mismatch: `offset` is int, `metaMarshalSize` is uint64\n\tif len(db.mmapData) < int(uint64(offset)+metaMarshalSize) {\n\t\treturn fmt.Errorf(\"mmap region too small to write meta page %d\", idx)\n\t}\n\n\t// Directly write the meta struct to the mmapData.\n\tsrcPtr := unsafe.Pointer(meta)\n\tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n\tfor i := 0; i < int(metaMarshalSize); i++ {\n\t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n\t}\n\treturn nil\n}\n\n// Close closes the database.\nfunc (db *DB) Close() error {\n\tdb.mu.Lock()\n\tif db.closed {\n\t\tdb.mu.Unlock()\n\t\treturn ErrDatabaseClosed\n\t}\n\tdb.closed = true\n\tdb.mu.Unlock()\n\n\t// Wait for all active transactions to complete before unmapping/closing\n\tfmt.Println(\"Waiting for active transactions to complete before closing DB...\")\n\tdb.activeTxns.Wait()\n\tfmt.Println(\"All active transactions completed, proceeding with DB close.\")\n\n\tdb.mmapLock.Lock() // Acquire write lock to ensure no new mmap operations or transactions start\n\tdefer db.mmapLock.Unlock()\n\n\tvar err error\n\t// Flush changes to disk before unmapping\n\tif db.mmapData != nil {\n\t\tif syncErr := unix.Msync(db.mmapData, unix.MS_SYNC); syncErr != nil {\n\t\t\terr = errors.Join(err, fmt.Errorf(\"failed to sync mmap data: %w\", syncErr))\n\t\t}\n\t\tif munmapErr := unix.Munmap(db.mmapData); munmapErr != nil {\n\t\t\terr = errors.Join(err, fmt.Errorf(\"failed to unmap data: %w\", munmapErr))\n\t\t}\n\t\tdb.mmapData = nil\n\t}\n\n\tif closeErr := db.file.Close(); closeErr != nil {\n\t\terr = errors.Join(err, fmt.Errorf(\"failed to close file: %w\", closeErr))\n\t}\n\n\treturn err\n}\n\n// mmap maps the database file into memory.\n// Assumes db.mmapLock is held (either RLock for initial mmap or Lock for remap).\nfunc (db *DB) mmap(size int) error {\n\tif db.mmapData != nil {\n\t\tif err := unix.Munmap(db.mmapData); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to munmap old region: %w\", err)\n\t\t}\n\t}\n\n\tvar err error\n\tdb.mmapData, err = unix.Mmap(int(db.file.Fd()), 0, size, unix.PROT_READ|unix.PROT_WRITE, unix.MAP_SHARED)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to mmap file: %w\", err)\n\t}\n\treturn nil\n}\n\n// expandFile expands the database file to a new size and remaps it.\n// This function requires an exclusive write lock on db.mmapLock.\n// Assumes db.mmapLock is already held.\nfunc (db *DB) expandFile(newSize uint64) error {\n\tif newSize <= uint64(len(db.mmapData)) {\n\t\treturn nil // No expansion needed or shrinking is not supported here.\n\t}\n\n\t// Round up to nearest page size\n\tif newSize%uint64(db.meta.PageSize) != 0 {\n\t\tnewSize = (newSize/uint64(db.meta.PageSize) + 1) * uint64(db.meta.PageSize)\n\t}\n\n\tfmt.Printf(\"Remapping DB: Old size %d, New size %d, File: %s\\n\", len(db.mmapData), newSize, db.path)\n\n\tdb.isRemapping = true\n\t// Wake up any new transactions waiting on remap, so they retry after it's done.\n\tdb.remapCond.Broadcast()\n\n\t// Wait for all existing transactions to complete.\n\t// This is the crucial step to ensure no one is accessing the old mmapData.\n\tfmt.Println(\"Waiting for all active transactions to complete before remap...\")\n\tdb.activeTxns.Wait()\n\tfmt.Println(\"All active transactions completed, proceeding with remap.\")\n\n\t// Truncate the file to the new size\n\tif err := db.file.Truncate(int64(newSize)); err != nil {\n\t\tdb.isRemapping = false\n\t\tdb.remapCond.Broadcast() // Re-broadcast in case of error\n\t\treturn fmt.Errorf(\"failed to truncate file to new size %d: %w\", newSize, err)\n\t}\n\n\t// Mmap the new region\n\tif err := db.mmap(int(newSize)); err != nil {\n\t\tdb.isRemapping = false\n\t\tdb.remapCond.Broadcast() // Re-broadcast in case of error\n\t\treturn fmt.Errorf(\"failed to mmap new file region of size %d: %w\", newSize, err)\n\t}\n\n\tdb.meta.FileSize = newSize\n\t// Fix type mismatch: DataOffset is int, db.meta.PageSize is uint32\n\tdb.meta.NumPages = (newSize - uint64(DataOffset)) / uint64(db.meta.PageSize) \n\tdb.isRemapping = false\n\tdb.remapCond.Broadcast() // Signal that remap is complete and new map is ready\n\n\tfmt.Printf(\"DB remapped successfully to size %d, File: %s\\n\", newSize, db.path)\n\treturn nil\n}\n\n// Begin starts a new transaction.\n// If writable is true, a read-write transaction is started. Otherwise, a read-only transaction.\nfunc (db *DB) Begin(writable bool) (Transaction, error) {\n\tdb.mu.Lock()\n\tif db.closed {\n\t\tdb.mu.Unlock()\n\t\treturn nil, ErrDatabaseClosed\n\t}\n\tdb.mu.Unlock()\n\n\ttxID := atomic.AddUint64(&db.txnIDCounter, 1)\n\tdb.activeTxns.Add(1) // Increment active transaction counter\n\n\tif writable {\n\t\tdb.mmapLock.Lock() // Acquire exclusive write lock for R/W transaction\n\t\t// A read-write transaction holds the database-wide write lock for its entire duration\n\t\t// (from Begin to Commit/Rollback). This ensures only one R/W transaction at a time.\n\t\t// It also protects against concurrent remap operations.\n\n\t\t// If a remap is in progress when a R/W transaction tries to begin, it must wait.\n\t\tfor db.isRemapping {\n\t\t\tdb.mmapLock.Unlock() // Release lock to allow remap to proceed\n\t\t\tdb.remapCond.L.Lock()\n\t\t\tif db.isRemapping { // Double check after acquiring cond mutex\n\t\t\t\tdb.remapCond.Wait() // Wait for remap to finish\n\t\t\t}\n\t\t\tdb.remapCond.L.Unlock()\n\t\t\tdb.mmapLock.Lock() // Re-acquire write lock\n\t\t}\n\n\t\t// After acquiring write lock and ensuring no remap, take a snapshot of meta and mmap.\n\t\tmetaCopy := *db.meta\n\t\tmmapSnapshot := db.mmapData\n\n\t\treturn &ReadWriteTx{\n\t\t\tbaseTx: baseTx{\n\t\t\t\tid:       txID,\n\t\t\t\tdb:       db,\n\t\t\t\tmmapData: mmapSnapshot, // Points to current mmap\n\t\t\t\tdone:     false,\n\t\t\t},\n\t\t\tmeta:          metaCopy,\n\t\t\tpendingWrites: make(map[uint64][]byte),\n\t\t\tnewFileSize:   db.meta.FileSize,\n\t\t}, nil\n\t} else {\n\t\tdb.mmapLock.RLock() // Acquire read lock for R/O transaction\n\n\t\t// If a remap is in progress, release read lock and wait.\n\t\tfor db.isRemapping {\n\t\t\tdb.mmapLock.RUnlock()\n\t\t\tdb.remapCond.L.Lock()\n\t\t\tif db.isRemapping { // Double check after acquiring cond mutex\n\t\t\t\tdb.remapCond.Wait() // Wait for remap to finish\n\t\t\t}\n\t\t\tdb.remapCond.L.Unlock()\n\t\t\tdb.mmapLock.RLock() // Re-acquire read lock for a new attempt\n\t\t}\n\t\t// After acquiring read lock and ensuring no remap, take a snapshot of meta and mmap.\n\t\tmetaCopy := *db.meta\n\t\tmmapSnapshot := db.mmapData\n\t\t\n\t\t// The RLock needs to be held until the transaction is done to prevent the underlying mmapData from being unmapped.\n\t\t// However, the activeTxns.Wait() in expandFile already ensures this.\n\t\t// Releasing the RLock here is okay, as activeTxns protects the underlying memory until all transactions complete.\n\t\tdefer db.mmapLock.RUnlock() // Release read lock after transaction creation.\n\n\t\treturn &ReadOnlyTx{\n\t\t\tbaseTx: baseTx{\n\t\t\t\tid:       txID,\n\t\t\t\tdb:       db,\n\t\t\t\tmmapData: mmapSnapshot, // Points to current mmap slice\n\t\t\t\tdone:     false,\n\t\t\t},\n\t\t\tsnapshotMeta: metaCopy,\n\t\t}, nil\n\t}\n}\n\n// Transaction interface for database operations.\ntype Transaction interface {\n\tID() uint64\n\tRead(offset, size uint64) ([]byte, error)\n\tPage(pageID uint64) ([]byte, error)\n\tCommit() error\n\tRollback() error\n\tIsWritable() bool\n}\n\n// baseTx provides common fields and methods for both transaction types.\ntype baseTx struct {\n\tid       uint64\n\tdb       *DB\n\tmmapData []byte // A snapshot of the mmapData when the transaction started\n\tdone     bool   // True if the transaction is committed or rolled back\n\tmu       sync.Mutex // Protects 'done'\n}\n\nfunc (b *baseTx) ID() uint64 {\n\treturn b.id\n}\n\n// doneTx cleans up common transaction resources.\nfunc (b *baseTx) doneTx() {\n\tb.mu.Lock()\n\tif !b.done {\n\t\tb.done = true\n\t\tb.db.activeTxns.Done() // Decrement active transaction count\n\n\t\t// If this was a ReadWriteTx, its db.mmapLock.Lock() needs to be released.\n\t\t// This is handled by a defer in ReadWriteTx.Commit/Rollback.\n\t}\n\tb.mu.Unlock()\n}\n\nfunc (b *baseTx) Read(offset, size uint64) ([]byte, error) {\n\tb.mu.Lock()\n\tif b.done {\n\t\tb.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\tb.mu.Unlock()\n\n\tif offset+size > uint64(len(b.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"read out of bounds: offset %d, size %d, mmap size %d\", offset, size, len(b.mmapData))\n\t}\n\treturn b.mmapData[offset : offset+size], nil\n}\n\n// ReadOnlyTx represents a read-only database transaction.\ntype ReadOnlyTx struct {\n\tbaseTx\n\tsnapshotMeta Meta // A copy of the metadata at the time of transaction creation.\n}\n\n// IsWritable returns false for a read-only transaction.\nfunc (tx *ReadOnlyTx) IsWritable() bool {\n\treturn false\n}\n\n// Page returns a byte slice representing the data for a given page ID for a ReadOnlyTx.\nfunc (tx *ReadOnlyTx) Page(pageID uint64) ([]byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.snapshotMeta.PageSize)\n\n\tif pageID >= tx.snapshotMeta.NumPages {\n\t\treturn nil, ErrPageNotFound\n\t}\n\n\toffset := uint64(DataOffset) + pageID*pageSize // Fix type mismatch: DataOffset is int\n\tif offset+pageSize > uint64(len(tx.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"page %d (offset %d) out of mmap bounds\", pageID, offset)\n\t}\n\treturn tx.mmapData[offset : offset+pageSize], nil\n}\n\n// Commit attempts to commit the read-only transaction.\n// For read-only transactions, this effectively just marks it as done.\nfunc (tx *ReadOnlyTx) Commit() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\ttx.doneTx()\n\treturn nil\n}\n\n// Rollback rolls back the read-only transaction.\n// For read-only transactions, this effectively just marks it as done.\nfunc (tx *ReadOnlyTx) Rollback() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\ttx.doneTx()\n\treturn nil\n}\n\n// ReadWriteTx represents a read-write database transaction.\ntype ReadWriteTx struct {\n\tbaseTx\n\tmeta          Meta              // A mutable copy of the metadata for this transaction\n\tpendingWrites map[uint64][]byte // Offset -> Data to write\n\tnewFileSize   uint64            // Tracks the required file size\n}\n\n// IsWritable returns true for a read-write transaction.\nfunc (tx *ReadWriteTx) IsWritable() bool {\n\treturn true\n}\n\n// Page returns a byte slice representing the data for a given page ID for a ReadWriteTx.\nfunc (tx *ReadWriteTx) Page(pageID uint64) ([]byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.meta.PageSize)\n\n\tif pageID >= tx.meta.NumPages {\n\t\treturn nil, ErrPageNotFound\n\t}\n\n\toffset := uint64(DataOffset) + pageID*pageSize // Fix type mismatch: DataOffset is int\n\tif offset+pageSize > uint64(len(tx.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"page %d (offset %d) out of mmap bounds\", pageID, offset)\n\t}\n\n\t// First check if there's a pending write for this page.\n\t// If a transaction writes to a page it just allocated, it should see its own write.\n\t// This simple implementation checks the starting offset. A more robust one might\n\t// need to handle partial page writes or page-level buffering.\n\tif data, ok := tx.pendingWrites[offset]; ok {\n\t\treturn data, nil\n\t}\n\n\treturn tx.mmapData[offset : offset+pageSize], nil\n}\n\n// Write writes data to the specified offset.\n// Data is buffered until Commit.\nfunc (tx *ReadWriteTx) Write(offset uint64, data []byte) error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tif offset+uint64(len(data)) > tx.newFileSize {\n\t\ttx.newFileSize = offset + uint64(len(data))\n\t}\n\ttx.pendingWrites[offset] = data\n\treturn nil\n}\n\n// AllocatePage allocates a new page and returns its ID and a byte slice to its data.\n// It will update `tx.newFileSize` and `tx.meta.NumPages` accordingly.\n// The returned slice is a temporary buffer for the caller to fill; actual write happens on Commit.\nfunc (tx *ReadWriteTx) AllocatePage(size uint64) (uint64, []byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn 0, nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.meta.PageSize)\n\tif size > pageSize {\n\t\treturn 0, nil, ErrInsufficientSpace // Can't allocate a page larger than PageSize\n\t}\n\n\tpageID := tx.meta.NumPages // New page ID is current NumPages\n\trequiredSize := uint64(DataOffset) + (pageID+1)*pageSize // Fix type mismatch: DataOffset is int\n\n\tif requiredSize > tx.newFileSize {\n\t\ttx.newFileSize = requiredSize\n\t}\n\n\ttx.meta.NumPages++ // Update transaction's meta for new page count\n\n\t// Return a temporary buffer for the caller to write into.\n\t// On commit, this data will be transferred from `pendingWrites` to `mmapData`.\n\ttempBuf := make([]byte, pageSize)\n\toffset := uint64(DataOffset) + pageID*pageSize // Fix type mismatch: DataOffset is int\n\ttx.pendingWrites[offset] = tempBuf // Buffer for the page data\n\treturn pageID, tempBuf, nil\n}\n\n// Commit commits the read-write transaction.\nfunc (tx *ReadWriteTx) Commit() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tdefer tx.db.mmapLock.Unlock() // Release the DB's write lock that was acquired in Begin\n\tdefer tx.doneTx()             // Ensure doneTx is called even on error\n\n\t// The mmapLock.Lock() was acquired by db.Begin(true) and is held throughout the R/W transaction.\n\t// This ensures that for the commit, no other transactions are active and no remapping is happening.\n\n\t// Step 1: Handle file expansion if needed\n\tif tx.newFileSize > tx.db.meta.FileSize {\n\t\tif err := tx.db.expandFile(tx.newFileSize); err != nil {\n\t\t\treturn fmt.Errorf(\"%w: failed to expand file during commit: %v\", ErrCommitFailed, err)\n\t\t}\n\t\t// Update our internal mmapData reference after expansion\n\t\ttx.mmapData = tx.db.mmapData\n\t} else if tx.newFileSize < tx.db.meta.FileSize {\n\t\t// If tx.newFileSize is smaller, it means we might have released pages.\n\t\t// For now, we only support growth, not shrinking the file on commit.\n\t\t// A full implementation would handle file shrinking (carefully).\n\t\ttx.newFileSize = tx.db.meta.FileSize // Revert to current file size if attempting to shrink\n\t}\n\n\t// Step 2: Apply pending writes to the mmapData\n\tfor offset, data := range tx.pendingWrites {\n\t\tif offset+uint64(len(data)) > uint64(len(tx.db.mmapData)) {\n\t\t\treturn fmt.Errorf(\"%w: write out of mmap bounds after expansion (offset %d, len %d, mmap size %d)\",\n\t\t\t\tErrCommitFailed, offset, len(data), len(tx.db.mmapData))\n\t\t}\n\t\tcopy(tx.db.mmapData[offset:], data)\n\t}\n\n\t// Step 3: Update database metadata\n\ttx.db.meta.Version = tx.meta.Version\n\ttx.db.meta.PageSize = tx.meta.PageSize\n\ttx.db.meta.FreeListHead = tx.meta.FreeListHead\n\ttx.db.meta.FileSize = tx.newFileSize // This should now be consistent\n\ttx.db.meta.NumPages = tx.meta.NumPages\n\ttx.db.meta.TxID = tx.id // Set the database's last committed TxID\n\n\t// Write the updated metadata to the *next* meta page.\n\t// This provides atomic metadata updates (two-phase commit for meta).\n\ttargetMetaIdx := (tx.db.meta.CurrentMetaIdx + 1) % 2\n\ttx.db.meta.CurrentMetaIdx = targetMetaIdx // Update the in-memory meta\n\tif err := tx.db.writeMeta(tx.db.meta, int(targetMetaIdx)); err != nil {\n\t\treturn fmt.Errorf(\"%w: failed to write meta page %d: %v\", ErrCommitFailed, targetMetaIdx, err)\n\t}\n\n\t// Step 4: Flush changes to disk\n\tif err := unix.Msync(tx.db.mmapData, unix.MS_SYNC); err != nil {\n\t\treturn fmt.Errorf(\"%w: failed to sync mmap data to disk: %v\", ErrCommitFailed, err)\n\t}\n\n\tfmt.Printf(\"Transaction %d committed successfully. DB FileSize: %d, NumPages: %d, File: %s\\n\", tx.id, tx.db.meta.FileSize, tx.db.meta.NumPages, tx.db.path)\n\n\treturn nil\n}\n\n// Rollback rolls back the read-write transaction.\nfunc (tx *ReadWriteTx) Rollback() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tdefer tx.db.mmapLock.Unlock() // Release the DB's write lock\n\tdefer tx.doneTx()             // Ensure doneTx is called\n\n\t// For a read-write transaction, rollback just discards pending writes.\n\ttx.pendingWrites = nil\n\tfmt.Printf(\"Transaction %d rolled back. File: %s\\n\", tx.id, tx.db.path)\n\treturn nil\n}\n\n// Example usage and demonstration of concurrency.\nfunc main() {\n\tdbPath := filepath.Join(os.TempDir(), \"mydata.db\")\n\t_ = os.Remove(dbPath) // Clean up any previous database file\n\n\tfmt.Printf(\"Database file: %s\\n\", dbPath)\n\n\tdb, err := Open(dbPath)\n\tif err != nil {\n\t\tfmt.Printf(\"Error opening database: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif closeErr := db.Close(); closeErr != nil {\n\t\t\tfmt.Printf(\"Error closing database: %v\\n\", closeErr)\n\t\t}\n\t}()\n\n\t// --- Test 1: Initial write and read ---\n\tfmt.Println(\"\\n--- Test 1: Initial write and read ---\")\n\trwTx, err := db.Begin(true)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RW transaction: %v\\n\", err)\n\t\treturn\n\t}\n\n\t// Allocate a page and write some data\n\tpageID, pageDataBuf, err := rwTx.(*ReadWriteTx).AllocatePage(DefaultPageSize)\n\tif err != nil {\n\t\tfmt.Printf(\"Error allocating page: %v\\n\", err)\n\t\trwTx.Rollback()\n\t\treturn\n\t}\n\tcopy(pageDataBuf, []byte(\"Hello, DB World from Page 0!\"))\n\n\t// Note: With AllocatePage buffering, `tx.Write` call for `pageDataBuf` is implicit.\n\t// If AllocatePage returns a direct slice to mmap, then `tx.Write` would be explicit.\n\n\tif err := rwTx.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RW transaction: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Initial data committed.\")\n\n\t// Read data back\n\troTx, err := db.Begin(false)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RO transaction: %v\\n\", err)\n\t\treturn\n\t}\n\treadPage0Data, err := roTx.Page(0)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading page 0: %v\\n\", err)\n\t\troTx.Rollback()\n\t\treturn\n\t}\n\tfmt.Printf(\"Read from Page 0: %s\\n\", string(readPage0Data[:len(\"Hello, DB World from Page 0!\")]))\n\troTx.Commit()\n\n\t// --- Test 2: Concurrent read-only transactions ---\n\tfmt.Println(\"\\n--- Test 2: Concurrent read-only transactions ---\")\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 5; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\ttx, err := db.Begin(false)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"RO Tx %d: Error beginning transaction: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\t\tfmt.Printf(\"RO Tx %d: Error committing: %v\\n\", id, err)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\treadData, err := tx.Page(0)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"RO Tx %d: Error reading page 0: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfmt.Printf(\"RO Tx %d: Read data: %s\\n\", id, string(readData[:len(\"Hello, DB World from Page 0!\")]))\n\t\t\ttime.Sleep(100 * time.Millisecond) // Simulate work\n\t\t}(i)\n\t}\n\twg.Wait()\n\tfmt.Println(\"All concurrent RO transactions completed.\")\n\n\t// --- Test 3: Read-write transaction with expansion and concurrent RO access ---\n\tfmt.Println(\"\\n--- Test 3: Read-write transaction with expansion and concurrent RO access ---\")\n\n\t// Start some background RO transactions that will be active during the RW commit.\n\tvar concurrentROwg sync.WaitGroup\n\tfor i := 0; i < 3; i++ {\n\t\tconcurrentROwg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer concurrentROwg.Done()\n\t\t\ttx, err := db.Begin(false)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error beginning transaction: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error committing: %v\\n\", id, err)\n\t\t\t\t}\n\t\t\t}()\n\t\t\t// Keep transaction active for a bit to ensure it is running during remap.\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t\treadData, err := tx.Page(0)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error reading page 0: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfmt.Printf(\"Background RO Tx %d: Read data: %s\\n\", id, string(readData[:len(\"Hello, DB World from Page 0!\")]))\n\t\t}(i)\n\t}\n\n\t// Give a moment for background RO transactions to start\n\ttime.Sleep(100 * time.Millisecond)\n\n\t// Start a R/W transaction that expands the file\n\tfmt.Println(\"Starting R/W transaction that will expand the database.\")\n\trwTx2, err := db.Begin(true)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RW transaction 2: %v\\n\", err)\n\t\treturn\n\t}\n\n\t// Allocate a new page, which should trigger file expansion\n\tnewPageID := rwTx2.(*ReadWriteTx).meta.NumPages // Get current number of pages for the new page ID from TX's meta\n\tnewPageDataBuf, _, err := rwTx2.(*ReadWriteTx).AllocatePage(DefaultPageSize)\n\tif err != nil {\n\t\tfmt.Printf(\"Error allocating new page for expansion: %v\\n\", err)\n\t\trwTx2.Rollback()\n\t\treturn\n\t}\n\tcopy(newPageDataBuf, []byte(\"This is new data in an expanded page!\"))\n\n\t// Update page 0 as well to show a write to an existing page\n\tupdatedPage0Data := []byte(\"UPDATED: Hello, DB World from Page 0!\")\n\tif err := rwTx2.(*ReadWriteTx).Write(DataOffset, updatedPage0Data); err != nil {\n\t\tfmt.Printf(\"Error writing updated page 0 data: %v\\n\", err)\n\t\trwTx2.Rollback()\n\t\treturn\n\t}\n\n\tfmt.Println(\"R/W transaction 2 attempting to commit and expand file...\")\n\tif err := rwTx2.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RW transaction 2: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"RW transaction 2 committed successfully, file expanded.\")\n\n\t// Wait for background RO transactions to finish\n\tconcurrentROwg.Wait()\n\tfmt.Println(\"All background RO transactions finished.\")\n\n\t// --- Test 4: Verify expanded data and new RO txns use new map ---\n\tfmt.Println(\"\\n--- Test 4: Verify expanded data and new RO txns use new map ---\")\n\tfinalRoTx, err := db.Begin(false)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning final RO transaction: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif err := finalRoTx.Commit(); err != nil {\n\t\t\tfmt.Printf(\"Error committing final RO: %v\\n\", err)\n\t\t}\n\t}()\n\n\treadUpdatedPage0Data, err := finalRoTx.Page(0)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading updated page 0: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Printf(\"Final RO Tx: Read from Page 0 (updated): %s\\n\", string(readUpdatedPage0Data[:len(updatedPage0Data)]))\n\n\treadNewPageData, err := finalRoTx.Page(newPageID)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading new page %d: %v\\n\", newPageID, err)\n\t\treturn\n\t}\n\tfmt.Printf(\"Final RO Tx: Read from new Page %d: %s\\n\", newPageID, string(readNewPageData[:len(\"This is new data in an expanded page!\")]))\n\n\t// Demonstrate a new transaction trying to start during a (simulated) remap\n\tfmt.Println(\"\\n--- Test 5: New transaction waiting on remap ---\")\n\t// Simulate a remap by manually locking and setting isRemapping\n\tdb.mmapLock.Lock() // Acquire DB write lock\n\tdb.isRemapping = true\n\tfmt.Println(\"Simulating remapping in progress...\")\n\n\tvar waitingTxErr error\n\tvar waitingTxWg sync.WaitGroup\n\twaitingTxWg.Add(1)\n\tgo func() {\n\t\tdefer waitingTxWg.Done()\n\t\tfmt.Println(\"Attempting to begin a new RO transaction during simulated remap...\")\n\t\ttx, err := db.Begin(false) // This call should block\n\t\tif err != nil {\n\t\t\twaitingTxErr = err\n\t\t\tfmt.Printf(\"Waiting RO Tx: Error beginning transaction: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer func() {\n\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\tfmt.Printf(\"Waiting RO Tx: Error committing: %v\\n\", err)\n\t\t\t}\n\t\t}()\n\t\tfmt.Println(\"Waiting RO Tx: Successfully started after remap completion.\")\n\t}()\n\n\ttime.Sleep(500 * time.Millisecond) // Give the waiting goroutine a chance to hit the wait condition\n\tfmt.Println(\"Releasing simulated remap...\")\n\tdb.isRemapping = false\n\tdb.remapCond.Broadcast() // Signal that remap is done\n\tdb.mmapLock.Unlock()     // Release the lock so the waiting tx can proceed\n\n\twaitingTxWg.Wait()\n\tif waitingTxErr != nil {\n\t\tfmt.Printf(\"Waiting transaction failed: %v\\n\", waitingTxErr)\n\t} else {\n\t\tfmt.Println(\"Waiting transaction successfully proceeded after remap.\")\n\t}\n\n\tfmt.Println(\"\\nDatabase operations complete.\")\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "go build -race",
                    "error": "# command-line-arguments\n./main.go:815:2: declared and not used: pageID\n./main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)"
                },
                {
                    "verifier": "gosec",
                    "error": "Results:\n\nGolang errors in file: []:\n\n  > [line 0 : column 0] - # command-line-arguments\n./main.go:815:2: declared and not used: pageID\n./main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n\n\nGolang errors in file: [/tmp/go_synthesis_4z44__2e/main.go]:\n\n  > [line 815 : column 2] - declared and not used: pageID\n\n  > [line 926 : column 7] - invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n\n\n\n[\u001b[30;43m/tmp/go_synthesis_4z44__2e/main.go:132\u001b[0m] - G304 (CWE-22): Potential file inclusion via variable (Confidence: HIGH, Severity: MEDIUM)\n    131: func Open(path string) (*DB, error) {\n  > 132: \tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n    133: \tif err != nil {\n\nAutofix: Consider using os.Root to scope file access under a fixed root (Go >=1.24). Prefer root.Open/root.Stat over os.Open/os.Stat to prevent directory traversal.\n\n[\u001b[30;43m/tmp/go_synthesis_4z44__2e/main.go:132\u001b[0m] - G302 (CWE-276): Expect file permissions to be 0600 or less (Confidence: HIGH, Severity: MEDIUM)\n    131: func Open(path string) (*DB, error) {\n  > 132: \tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n    133: \tif err != nil {\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:314\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    313: \tfor i := 0; i < int(metaMarshalSize); i++ {\n  > 314: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n    315: \t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:314\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    313: \tfor i := 0; i < int(metaMarshalSize); i++ {\n  > 314: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n    315: \t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:312\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    311: \tsrcPtr := unsafe.Pointer(meta)\n  > 312: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n    313: \tfor i := 0; i < int(metaMarshalSize); i++ {\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:311\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    310: \t// Directly write the meta struct to the mmapData.\n  > 311: \tsrcPtr := unsafe.Pointer(meta)\n    312: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:257\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    256: \tbuf1 := db.mmapData[MetaPage1Offset : MetaPage1Offset+metaMarshalSize]\n  > 257: \tmeta1Ptr := (*Meta)(unsafe.Pointer(&buf1[0]))\n    258: \tmeta1 = *meta1Ptr // Copy the meta data\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:252\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    251: \t// For production-grade, consider using `encoding/binary` for portability.\n  > 252: \tmeta0Ptr := (*Meta)(unsafe.Pointer(&buf0[0]))\n    253: \tmeta0 = *meta0Ptr // Copy the meta data\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:932\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    931: \t\tfmt.Printf(\"Error writing updated page 0 data: %v\\n\", err)\n  > 932: \t\trwTx2.Rollback()\n    933: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:923\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    922: \t\tfmt.Printf(\"Error allocating new page for expansion: %v\\n\", err)\n  > 923: \t\trwTx2.Rollback()\n    924: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:845\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    844: \tfmt.Printf(\"Read from Page 0: %s\\n\", string(readPage0Data[:len(\"Hello, DB World from Page 0!\")]))\n  > 845: \troTx.Commit()\n    846: \n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:841\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    840: \t\tfmt.Printf(\"Error reading page 0: %v\\n\", err)\n  > 841: \t\troTx.Rollback()\n    842: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:818\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    817: \t\tfmt.Printf(\"Error allocating page: %v\\n\", err)\n  > 818: \t\trwTx.Rollback()\n    819: \t\treturn\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 1015\n  Nosec  : 0\n  Issues : \u001b[1;31m13\u001b[0m\n\n[gosec] 2025/12/01 13:23:25 Including rules: default\n[gosec] 2025/12/01 13:23:25 Excluding rules: default\n[gosec] 2025/12/01 13:23:25 Including analyzers: default\n[gosec] 2025/12/01 13:23:25 Excluding analyzers: default\n[gosec] 2025/12/01 13:23:25 Import directory: /tmp/go_synthesis_4z44__2e\n[gosec] 2025/12/01 13:23:25 Checking package: main\n[gosec] 2025/12/01 13:23:25 Checking file: /tmp/go_synthesis_4z44__2e/main.go\n[gosec] 2025/12/01 13:23:28 Panic when running SSA analyzer on package: main. Panic: interface conversion: types.Type is *types.Basic, not *types.Signature\nStack trace:\ngoroutine 1 [running]:\nruntime/debug.Stack()\n\t/usr/local/go/src/runtime/debug/stack.go:26 +0x5e\ngithub.com/securego/gosec/v2.(*Analyzer).buildSSA.func1()\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:501 +0x58\npanic({0xed8f20?, 0xc001aeb500?})\n\t/usr/local/go/src/runtime/panic.go:783 +0x132\ngolang.org/x/tools/go/ssa.(*builder).expr0(0xc0015af8c0, 0xc001a8fc00, {0x11e2648?, 0xc00145c4e0}, {0x2, {0x11df418, 0x19a55a0}, {0x0, 0x0}})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:813 +0x239c\ngolang.org/x/tools/go/ssa.(*builder).expr(0xc0015af8c0, 0xc001a8fc00, {0x11e2648, 0xc00145c4e0})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:641 +0x17b\ngolang.org/x/tools/go/ssa.(*builder).setCallFunc(0x78?, 0xf4d160?, 0x3a?, 0xc001af25c0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1066 +0x2b6\ngolang.org/x/tools/go/ssa.(*builder).setCall(0xc0015af8c0, 0xc001a8fc00, 0xc001459100, 0xc001af25c0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:1144 +0x2b\ngolang.org/x/tools/go/ssa.(*builder).expr0(0xc0015af8c0, 0xc001a8fc00, {0x11e2708?, 0xc001459100}, {0x0, {0x0, 0x0}, {0x0, 0x0}})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:724 +0x2105\ngolang.org/x/tools/go/ssa.(*builder).expr(0xc0015af8c0, 0xc001a8fc00, {0x11e2708, 0xc001459100})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:641 +0x17b\ngolang.org/x/tools/go/ssa.(*builder).stmt(0xc0015af8c0, 0xc001a8fc00, {0x11e2d00?, 0xc00144c990?})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2699 +0xf5f\ngolang.org/x/tools/go/ssa.(*builder).stmtList(...)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:972\ngolang.org/x/tools/go/ssa.(*builder).stmt(0xc0015af8c0, 0xc001a8fc00, {0x11e2eb0?, 0xc00145a750?})\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2753 +0x1109\ngolang.org/x/tools/go/ssa.(*builder).buildFromSyntax(0xc0015af8c0, 0xc001a8fc00)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2962 +0x25a\ngolang.org/x/tools/go/ssa.(*builder).buildFunction(0xf73000?, 0xc001a8fc00)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2912 +0x183\ngolang.org/x/tools/go/ssa.(*builder).iterate(0xc0015af8c0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:2897 +0x25\ngolang.org/x/tools/go/ssa.(*Package).build(0xc000e85000)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:3183 +0xce\nsync.(*Once).doSlow(0xc00016e240?, 0xc001412660?)\n\t/usr/local/go/src/sync/once.go:78 +0xac\nsync.(*Once).Do(...)\n\t/usr/local/go/src/sync/once.go:69\ngolang.org/x/tools/go/ssa.(*Package).Build(...)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/ssa/builder.go:3172\ngolang.org/x/tools/go/analysis/passes/buildssa.run(0xc0010f21c0)\n\t/home/fanbao/go/pkg/mod/golang.org/x/tools@v0.38.0/go/analysis/passes/buildssa/buildssa.go:59 +0x21d\ngithub.com/securego/gosec/v2.(*Analyzer).buildSSA(0x71a07b?, 0xc000cbeea0)\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:524 +0x1d3\ngithub.com/securego/gosec/v2.(*Analyzer).CheckAnalyzers(0xc0003b6980, 0xc000cbeea0)\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:415 +0x32\ngithub.com/securego/gosec/v2.(*Analyzer).Process(0xc0003b6980, {0x0, 0x0, 0x0}, {0xc0001ecd30, 0x1, 0x3d?})\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/analyzer.go:321 +0x487\nmain.main()\n\t/home/fanbao/go/pkg/mod/github.com/securego/gosec/v2@v2.22.10/cmd/gosec/main.go:474 +0xde7\n[gosec] 2025/12/01 13:23:28 Error building the SSA representation of the package main: no ssa result"
                },
                {
                    "verifier": "govulncheck",
                    "error": "govulncheck: loading packages: \nThere are errors with the provided package patterns:\n\n/tmp/go_synthesis_4z44__2e/main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n/tmp/go_synthesis_4z44__2e/main.go:815:2: declared and not used: pageID\n\nFor details on package patterns, see https://pkg.go.dev/cmd/go#hdr-Package_lists_and_patterns."
                },
                {
                    "verifier": "errcheck",
                    "error": "error: failed to check packages: errors while loading package command-line-arguments: [-: # command-line-arguments\n./main.go:815:2: declared and not used: pageID\n./main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64) /tmp/go_synthesis_4z44__2e/main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64) /tmp/go_synthesis_4z44__2e/main.go:815:2: declared and not used: pageID]"
                },
                {
                    "verifier": "go-errorlint",
                    "error": "-: # command-line-arguments\n./main.go:815:2: declared and not used: pageID\n./main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n/tmp/go_synthesis_4z44__2e/main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64)\n/tmp/go_synthesis_4z44__2e/main.go:815:2: declared and not used: pageID\nerrorlint: analysis skipped due to errors in package"
                },
                {
                    "verifier": "staticcheck",
                    "error": "-: # command-line-arguments\n./main.go:815:2: declared and not used: pageID\n./main.go:926:7: invalid copy: argument must be a slice; have newPageDataBuf (variable of type uint64) (compile)"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unsafe\"\n\n\tunix \"golang.org/x/sys/unix\" // For mmap, munmap, msync\n)\n\nconst (\n\t// DefaultPageSize is the size of a page in the database file.\n\tDefaultPageSize = 4096 // 4KB\n\n\t// MetaPageSize is the size of a metadata page.\n\tMetaPageSize = DefaultPageSize\n\n\t// InitialDBSize is the initial size of the database file.\n\t// It must be large enough to hold at least two meta pages and some initial data.\n\tInitialDBSize = 4 * DefaultPageSize // 2 meta pages + 2 data pages initially\n\n\t// MetaPage0Offset is the offset of the first meta page.\n\tMetaPage0Offset = 0\n\n\t// MetaPage1Offset is the offset of the second meta page.\n\tMetaPage1Offset = MetaPageSize\n\n\t// DataOffset is the offset where actual data pages begin.\n\tDataOffset = 2 * MetaPageSize\n)\n\nvar (\n\tErrTxClosed          = errors.New(\"transaction closed\")\n\tErrDatabaseClosed    = errors.New(\"database closed\")\n\tErrReadOnlyTx        = errors.New(\"cannot write in a read-only transaction\")\n\tErrPageNotFound      = errors.New(\"page not found\")\n\tErrInvalidMeta       = errors.New(\"invalid metadata page\")\n\tErrCommitFailed      = errors.New(\"transaction commit failed\")\n\tErrInsufficientSpace = errors.New(\"insufficient space for data\")\n\tErrFileSizeTooSmall  = errors.New(\"database file size is too small for metadata\")\n)\n\n// Meta represents the database's metadata.\n// It's a fixed-size structure that will be directly mapped to the database file.\n// Fields are ordered for direct binary serialization.\ntype Meta struct {\n\tVersion        uint64 // Database version\n\tPageSize       uint32 // Size of a data page\n\tFreeListHead   uint64 // Pointer to the head of the free list (page ID)\n\tFileSize       uint64 // Current total size of the database file\n\tNumPages       uint64 // Total number of data pages\n\tChecksum       uint64 // Checksum of the Meta data\n\tCurrentMetaIdx uint32 // Index of the currently active meta page (0 or 1)\n\tTxID           uint64 // Last committed transaction ID\n\t_              [4]byte // Padding for alignment, ensure consistent size (e.g., 64 bytes)\n}\n\n// metaMarshalSize is the size of the Meta struct in bytes.\nvar metaMarshalSize = uint64(unsafe.Sizeof(Meta{}))\n\n// CalculateChecksum calculates the CRC32 checksum of the Meta struct.\n// The Checksum field itself is set to 0 during calculation.\nfunc (m *Meta) CalculateChecksum() uint64 {\n\toriginalChecksum := m.Checksum\n\tm.Checksum = 0 // Exclude checksum field itself from calculation\n\tbuf := make([]byte, metaMarshalSize)\n\t// Manually serialize the fields to a buffer for checksum calculation\n\tbinary.BigEndian.PutUint64(buf[0:8], m.Version)\n\tbinary.BigEndian.PutUint32(buf[8:12], m.PageSize)\n\tbinary.BigEndian.PutUint64(buf[12:20], m.FreeListHead)\n\tbinary.BigEndian.PutUint64(buf[20:28], m.FileSize)\n\tbinary.BigEndian.PutUint64(buf[28:36], m.NumPages)\n\tbinary.BigEndian.PutUint64(buf[36:44], m.Checksum) // This will be 0\n\tbinary.BigEndian.PutUint32(buf[44:48], m.CurrentMetaIdx)\n\tbinary.BigEndian.PutUint64(buf[48:56], m.TxID)\n\n\tchecksum := uint64(crc32.ChecksumIEEE(buf))\n\tm.Checksum = originalChecksum // Restore original checksum\n\treturn checksum\n}\n\n// ValidateChecksum validates the checksum of the Meta struct.\nfunc (m *Meta) ValidateChecksum() bool {\n\tchecksum := m.Checksum\n\tcalculated := m.CalculateChecksum()\n\treturn checksum == calculated\n}\n\n// DB represents the memory-mapped database.\ntype DB struct {\n\tpath string\n\tfile *os.File\n\n\t// mmapLock protects access to mmapData and its associated file operations (remap).\n\t// Read-locks are held by transactions to ensure they work with a stable mmap.\n\t// Write-locks are held during remap operations.\n\tmmapLock sync.RWMutex\n\tmmapData []byte // The current memory-mapped region\n\n\t// meta represents the current database metadata.\n\t// It is a *copy* of the on-disk meta, updated during write transactions.\n\t// Access to this field is protected by mmapLock.\n\tmeta *Meta\n\n\t// activeTxns tracks the number of active transactions.\n\t// Used during remap to wait for old transactions to complete.\n\tactiveTxns sync.WaitGroup\n\n\t// txnIDCounter generates unique IDs for transactions.\n\ttxnIDCounter uint64\n\n\t// remapCond is used to signal and wait for remap operations.\n\t// It's associated with mmapLock.\n\tremapCond *sync.Cond\n\n\t// isRemapping is a flag indicating if a remap operation is in progress.\n\t// Protected by mmapLock.\n\tisRemapping bool\n\n\t// closed indicates if the database is closed.\n\tclosed bool\n\tmu     sync.Mutex // Protects 'closed' field.\n}\n\n// Open opens a database file at the given path. If the file does not exist, it is created.\nfunc Open(path string) (*DB, error) {\n\tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to open database file: %w\", err)\n\t}\n\n\tdb := &DB{\n\t\tpath: file.Name(),\n\t\tfile: file,\n\t}\n\tdb.remapCond = sync.NewCond(&db.mmapLock) // Condition associated with the write-part of RWMutex\n\n\t// Initialize database file if it's new/empty\n\tfi, err := file.Stat()\n\tif err != nil {\n\t\t_ = db.Close() // Attempt to clean up\n\t\treturn nil, fmt.Errorf(\"failed to stat database file: %w\", err)\n\t}\n\n\tcurrentFileSize := uint64(fi.Size())\n\tif currentFileSize < uint64(DataOffset) { // Ensure enough space for meta pages + some initial data\n\t\tif err := db.initFile(); err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, fmt.Errorf(\"failed to initialize database file: %w\", err)\n\t\t}\n\t\tcurrentFileSize = InitialDBSize\n\t} else if currentFileSize < InitialDBSize {\n\t\t// If the file exists but is smaller than our initial requirement, expand it.\n\t\t// This handles cases where it was previously initialized but then truncated.\n\t\tdb.mmapLock.Lock() // Acquire lock for expansion\n\t\tdefer db.mmapLock.Unlock()\n\t\tif err := db.expandFile(InitialDBSize); err != nil {\n\t\t\t_ = db.Close()\n\t\t\treturn nil, fmt.Errorf(\"failed to expand database file to initial size: %w\", err)\n\t\t}\n\t\tcurrentFileSize = InitialDBSize\n\t}\n\n\t// Initial mmap\n\tdb.mmapLock.Lock() // Need a write lock to initialize mmapData\n\tdefer db.mmapLock.Unlock()\n\tif err := db.mmap(int(currentFileSize)); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, fmt.Errorf(\"failed to mmap database file: %w\", err)\n\t}\n\n\t// Load metadata\n\tif err := db.loadMeta(); err != nil {\n\t\t_ = db.Close()\n\t\treturn nil, fmt.Errorf(\"failed to load metadata: %w\", err)\n\t}\n\n\treturn db, nil\n}\n\n// initFile initializes a new database file with default metadata.\n// This function must acquire the db.mmapLock (write lock) as it modifies the file system and mmapData.\nfunc (db *DB) initFile() error {\n\tdb.mmapLock.Lock()\n\tdefer db.mmapLock.Unlock()\n\n\t// Truncate to initial size\n\tif err := db.file.Truncate(InitialDBSize); err != nil {\n\t\treturn fmt.Errorf(\"failed to truncate file for initialization: %w\", err)\n\t}\n\n\t// Mmap the initial file size\n\tif db.mmapData != nil { // Unmap existing if any, though unlikely on initFile\n\t\tif err := unix.Munmap(db.mmapData); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to munmap old region during init: %w\", err)\n\t\t}\n\t}\n\tvar err error\n\tdb.mmapData, err = unix.Mmap(int(db.file.Fd()), 0, InitialDBSize, unix.PROT_READ|unix.PROT_WRITE, unix.MAP_SHARED)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to mmap file for initialization: %w\", err)\n\t}\n\n\tdb.meta = &Meta{\n\t\tVersion:        1,\n\t\tPageSize:       DefaultPageSize,\n\t\tFreeListHead:   0, // No free pages initially\n\t\tFileSize:       InitialDBSize,\n\t\tNumPages:       uint64(InitialDBSize-DataOffset) / uint64(DefaultPageSize),\n\t\tTxID:           0,\n\t\tCurrentMetaIdx: 0, // Start with meta page 0\n\t}\n\n\t// Write initial metadata to both meta pages for redundancy\n\t// We increment TxID to differentiate the two meta pages if both are valid\n\tif err := db.writeMeta(db.meta, 0); err != nil {\n\t\treturn fmt.Errorf(\"failed to write initial meta page 0: %w\", err)\n\t}\n\tdb.meta.CurrentMetaIdx = 1 // Switch to next meta page for next write\n\tdb.meta.TxID++             // Increment TxID for the second meta page\n\tif err := db.writeMeta(db.meta, 1); err != nil {\n\t\treturn fmt.Errorf(\"failed to write initial meta page 1: %w\", err)\n\t}\n\n\t// Sync to disk\n\tif err := unix.Msync(db.mmapData, unix.MS_SYNC); err != nil {\n\t\treturn fmt.Errorf(\"failed to sync initial metadata: %w\", err)\n\t}\n\n\treturn nil\n}\n\n// loadMeta loads the metadata from the database file.\n// It reads both meta pages, validates their checksums, and uses the most recent valid one.\n// Assumes db.mmapLock (write lock) is held.\nfunc (db *DB) loadMeta() error {\n\tvar meta0, meta1 Meta\n\tif len(db.mmapData) < int(uint64(MetaPage1Offset)+metaMarshalSize) {\n\t\treturn ErrFileSizeTooSmall\n\t}\n\n\t// Read meta page 0\n\tbuf0 := db.mmapData[MetaPage0Offset : MetaPage0Offset+metaMarshalSize]\n\tmeta0Ptr := (*Meta)(unsafe.Pointer(&buf0[0]))\n\tmeta0 = *meta0Ptr // Copy the meta data\n\n\t// Read meta page 1\n\tbuf1 := db.mmapData[MetaPage1Offset : MetaPage1Offset+metaMarshalSize]\n\tmeta1Ptr := (*Meta)(unsafe.Pointer(&buf1[0]))\n\tmeta1 = *meta1Ptr // Copy the meta data\n\n\tvar valid0, valid1 bool\n\tif meta0.ValidateChecksum() {\n\t\tvalid0 = true\n\t} else {\n\t\tfmt.Printf(\"Warning: Meta page 0 checksum invalid (txid %d). File: %s\\n\", meta0.TxID, db.path)\n\t}\n\tif meta1.ValidateChecksum() {\n\t\tvalid1 = true\n\t} else {\n\t\tfmt.Printf(\"Warning: Meta page 1 checksum invalid (txid %d). File: %s\\n\", meta1.TxID, db.path)\n\t}\n\n\tswitch {\n\tcase valid0 && valid1:\n\t\t// Both are valid, pick the one with the higher TxID (most recent)\n\t\tif meta0.TxID > meta1.TxID {\n\t\t\tdb.meta = &meta0\n\t\t} else {\n\t\t\tdb.meta = &meta1\n\t\t}\n\tcase valid0:\n\t\tdb.meta = &meta0\n\tcase valid1:\n\t\tdb.meta = &meta1\n\tdefault:\n\t\treturn ErrInvalidMeta // Neither meta page is valid\n\t}\n\n\t// Ensure the database file size is consistent with the loaded meta.\n\tif uint64(len(db.mmapData)) < db.meta.FileSize {\n\t\treturn fmt.Errorf(\"mmap data size (%d) is smaller than meta.FileSize (%d)\", len(db.mmapData), db.meta.FileSize)\n\t}\n\treturn nil\n}\n\n// writeMeta writes the given metadata to the specified meta page index (0 or 1).\n// Assumes db.mmapLock.Lock() is held.\nfunc (db *DB) writeMeta(meta *Meta, idx int) error {\n\tmeta.Checksum = meta.CalculateChecksum() // Update checksum before writing\n\n\toffset := MetaPage0Offset\n\tif idx == 1 {\n\t\toffset = MetaPage1Offset\n\t}\n\n\tif len(db.mmapData) < int(uint64(offset)+metaMarshalSize) {\n\t\treturn fmt.Errorf(\"mmap region too small to write meta page %d\", idx)\n\t}\n\n\t// Directly write the meta struct to the mmapData.\n\tsrcPtr := unsafe.Pointer(meta)\n\tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n\tfor i := 0; i < int(metaMarshalSize); i++ {\n\t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n\t}\n\treturn nil\n}\n\n// Close closes the database.\nfunc (db *DB) Close() error {\n\tdb.mu.Lock()\n\tif db.closed {\n\t\tdb.mu.Unlock()\n\t\treturn ErrDatabaseClosed\n\t}\n\tdb.closed = true\n\tdb.mu.Unlock()\n\n\t// Wait for all active transactions to complete before unmapping/closing\n\tfmt.Println(\"Waiting for active transactions to complete before closing DB...\")\n\tdb.activeTxns.Wait()\n\tfmt.Println(\"All active transactions completed, proceeding with DB close.\")\n\n\tdb.mmapLock.Lock() // Acquire write lock to ensure no new mmap operations or transactions start\n\tdefer db.mmapLock.Unlock()\n\n\tvar err error\n\t// Flush changes to disk before unmapping\n\tif db.mmapData != nil {\n\t\tif syncErr := unix.Msync(db.mmapData, unix.MS_SYNC); syncErr != nil {\n\t\t\terr = errors.Join(err, fmt.Errorf(\"failed to sync mmap data: %w\", syncErr))\n\t\t}\n\t\tif munmapErr := unix.Munmap(db.mmapData); munmapErr != nil {\n\t\t\terr = errors.Join(err, fmt.Errorf(\"failed to unmap data: %w\", munmapErr))\n\t\t}\n\t\tdb.mmapData = nil\n\t}\n\n\tif closeErr := db.file.Close(); closeErr != nil {\n\t\terr = errors.Join(err, fmt.Errorf(\"failed to close file: %w\", closeErr))\n\t}\n\n\treturn err\n}\n\n// mmap maps the database file into memory.\n// Assumes db.mmapLock is held (either RLock for initial mmap or Lock for remap).\nfunc (db *DB) mmap(size int) error {\n\tif db.mmapData != nil {\n\t\tif err := unix.Munmap(db.mmapData); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to munmap old region: %w\", err)\n\t\t}\n\t}\n\n\tvar err error\n\tdb.mmapData, err = unix.Mmap(int(db.file.Fd()), 0, size, unix.PROT_READ|unix.PROT_WRITE, unix.MAP_SHARED)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to mmap file: %w\", err)\n\t}\n\treturn nil\n}\n\n// expandFile expands the database file to a new size and remaps it.\n// This function requires an exclusive write lock on db.mmapLock.\n// Assumes db.mmapLock is already held.\nfunc (db *DB) expandFile(newSize uint64) error {\n\tif newSize <= uint64(len(db.mmapData)) {\n\t\treturn nil // No expansion needed or shrinking is not supported here.\n\t}\n\n\t// Round up to nearest page size\n\tif newSize%uint64(db.meta.PageSize) != 0 {\n\t\tnewSize = (newSize/uint64(db.meta.PageSize) + 1) * uint64(db.meta.PageSize)\n\t}\n\n\tfmt.Printf(\"Remapping DB: Old size %d, New size %d, File: %s\\n\", len(db.mmapData), newSize, db.path)\n\n\tdb.isRemapping = true\n\t// Wake up any new transactions waiting on remap, so they retry after it's done.\n\tdb.remapCond.Broadcast()\n\n\t// Wait for all existing transactions to complete.\n\t// This is the crucial step to ensure no one is accessing the old mmapData.\n\tfmt.Println(\"Waiting for all active transactions to complete before remap...\")\n\tdb.activeTxns.Wait()\n\tfmt.Println(\"All active transactions completed, proceeding with remap.\")\n\n\t// Truncate the file to the new size\n\tif err := db.file.Truncate(int64(newSize)); err != nil {\n\t\tdb.isRemapping = false\n\t\tdb.remapCond.Broadcast() // Re-broadcast in case of error\n\t\treturn fmt.Errorf(\"failed to truncate file to new size %d: %w\", newSize, err)\n\t}\n\n\t// Mmap the new region\n\tif err := db.mmap(int(newSize)); err != nil {\n\t\tdb.isRemapping = false\n\t\tdb.remapCond.Broadcast() // Re-broadcast in case of error\n\t\treturn fmt.Errorf(\"failed to mmap new file region of size %d: %w\", newSize, err)\n\t}\n\n\tdb.meta.FileSize = newSize\n\tdb.meta.NumPages = (newSize - uint64(DataOffset)) / uint64(db.meta.PageSize)\n\tdb.isRemapping = false\n\tdb.remapCond.Broadcast() // Signal that remap is complete and new map is ready\n\n\tfmt.Printf(\"DB remapped successfully to size %d, File: %s\\n\", newSize, db.path)\n\treturn nil\n}\n\n// Begin starts a new transaction.\n// If writable is true, a read-write transaction is started. Otherwise, a read-only transaction.\nfunc (db *DB) Begin(writable bool) (Transaction, error) {\n\tdb.mu.Lock()\n\tif db.closed {\n\t\tdb.mu.Unlock()\n\t\treturn nil, ErrDatabaseClosed\n\t}\n\tdb.mu.Unlock()\n\n\ttxID := atomic.AddUint64(&db.txnIDCounter, 1)\n\tdb.activeTxns.Add(1) // Increment active transaction counter\n\n\tif writable {\n\t\tdb.mmapLock.Lock() // Acquire exclusive write lock for R/W transaction\n\t\t// A read-write transaction holds the database-wide write lock for its entire duration\n\t\t// (from Begin to Commit/Rollback). This ensures only one R/W transaction at a time.\n\t\t// It also protects against concurrent remap operations.\n\n\t\t// If a remap is in progress when a R/W transaction tries to begin, it must wait.\n\t\tfor db.isRemapping {\n\t\t\tdb.mmapLock.Unlock() // Release lock to allow remap to proceed\n\t\t\tdb.remapCond.L.Lock()\n\t\t\tif db.isRemapping { // Double check after acquiring cond mutex\n\t\t\t\tdb.remapCond.Wait() // Wait for remap to finish\n\t\t\t}\n\t\t\tdb.remapCond.L.Unlock()\n\t\t\tdb.mmapLock.Lock() // Re-acquire write lock\n\t\t}\n\n\t\t// After acquiring write lock and ensuring no remap, take a snapshot of meta and mmap.\n\t\tmetaCopy := *db.meta\n\t\tmmapSnapshot := db.mmapData\n\n\t\treturn &ReadWriteTx{\n\t\t\tbaseTx: baseTx{\n\t\t\t\tid:       txID,\n\t\t\t\tdb:       db,\n\t\t\t\tmmapData: mmapSnapshot, // Points to current mmap\n\t\t\t\tdone:     false,\n\t\t\t},\n\t\t\tmeta:          metaCopy,\n\t\t\tpendingWrites: make(map[uint64][]byte),\n\t\t\tnewFileSize:   db.meta.FileSize,\n\t\t}, nil\n\t} else {\n\t\tdb.mmapLock.RLock() // Acquire read lock for R/O transaction\n\n\t\t// If a remap is in progress, release read lock and wait.\n\t\tfor db.isRemapping {\n\t\t\tdb.mmapLock.RUnlock()\n\t\t\tdb.remapCond.L.Lock()\n\t\t\tif db.isRemapping { // Double check after acquiring cond mutex\n\t\t\t\tdb.remapCond.Wait() // Wait for remap to finish\n\t\t\t}\n\t\t\tdb.remapCond.L.Unlock()\n\t\t\tdb.mmapLock.RLock() // Re-acquire read lock for a new attempt\n\t\t}\n\t\t// After acquiring read lock and ensuring no remap, take a snapshot of meta and mmap.\n\t\tmetaCopy := *db.meta\n\t\tmmapSnapshot := db.mmapData\n\n\t\t// The RLock needs to be held until the transaction is done to prevent the underlying mmapData from being unmapped.\n\t\t// However, the activeTxns.Wait() in expandFile already ensures this.\n\t\t// Releasing the RLock here is okay, as activeTxns protects the underlying memory until all transactions complete.\n\t\tdefer db.mmapLock.RUnlock() // Release read lock after transaction creation.\n\n\t\treturn &ReadOnlyTx{\n\t\t\tbaseTx: baseTx{\n\t\t\t\tid:       txID,\n\t\t\t\tdb:       db,\n\t\t\t\tmmapData: mmapSnapshot, // Points to current mmap slice\n\t\t\t\tdone:     false,\n\t\t\t},\n\t\t\tsnapshotMeta: metaCopy,\n\t\t}, nil\n\t}\n}\n\n// Transaction interface for database operations.\ntype Transaction interface {\n\tID() uint64\n\tRead(offset, size uint64) ([]byte, error)\n\tPage(pageID uint64) ([]byte, error)\n\tCommit() error\n\tRollback() error\n\tIsWritable() bool\n}\n\n// baseTx provides common fields and methods for both transaction types.\ntype baseTx struct {\n\tid       uint64\n\tdb       *DB\n\tmmapData []byte // A snapshot of the mmapData when the transaction started\n\tdone     bool   // True if the transaction is committed or rolled back\n\tmu       sync.Mutex // Protects 'done'\n}\n\nfunc (b *baseTx) ID() uint64 {\n\treturn b.id\n}\n\n// doneTx cleans up common transaction resources.\nfunc (b *baseTx) doneTx() {\n\tb.mu.Lock()\n\tif !b.done {\n\t\tb.done = true\n\t\tb.db.activeTxns.Done() // Decrement active transaction count\n\n\t\t// If this was a ReadWriteTx, its db.mmapLock.Lock() needs to be released.\n\t\t// This is handled by a defer in ReadWriteTx.Commit/Rollback.\n\t}\n\tb.mu.Unlock()\n}\n\nfunc (b *baseTx) Read(offset, size uint64) ([]byte, error) {\n\tb.mu.Lock()\n\tif b.done {\n\t\tb.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\tb.mu.Unlock()\n\n\tif offset+size > uint64(len(b.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"read out of bounds: offset %d, size %d, mmap size %d\", offset, size, len(b.mmapData))\n\t}\n\treturn b.mmapData[offset : offset+size], nil\n}\n\n// ReadOnlyTx represents a read-only database transaction.\ntype ReadOnlyTx struct {\n\tbaseTx\n\tsnapshotMeta Meta // A copy of the metadata at the time of transaction creation.\n}\n\n// IsWritable returns false for a read-only transaction.\nfunc (tx *ReadOnlyTx) IsWritable() bool {\n\treturn false\n}\n\n// Page returns a byte slice representing the data for a given page ID for a ReadOnlyTx.\nfunc (tx *ReadOnlyTx) Page(pageID uint64) ([]byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.snapshotMeta.PageSize)\n\n\tif pageID >= tx.snapshotMeta.NumPages {\n\t\treturn nil, ErrPageNotFound\n\t}\n\n\toffset := uint64(DataOffset) + pageID*pageSize\n\tif offset+pageSize > uint64(len(tx.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"page %d (offset %d) out of mmap bounds\", pageID, offset)\n\t}\n\treturn tx.mmapData[offset : offset+pageSize], nil\n}\n\n// Commit attempts to commit the read-only transaction.\n// For read-only transactions, this effectively just marks it as done.\nfunc (tx *ReadOnlyTx) Commit() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\ttx.doneTx()\n\treturn nil\n}\n\n// Rollback rolls back the read-only transaction.\n// For read-only transactions, this effectively just marks it as done.\nfunc (tx *ReadOnlyTx) Rollback() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\ttx.doneTx()\n\treturn nil\n}\n\n// ReadWriteTx represents a read-write database transaction.\ntype ReadWriteTx struct {\n\tbaseTx\n\tmeta          Meta              // A mutable copy of the metadata for this transaction\n\tpendingWrites map[uint64][]byte // Offset -> Data to write\n\tnewFileSize   uint64            // Tracks the required file size\n}\n\n// IsWritable returns true for a read-write transaction.\nfunc (tx *ReadWriteTx) IsWritable() bool {\n\treturn true\n}\n\n// Page returns a byte slice representing the data for a given page ID for a ReadWriteTx.\nfunc (tx *ReadWriteTx) Page(pageID uint64) ([]byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.meta.PageSize)\n\n\tif pageID >= tx.meta.NumPages {\n\t\treturn nil, ErrPageNotFound\n\t}\n\n\toffset := uint64(DataOffset) + pageID*pageSize\n\tif offset+pageSize > uint64(len(tx.mmapData)) {\n\t\treturn nil, fmt.Errorf(\"page %d (offset %d) out of mmap bounds\", pageID, offset)\n\t}\n\n\t// First check if there's a pending write for this page.\n\t// If a transaction writes to a page it just allocated, it should see its own write.\n\tif data, ok := tx.pendingWrites[offset]; ok {\n\t\treturn data, nil\n\t}\n\n\treturn tx.mmapData[offset : offset+pageSize], nil\n}\n\n// Write writes data to the specified offset.\n// Data is buffered until Commit.\nfunc (tx *ReadWriteTx) Write(offset uint64, data []byte) error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tif offset+uint64(len(data)) > tx.newFileSize {\n\t\ttx.newFileSize = offset + uint64(len(data))\n\t}\n\ttx.pendingWrites[offset] = data\n\treturn nil\n}\n\n// AllocatePage allocates a new page and returns its ID and a byte slice to its data.\n// It will update `tx.newFileSize` and `tx.meta.NumPages` accordingly.\n// The returned slice is a temporary buffer for the caller to fill; actual write happens on Commit.\nfunc (tx *ReadWriteTx) AllocatePage(size uint64) (uint64, []byte, error) {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn 0, nil, ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tpageSize := uint64(tx.meta.PageSize)\n\tif size > pageSize {\n\t\treturn 0, nil, ErrInsufficientSpace // Can't allocate a page larger than PageSize\n\t}\n\n\tpageID := tx.meta.NumPages // New page ID is current NumPages\n\trequiredSize := uint64(DataOffset) + (pageID+1)*pageSize\n\n\tif requiredSize > tx.newFileSize {\n\t\ttx.newFileSize = requiredSize\n\t}\n\n\ttx.meta.NumPages++ // Update transaction's meta for new page count\n\n\t// Return a temporary buffer for the caller to write into.\n\t// On commit, this data will be transferred from `pendingWrites` to `mmapData`.\n\ttempBuf := make([]byte, pageSize)\n\toffset := uint64(DataOffset) + pageID*pageSize\n\ttx.pendingWrites[offset] = tempBuf // Buffer for the page data\n\treturn pageID, tempBuf, nil\n}\n\n// Commit commits the read-write transaction.\nfunc (tx *ReadWriteTx) Commit() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tdefer tx.db.mmapLock.Unlock() // Release the DB's write lock that was acquired in Begin\n\tdefer tx.doneTx()             // Ensure doneTx is called even on error\n\n\t// The mmapLock.Lock() was acquired by db.Begin(true) and is held throughout the R/W transaction.\n\t// This ensures that for the commit, no other transactions are active and no remapping is happening.\n\n\t// Step 1: Handle file expansion if needed\n\tif tx.newFileSize > tx.db.meta.FileSize {\n\t\tif err := tx.db.expandFile(tx.newFileSize); err != nil {\n\t\t\treturn fmt.Errorf(\"%w: failed to expand file during commit: %v\", ErrCommitFailed, err)\n\t\t}\n\t\t// Update our internal mmapData reference after expansion\n\t\ttx.mmapData = tx.db.mmapData\n\t} else if tx.newFileSize < tx.db.meta.FileSize {\n\t\t// If tx.newFileSize is smaller, it means we might have released pages.\n\t\t// For now, we only support growth, not shrinking the file on commit.\n\t\t// A full implementation would handle file shrinking (carefully).\n\t\ttx.newFileSize = tx.db.meta.FileSize // Revert to current file size if attempting to shrink\n\t}\n\n\t// Step 2: Apply pending writes to the mmapData\n\tfor offset, data := range tx.pendingWrites {\n\t\tif offset+uint64(len(data)) > uint64(len(tx.db.mmapData)) {\n\t\t\treturn fmt.Errorf(\"%w: write out of mmap bounds after expansion (offset %d, len %d, mmap size %d)\",\n\t\t\t\tErrCommitFailed, offset, len(data), len(tx.db.mmapData))\n\t\t}\n\t\tcopy(tx.db.mmapData[offset:], data)\n\t}\n\n\t// Step 3: Update database metadata\n\ttx.db.meta.Version = tx.meta.Version\n\ttx.db.meta.PageSize = tx.meta.PageSize\n\ttx.db.meta.FreeListHead = tx.meta.FreeListHead\n\ttx.db.meta.FileSize = tx.newFileSize // This should now be consistent\n\ttx.db.meta.NumPages = tx.meta.NumPages\n\ttx.db.meta.TxID = tx.id // Set the database's last committed TxID\n\n\t// Write the updated metadata to the *next* meta page.\n\t// This provides atomic metadata updates (two-phase commit for meta).\n\ttargetMetaIdx := (tx.db.meta.CurrentMetaIdx + 1) % 2\n\ttx.db.meta.CurrentMetaIdx = targetMetaIdx // Update the in-memory meta\n\tif err := tx.db.writeMeta(tx.db.meta, int(targetMetaIdx)); err != nil {\n\t\treturn fmt.Errorf(\"%w: failed to write meta page %d: %v\", ErrCommitFailed, targetMetaIdx, err)\n\t}\n\n\t// Step 4: Flush changes to disk\n\tif err := unix.Msync(tx.db.mmapData, unix.MS_SYNC); err != nil {\n\t\treturn fmt.Errorf(\"%w: failed to sync mmap data to disk: %v\", ErrCommitFailed, err)\n\t}\n\n\tfmt.Printf(\"Transaction %d committed successfully. DB FileSize: %d, NumPages: %d, File: %s\\n\", tx.id, tx.db.meta.FileSize, tx.db.meta.NumPages, tx.db.path)\n\n\treturn nil\n}\n\n// Rollback rolls back the read-write transaction.\nfunc (tx *ReadWriteTx) Rollback() error {\n\ttx.mu.Lock()\n\tif tx.done {\n\t\ttx.mu.Unlock()\n\t\treturn ErrTxClosed\n\t}\n\ttx.mu.Unlock()\n\n\tdefer tx.db.mmapLock.Unlock() // Release the DB's write lock\n\tdefer tx.doneTx()             // Ensure doneTx is called\n\n\t// For a read-write transaction, rollback just discards pending writes.\n\ttx.pendingWrites = nil\n\tfmt.Printf(\"Transaction %d rolled back. File: %s\\n\", tx.id, tx.db.path)\n\treturn nil\n}\n\n// Example usage and demonstration of concurrency.\nfunc main() {\n\tdbPath := filepath.Join(os.TempDir(), \"mydata.db\")\n\t_ = os.Remove(dbPath) // Clean up any previous database file\n\n\tfmt.Printf(\"Database file: %s\\n\", dbPath)\n\n\tdb, err := Open(dbPath)\n\tif err != nil {\n\t\tfmt.Printf(\"Error opening database: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif closeErr := db.Close(); closeErr != nil {\n\t\t\tfmt.Printf(\"Error closing database: %v\\n\", closeErr)\n\t\t}\n\t}()\n\n\t// --- Test 1: Initial write and read ---\n\tfmt.Println(\"\\n--- Test 1: Initial write and read ---\")\n\trwTx, err := db.Begin(true)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RW transaction: %v\\n\", err)\n\t\treturn\n\t}\n\n\t// Allocate a page and write some data\n\tallocatedPageID0, pageDataBuf, err := rwTx.(*ReadWriteTx).AllocatePage(DefaultPageSize)\n\tif err != nil {\n\t\tfmt.Printf(\"Error allocating page: %v\\n\", err)\n\t\trwTx.Rollback()\n\t\treturn\n\t}\n\tcopy(pageDataBuf, []byte(\"Hello, DB World from Page 0!\"))\n\tfmt.Printf(\"Allocated page with ID %d and wrote initial data.\\n\", allocatedPageID0)\n\n\tif err := rwTx.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RW transaction: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"Initial data committed.\")\n\n\t// Read data back\n\troTx, err := db.Begin(false)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RO transaction: %v\\n\", err)\n\t\treturn\n\t}\n\treadPage0Data, err := roTx.Page(allocatedPageID0)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading page %d: %v\\n\", allocatedPageID0, err)\n\t\troTx.Rollback()\n\t\treturn\n\t}\n\tfmt.Printf(\"Read from Page %d: %s\\n\", allocatedPageID0, string(readPage0Data[:len(\"Hello, DB World from Page 0!\")]))\n\tif err := roTx.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RO: %v\\n\", err)\n\t}\n\n\t// --- Test 2: Concurrent read-only transactions ---\n\tfmt.Println(\"\\n--- Test 2: Concurrent read-only transactions ---\")\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < 5; i++ {\n\t\twg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer wg.Done()\n\t\t\ttx, err := db.Begin(false)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"RO Tx %d: Error beginning transaction: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\t\tfmt.Printf(\"RO Tx %d: Error committing: %v\\n\", id, err)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\treadData, err := tx.Page(allocatedPageID0)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"RO Tx %d: Error reading page %d: %v\\n\", id, allocatedPageID0, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfmt.Printf(\"RO Tx %d: Read data: %s\\n\", id, string(readData[:len(\"Hello, DB World from Page 0!\")]))\n\t\t\ttime.Sleep(100 * time.Millisecond) // Simulate work\n\t\t}(i)\n\t}\n\twg.Wait()\n\tfmt.Println(\"All concurrent RO transactions completed.\")\n\n\t// --- Test 3: Read-write transaction with expansion and concurrent RO access ---\n\tfmt.Println(\"\\n--- Test 3: Read-write transaction with expansion and concurrent RO access ---\")\n\n\t// Start some background RO transactions that will be active during the RW commit.\n\tvar concurrentROwg sync.WaitGroup\n\tfor i := 0; i < 3; i++ {\n\t\tconcurrentROwg.Add(1)\n\t\tgo func(id int) {\n\t\t\tdefer concurrentROwg.Done()\n\t\t\ttx, err := db.Begin(false)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error beginning transaction: %v\\n\", id, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error committing: %v\\n\", id, err)\n\t\t\t\t}\n\t\t\t}()\n\t\t\t// Keep transaction active for a bit to ensure it is running during remap.\n\t\t\ttime.Sleep(2 * time.Second)\n\t\t\treadData, err := tx.Page(allocatedPageID0)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Background RO Tx %d: Error reading page %d: %v\\n\", id, allocatedPageID0, err)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfmt.Printf(\"Background RO Tx %d: Read data: %s\\n\", id, string(readData[:len(\"Hello, DB World from Page 0!\")]))\n\t\t}(i)\n\t}\n\n\t// Give a moment for background RO transactions to start\n\ttime.Sleep(100 * time.Millisecond)\n\n\t// Start a R/W transaction that expands the file\n\tfmt.Println(\"Starting R/W transaction that will expand the database.\")\n\trwTx2, err := db.Begin(true)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning RW transaction 2: %v\\n\", err)\n\t\treturn\n\t}\n\n\t// Allocate a new page, which should trigger file expansion\n\tnewlyAllocatedPageID, newPageDataBuf, err := rwTx2.(*ReadWriteTx).AllocatePage(DefaultPageSize)\n\tif err != nil {\n\t\tfmt.Printf(\"Error allocating new page for expansion: %v\\n\", err)\n\t\trwTx2.Rollback()\n\t\treturn\n\t}\n\tcopy(newPageDataBuf, []byte(\"This is new data in an expanded page!\"))\n\tfmt.Printf(\"Allocated new page with ID %d.\\n\", newlyAllocatedPageID)\n\n\t// Update page 0 as well to show a write to an existing page\n\tupdatedPage0Data := []byte(\"UPDATED: Hello, DB World from Page 0!\")\n\tif err := rwTx2.(*ReadWriteTx).Write(uint64(DataOffset)+allocatedPageID0*uint64(DefaultPageSize), updatedPage0Data); err != nil {\n\t\tfmt.Printf(\"Error writing updated page %d data: %v\\n\", allocatedPageID0, err)\n\t\trwTx2.Rollback()\n\t\treturn\n\t}\n\n\tfmt.Println(\"R/W transaction 2 attempting to commit and expand file...\")\n\tif err := rwTx2.Commit(); err != nil {\n\t\tfmt.Printf(\"Error committing RW transaction 2: %v\\n\", err)\n\t\treturn\n\t}\n\tfmt.Println(\"RW transaction 2 committed successfully, file expanded.\")\n\n\t// Wait for background RO transactions to finish\n\tconcurrentROwg.Wait()\n\tfmt.Println(\"All background RO transactions finished.\")\n\n\t// --- Test 4: Verify expanded data and new RO txns use new map ---\n\tfmt.Println(\"\\n--- Test 4: Verify expanded data and new RO txns use new map ---\")\n\tfinalRoTx, err := db.Begin(false)\n\tif err != nil {\n\t\tfmt.Printf(\"Error beginning final RO transaction: %v\\n\", err)\n\t\treturn\n\t}\n\tdefer func() {\n\t\tif err := finalRoTx.Commit(); err != nil {\n\t\t\tfmt.Printf(\"Error committing final RO: %v\\n\", err)\n\t\t}\n\t}()\n\n\treadUpdatedPage0Data, err := finalRoTx.Page(allocatedPageID0)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading updated page %d: %v\\n\", allocatedPageID0, err)\n\t\treturn\n\t}\n\tfmt.Printf(\"Final RO Tx: Read from Page %d (updated): %s\\n\", allocatedPageID0, string(readUpdatedPage0Data[:len(updatedPage0Data)]))\n\n\treadNewPageData, err := finalRoTx.Page(newlyAllocatedPageID)\n\tif err != nil {\n\t\tfmt.Printf(\"Error reading new page %d: %v\\n\", newlyAllocatedPageID, err)\n\t\treturn\n\t}\n\tfmt.Printf(\"Final RO Tx: Read from new Page %d: %s\\n\", newlyAllocatedPageID, string(readNewPageData[:len(\"This is new data in an expanded page!\")]))\n\n\t// Demonstrate a new transaction trying to start during a (simulated) remap\n\tfmt.Println(\"\\n--- Test 5: New transaction waiting on remap ---\")\n\t// Simulate a remap by manually locking and setting isRemapping\n\tdb.mmapLock.Lock() // Acquire DB write lock\n\tdb.isRemapping = true\n\tfmt.Println(\"Simulating remapping in progress...\")\n\n\tvar waitingTxErr error\n\tvar waitingTxWg sync.WaitGroup\n\twaitingTxWg.Add(1)\n\tgo func() {\n\t\tdefer waitingTxWg.Done()\n\t\tfmt.Println(\"Attempting to begin a new RO transaction during simulated remap...\")\n\t\ttx, err := db.Begin(false) // This call should block\n\t\tif err != nil {\n\t\t\twaitingTxErr = err\n\t\t\tfmt.Printf(\"Waiting RO Tx: Error beginning transaction: %v\\n\", err)\n\t\t\treturn\n\t\t}\n\t\tdefer func() {\n\t\t\tif err := tx.Commit(); err != nil {\n\t\t\t\tfmt.Printf(\"Waiting RO Tx: Error committing: %v\\n\", err)\n\t\t\t}\n\t\t}()\n\t\tfmt.Println(\"Waiting RO Tx: Successfully started after remap completion.\")\n\t}()\n\n\ttime.Sleep(500 * time.Millisecond) // Give the waiting goroutine a chance to hit the wait condition\n\tfmt.Println(\"Releasing simulated remap...\")\n\tdb.isRemapping = false\n\tdb.remapCond.Broadcast() // Signal that remap is done\n\tdb.mmapLock.Unlock()     // Release the lock so the waiting tx can proceed\n\n\twaitingTxWg.Wait()\n\tif waitingTxErr != nil {\n\t\tfmt.Printf(\"Waiting transaction failed: %v\\n\", waitingTxErr)\n\t} else {\n\t\tfmt.Println(\"Waiting transaction successfully proceeded after remap.\")\n\t}\n\n\tfmt.Println(\"\\nDatabase operations complete.\")\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:397\u001b[0m] - G115 (CWE-190): integer overflow conversion uint64 -> int64 (Confidence: MEDIUM, Severity: HIGH)\n    396: \t// Truncate the file to the new size\n  > 397: \tif err := db.file.Truncate(int64(newSize)); err != nil {\n    398: \t\tdb.isRemapping = false\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:404\u001b[0m] - G115 (CWE-190): integer overflow conversion uint64 -> int (Confidence: MEDIUM, Severity: HIGH)\n    403: \t// Mmap the new region\n  > 404: \tif err := db.mmap(int(newSize)); err != nil {\n    405: \t\tdb.isRemapping = false\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:311\u001b[0m] - G115 (CWE-190): integer overflow conversion uint64 -> int (Confidence: MEDIUM, Severity: HIGH)\n    310: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n  > 311: \tfor i := 0; i < int(metaMarshalSize); i++ {\n    312: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:304\u001b[0m] - G115 (CWE-190): integer overflow conversion uint64 -> int (Confidence: MEDIUM, Severity: HIGH)\n    303: \n  > 304: \tif len(db.mmapData) < int(uint64(offset)+metaMarshalSize) {\n    305: \t\treturn fmt.Errorf(\"mmap region too small to write meta page %d\", idx)\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:245\u001b[0m] - G115 (CWE-190): integer overflow conversion uint64 -> int (Confidence: MEDIUM, Severity: HIGH)\n    244: \tvar meta0, meta1 Meta\n  > 245: \tif len(db.mmapData) < int(uint64(MetaPage1Offset)+metaMarshalSize) {\n    246: \t\treturn ErrFileSizeTooSmall\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:174\u001b[0m] - G115 (CWE-190): integer overflow conversion uint64 -> int (Confidence: MEDIUM, Severity: HIGH)\n    173: \tdefer db.mmapLock.Unlock()\n  > 174: \tif err := db.mmap(int(currentFileSize)); err != nil {\n    175: \t\t_ = db.Close()\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:152\u001b[0m] - G115 (CWE-190): integer overflow conversion int64 -> uint64 (Confidence: MEDIUM, Severity: HIGH)\n    151: \n  > 152: \tcurrentFileSize := uint64(fi.Size())\n    153: \tif currentFileSize < uint64(DataOffset) { // Ensure enough space for meta pages + some initial data\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_4z44__2e/main.go:304\u001b[0m] - G115 (CWE-190): integer overflow conversion int -> uint64 (Confidence: MEDIUM, Severity: HIGH)\n    303: \n  > 304: \tif len(db.mmapData) < int(uint64(offset)+metaMarshalSize) {\n    305: \t\treturn fmt.Errorf(\"mmap region too small to write meta page %d\", idx)\n\nAutofix: \n\n[\u001b[30;43m/tmp/go_synthesis_4z44__2e/main.go:134\u001b[0m] - G304 (CWE-22): Potential file inclusion via variable (Confidence: HIGH, Severity: MEDIUM)\n    133: func Open(path string) (*DB, error) {\n  > 134: \tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n    135: \tif err != nil {\n\nAutofix: Consider using os.Root to scope file access under a fixed root (Go >=1.24). Prefer root.Open/root.Stat over os.Open/os.Stat to prevent directory traversal.\n\n[\u001b[30;43m/tmp/go_synthesis_4z44__2e/main.go:134\u001b[0m] - G302 (CWE-276): Expect file permissions to be 0600 or less (Confidence: HIGH, Severity: MEDIUM)\n    133: func Open(path string) (*DB, error) {\n  > 134: \tfile, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE, 0660)\n    135: \tif err != nil {\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:312\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    311: \tfor i := 0; i < int(metaMarshalSize); i++ {\n  > 312: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n    313: \t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:312\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    311: \tfor i := 0; i < int(metaMarshalSize); i++ {\n  > 312: \t\t*(*byte)(unsafe.Pointer(uintptr(dstPtr) + uintptr(i))) = *(*byte)(unsafe.Pointer(uintptr(srcPtr) + uintptr(i)))\n    313: \t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:310\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    309: \tsrcPtr := unsafe.Pointer(meta)\n  > 310: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n    311: \tfor i := 0; i < int(metaMarshalSize); i++ {\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:309\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    308: \t// Directly write the meta struct to the mmapData.\n  > 309: \tsrcPtr := unsafe.Pointer(meta)\n    310: \tdstPtr := unsafe.Pointer(&db.mmapData[offset])\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:256\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    255: \tbuf1 := db.mmapData[MetaPage1Offset : MetaPage1Offset+metaMarshalSize]\n  > 256: \tmeta1Ptr := (*Meta)(unsafe.Pointer(&buf1[0]))\n    257: \tmeta1 = *meta1Ptr // Copy the meta data\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:251\u001b[0m] - G103 (CWE-242): Use of unsafe calls should be audited (Confidence: HIGH, Severity: LOW)\n    250: \tbuf0 := db.mmapData[MetaPage0Offset : MetaPage0Offset+metaMarshalSize]\n  > 251: \tmeta0Ptr := (*Meta)(unsafe.Pointer(&buf0[0]))\n    252: \tmeta0 = *meta0Ptr // Copy the meta data\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:927\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    926: \t\tfmt.Printf(\"Error writing updated page %d data: %v\\n\", allocatedPageID0, err)\n  > 927: \t\trwTx2.Rollback()\n    928: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:917\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    916: \t\tfmt.Printf(\"Error allocating new page for expansion: %v\\n\", err)\n  > 917: \t\trwTx2.Rollback()\n    918: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:834\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    833: \t\tfmt.Printf(\"Error reading page %d: %v\\n\", allocatedPageID0, err)\n  > 834: \t\troTx.Rollback()\n    835: \t\treturn\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_4z44__2e/main.go:813\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    812: \t\tfmt.Printf(\"Error allocating page: %v\\n\", err)\n  > 813: \t\trwTx.Rollback()\n    814: \t\treturn\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 1010\n  Nosec  : 0\n  Issues : \u001b[1;31m20\u001b[0m\n\n[gosec] 2025/12/01 13:24:37 Including rules: default\n[gosec] 2025/12/01 13:24:37 Excluding rules: default\n[gosec] 2025/12/01 13:24:37 Including analyzers: default\n[gosec] 2025/12/01 13:24:37 Excluding analyzers: default\n[gosec] 2025/12/01 13:24:37 Import directory: /tmp/go_synthesis_4z44__2e\n[gosec] 2025/12/01 13:24:37 Checking package: main\n[gosec] 2025/12/01 13:24:37 Checking file: /tmp/go_synthesis_4z44__2e/main.go"
                },
                {
                    "verifier": "errcheck",
                    "error": "main.go:813:16:\trwTx.Rollback()\nmain.go:834:16:\troTx.Rollback()\nmain.go:917:17:\trwTx2.Rollback()\nmain.go:927:17:\trwTx2.Rollback()"
                }
            ]
        }
    ],
    "passed": false
}