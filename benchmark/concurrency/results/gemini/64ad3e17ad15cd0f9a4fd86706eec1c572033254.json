{
    "task": "64ad3e17ad15cd0f9a4fd86706eec1c572033254.txt",
    "provider": "gemini",
    "model": "gemini-2.5-flash",
    "rounds": [
        {
            "code": "package main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Task represents a task with its status and metadata.\ntype Task struct {\n\tName     string\n\tStatus   string\n\tMetadata map[string]string\n}\n\n// TaskStatusUpdate is sent through the channel to signal a status change.\ntype TaskStatusUpdate struct {\n\tTaskName  string\n\tNewStatus string\n}\n\n// TaskManager manages tasks, their status, and synchronization.\ntype TaskManager struct {\n\ttasks           map[string]*Task\n\tmu              sync.RWMutex // Protects the tasks map\n\tstatusCh        chan TaskStatusUpdate\n\tstopCh          chan struct{}\n\twg              sync.WaitGroup // For waiting on background goroutines\n\tsyncedCount     int64\n\tfailedSyncCount int64\n\tcounterMu       sync.Mutex // Protects syncedCount and failedSyncCount\n}\n\nconst (\n\tstatusChannelCapacity = 5\n\tsyncFailureRate       = 0.2 // 20% chance of sync failure\n\tworkerCount           = 10\n\tsimulationDuration    = 7 * time.Second\n)\n\n// NewTaskManager creates and initializes a new TaskManager.\nfunc NewTaskManager() *TaskManager {\n\ttm := &TaskManager{\n\t\ttasks:    make(map[string]*Task),\n\t\tstatusCh: make(chan TaskStatusUpdate, statusChannelCapacity),\n\t\tstopCh:   make(chan struct{}),\n\t}\n\n\t// Start the background goroutine for syncing\n\ttm.wg.Add(1)\n\tgo tm.syncBatchWorker()\n\n\treturn tm\n}\n\n// SetTaskStatus updates a task's status in memory and sends an update for synchronization.\n// It creates the task if it doesn't exist.\n// If metadata is provided, it updates the task's metadata upon creation.\nfunc (tm *TaskManager) SetTaskStatus(taskName string, status string, metadata map[string]string) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\ttask, exists := tm.tasks[taskName]\n\tif !exists {\n\t\ttask = &Task{\n\t\t\tName:     taskName,\n\t\t\tStatus:   \"initialized\", // A neutral status before first \"pending\"\n\t\t\tMetadata: make(map[string]string),\n\t\t}\n\t\tif metadata != nil {\n\t\t\tfor k, v := range metadata {\n\t\t\t\ttask.Metadata[k] = v\n\t\t\t}\n\t\t}\n\t\ttm.tasks[taskName] = task\n\t}\n\n\t// Only send update if status actually changed\n\tif task.Status == status {\n\t\treturn\n\t}\n\n\t// Update status in memory\n\ttask.Status = status\n\tlog.Printf(\"TASK | %s | Status updated to: %s\", taskName, status)\n\n\t// Send update to the channel for backend sync\n\tselect {\n\tcase tm.statusCh <- TaskStatusUpdate{TaskName: taskName, NewStatus: status}:\n\t\t// Successfully sent\n\tdefault:\n\t\t// Channel is full, log and drop. In a production system, one might implement\n\t\t// retry logic, a larger buffer, or block the sender.\n\t\tlog.Printf(\"WARN | Channel full | Task %s status %s. Dropping update.\", taskName, status)\n\t}\n}\n\n// DeleteTaskStatus removes a task's status from the in-memory cache.\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\tif _, exists := tm.tasks[taskName]; exists {\n\t\tdelete(tm.tasks, taskName)\n\t\tlog.Printf(\"CACHE | %s | Deleted from cache due to sync failure.\", taskName)\n\t}\n}\n\n// GetTaskStatus retrieves the current status of a task.\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (string, bool) {\n\ttm.mu.RLock()\n\tdefer tm.mu.RUnlock()\n\n\ttask, exists := tm.tasks[taskName]\n\tif !exists {\n\t\treturn \"\", false\n\t}\n\treturn task.Status, true\n}\n\n// syncBatchWorker continuously processes status updates from the channel.\nfunc (tm *TaskManager) syncBatchWorker() {\n\tdefer tm.wg.Done()\n\tlog.Println(\"SYNC | Worker started.\")\n\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusCh:\n\t\t\ttm.processUpdate(update)\n\t\tcase <-tm.stopCh:\n\t\t\t// Stop signal received, drain remaining updates from the channel\n\t\t\tlog.Println(\"SYNC | Stop signal received. Draining channel...\")\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusCh:\n\t\t\t\t\ttm.processUpdate(update) // Process updates during drain\n\t\t\t\tdefault:\n\t\t\t\t\tlog.Println(\"SYNC | Channel drained. Shutting down.\")\n\t\t\t\t\treturn // Channel is empty, exit goroutine\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// processUpdate handles a single TaskStatusUpdate, simulating backend sync and potential failure.\nfunc (tm *TaskManager) processUpdate(update TaskStatusUpdate) {\n\tlog.Printf(\"SYNC | Attempting to sync Task %s status: %s\", update.TaskName, update.NewStatus)\n\n\tif rand.Float64() < syncFailureRate {\n\t\tlog.Printf(\"SYNC | FAILED to sync Task %s status: %s. Deleting from cache.\", update.TaskName, update.NewStatus)\n\t\ttm.DeleteTaskStatus(update.TaskName) // Clean up cache on failure\n\t\ttm.counterMu.Lock()\n\t\ttm.failedSyncCount++\n\t\ttm.counterMu.Unlock()\n\t} else {\n\t\tlog.Printf(\"SYNC | SUCCESSFULLY synced Task %s status: %s\", update.TaskName, update.NewStatus)\n\t\ttm.counterMu.Lock()\n\t\ttm.syncedCount++\n\t\ttm.counterMu.Unlock()\n\t}\n}\n\n// Shutdown gracefully stops the TaskManager's background goroutines.\nfunc (tm *TaskManager) Shutdown() {\n\tlog.Println(\"MANAGER | Shutting down...\")\n\tclose(tm.stopCh)  // Signal the sync worker to stop\n\ttm.wg.Wait()      // Wait for the sync worker to finish\n\tclose(tm.statusCh) // Close the status channel after the worker has stopped consuming\n\tlog.Println(\"MANAGER | Shutdown complete.\")\n}\n\n// GetStats returns current synchronization statistics.\nfunc (tm *TaskManager) GetStats() (synced int64, failed int64) {\n\ttm.counterMu.Lock()\n\tdefer tm.counterMu.Unlock()\n\treturn tm.syncedCount, tm.failedSyncCount\n}\n\nfunc main() {\n\trand.Seed(time.Now().UnixNano())\n\tlog.SetFlags(log.Ldate | log.Ltime | log.Lmicroseconds)\n\n\ttaskManager := NewTaskManager()\n\tdefer taskManager.Shutdown()\n\n\tvar workerWg sync.WaitGroup\n\n\t// Start worker goroutines\n\tfor i := 0; i < workerCount; i++ {\n\t\tworkerWg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer workerWg.Done()\n\t\t\tnumTasks := rand.Intn(3) + 3 // Each worker updates 3 to 5 tasks\n\t\t\ttaskNames := make([]string, numTasks)\n\t\t\tfor t := 0; t < numTasks; t++ {\n\t\t\t\ttaskNames[t] = fmt.Sprintf(\"worker%d_task%d\", workerID, t)\n\t\t\t\t// Initial creation with \"pending\" status\n\t\t\t\ttaskManager.SetTaskStatus(taskNames[t], \"pending\", map[string]string{\"createdBy\": fmt.Sprintf(\"worker%d\", workerID)})\n\t\t\t\ttime.Sleep(time.Duration(rand.Intn(50)+10) * time.Millisecond) // Small delay\n\t\t\t}\n\n\t\t\t// Update tasks through several state transitions\n\t\t\tfor updateIter := 0; updateIter < rand.Intn(5)+5; updateIter++ { // Each task gets 5-9 updates on average\n\t\t\t\ttaskIdx := rand.Intn(numTasks)\n\t\t\t\ttaskName := taskNames[taskIdx]\n\n\t\t\t\t// Simulate state transitions: pending -> running -> completed/failed\n\t\t\t\tvar newStatus string\n\t\t\t\tcurrentStatus, _ := taskManager.GetTaskStatus(taskName) // Get current status to make transitions realistic\n\n\t\t\t\tswitch currentStatus {\n\t\t\t\tcase \"pending\", \"initialized\":\n\t\t\t\t\tnewStatus = \"running\"\n\t\t\t\tcase \"running\":\n\t\t\t\t\tif rand.Float64() < 0.7 { // 70% chance to complete from running\n\t\t\t\t\t\tnewStatus = \"completed\"\n\t\t\t\t\t} else { // 30% chance to fail\n\t\t\t\t\t\tnewStatus = \"failed\"\n\t\t\t\t\t}\n\t\t\t\tcase \"completed\", \"failed\":\n\t\t\t\t\t// Once completed or failed, tasks typically don't change status,\n\t\t\t\t\t// but for simulation, we can re-activate or just make it stable.\n\t\t\t\t\t// Let's keep them stable for simplicity or introduce a re-run option.\n\t\t\t\t\t// For now, if already completed/failed, try to set to running again with a low probability\n\t\t\t\t\tif rand.Float64() < 0.1 {\n\t\t\t\t\t\tnewStatus = \"pending\" // Simulate a re-run\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnewStatus = currentStatus // Stay in current terminal state\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tnewStatus = \"pending\" // Fallback\n\t\t\t\t}\n\n\t\t\t\tif newStatus != currentStatus { // Only update if it's a real status change\n\t\t\t\t\ttaskManager.SetTaskStatus(taskName, newStatus, nil) // metadata not changing typically\n\t\t\t\t}\n\t\t\t\ttime.Sleep(time.Duration(rand.Intn(100)+20) * time.Millisecond) // Simulate work\n\t\t\t}\n\t\t\tlog.Printf(\"WORKER | %d | Finished its updates.\", workerID)\n\t\t}(i)\n\t}\n\n\t// Goroutine to print periodic statistics\n\tstatsTicker := time.NewTicker(1 * time.Second)\n\tdefer statsTicker.Stop()\n\tstopStats := make(chan struct{})\n\tstatsWg := sync.WaitGroup{}\n\tstatsWg.Add(1)\n\tgo func() {\n\t\tdefer statsWg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-statsTicker.C:\n\t\t\t\tsynced, failed := taskManager.GetStats()\n\t\t\t\ttaskManager.mu.RLock()\n\t\t\t\tcurrentTasks := len(taskManager.tasks)\n\t\t\t\ttaskManager.mu.RUnlock()\n\t\t\t\tlog.Printf(\"STATS | Total Synced: %d, Failed Syncs: %d, Cached Tasks: %d\", synced, failed, currentTasks)\n\t\t\tcase <-stopStats:\n\t\t\t\tlog.Println(\"STATS | Stats printer stopped.\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for the simulation duration\n\tlog.Printf(\"MAIN | Running simulation for %v...\", simulationDuration)\n\ttime.Sleep(simulationDuration)\n\tlog.Println(\"MAIN | Simulation duration elapsed. Signaling workers to finish and manager to shutdown.\")\n\n\tworkerWg.Wait() // Wait for all worker goroutines to finish their updates\n\tlog.Println(\"MAIN | All worker goroutines finished their updates.\")\n\n\t// Stop stats goroutine\n\tclose(stopStats)\n\tstatsWg.Wait() // Wait for the stats goroutine to stop\n\n\tlog.Println(\"MAIN | Program exiting.\")\n\t// The deferred taskManager.Shutdown() will now execute\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:240\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    239: \t\t\t\t}\n  > 240: \t\t\t\ttime.Sleep(time.Duration(rand.Intn(100)+20) * time.Millisecond) // Simulate work\n    241: \t\t\t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:228\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    227: \t\t\t\t\t// For now, if already completed/failed, try to set to running again with a low probability\n  > 228: \t\t\t\t\tif rand.Float64() < 0.1 {\n    229: \t\t\t\t\t\tnewStatus = \"pending\" // Simulate a re-run\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:218\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    217: \t\t\t\tcase \"running\":\n  > 218: \t\t\t\t\tif rand.Float64() < 0.7 { // 70% chance to complete from running\n    219: \t\t\t\t\t\tnewStatus = \"completed\"\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:207\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    206: \t\t\tfor updateIter := 0; updateIter < rand.Intn(5)+5; updateIter++ { // Each task gets 5-9 updates on average\n  > 207: \t\t\t\ttaskIdx := rand.Intn(numTasks)\n    208: \t\t\t\ttaskName := taskNames[taskIdx]\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:206\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    205: \t\t\t// Update tasks through several state transitions\n  > 206: \t\t\tfor updateIter := 0; updateIter < rand.Intn(5)+5; updateIter++ { // Each task gets 5-9 updates on average\n    207: \t\t\t\ttaskIdx := rand.Intn(numTasks)\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:202\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    201: \t\t\t\ttaskManager.SetTaskStatus(taskNames[t], \"pending\", map[string]string{\"createdBy\": fmt.Sprintf(\"worker%d\", workerID)})\n  > 202: \t\t\t\ttime.Sleep(time.Duration(rand.Intn(50)+10) * time.Millisecond) // Small delay\n    203: \t\t\t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:196\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    195: \t\t\tdefer workerWg.Done()\n  > 196: \t\t\tnumTasks := rand.Intn(3) + 3 // Each worker updates 3 to 5 tasks\n    197: \t\t\ttaskNames := make([]string, numTasks)\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:152\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    151: \n  > 152: \tif rand.Float64() < syncFailureRate {\n    153: \t\tlog.Printf(\"SYNC | FAILED to sync Task %s status: %s. Deleting from cache.\", update.TaskName, update.NewStatus)\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 283\n  Nosec  : 0\n  Issues : \u001b[1;31m8\u001b[0m\n\n[gosec] 2025/12/01 13:02:29 Including rules: default\n[gosec] 2025/12/01 13:02:29 Excluding rules: default\n[gosec] 2025/12/01 13:02:29 Including analyzers: default\n[gosec] 2025/12/01 13:02:29 Excluding analyzers: default\n[gosec] 2025/12/01 13:02:29 Import directory: /tmp/go_synthesis_izhbsekr\n[gosec] 2025/12/01 13:02:29 Checking package: main\n[gosec] 2025/12/01 13:02:29 Checking file: /tmp/go_synthesis_izhbsekr/main.go"
                },
                {
                    "verifier": "staticcheck",
                    "error": "main.go:72:3: unnecessary nil check around range (S1031)\nmain.go:183:2: rand.Seed has been deprecated since Go 1.20 and an alternative has been available since Go 1.0: As of Go 1.20 there is no reason to call Seed with a random value. Programs that call Seed with a known value to get a specific sequence of results should use New(NewSource(seed)) to obtain a local random generator. (SA1019)"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand/v2\" // Use v2 for improved random number generation and no explicit seeding\n\t\"sync\"\n\t\"time\"\n)\n\n// Task represents a task with its status and metadata.\ntype Task struct {\n\tName     string\n\tStatus   string\n\tMetadata map[string]string\n}\n\n// TaskStatusUpdate is sent through the channel to signal a status change.\ntype TaskStatusUpdate struct {\n\tTaskName  string\n\tNewStatus string\n}\n\n// TaskManager manages tasks, their status, and synchronization.\ntype TaskManager struct {\n\ttasks           map[string]*Task\n\tmu              sync.RWMutex // Protects the tasks map\n\tstatusCh        chan TaskStatusUpdate\n\tstopCh          chan struct{}\n\twg              sync.WaitGroup // For waiting on background goroutines\n\tsyncedCount     int64\n\tfailedSyncCount int64\n\tcounterMu       sync.Mutex // Protects syncedCount and failedSyncCount\n}\n\nconst (\n\tstatusChannelCapacity = 5\n\tsyncFailureRate       = 0.2 // 20% chance of sync failure\n\tworkerCount           = 10\n\tsimulationDuration    = 7 * time.Second\n)\n\n// NewTaskManager creates and initializes a new TaskManager.\nfunc NewTaskManager() *TaskManager {\n\ttm := &TaskManager{\n\t\ttasks:    make(map[string]*Task),\n\t\tstatusCh: make(chan TaskStatusUpdate, statusChannelCapacity),\n\t\tstopCh:   make(chan struct{}),\n\t}\n\n\t// Start the background goroutine for syncing\n\ttm.wg.Add(1)\n\tgo tm.syncBatchWorker()\n\n\treturn tm\n}\n\n// SetTaskStatus updates a task's status in memory and sends an update for synchronization.\n// It creates the task if it doesn't exist.\n// If metadata is provided, it updates the task's metadata upon creation.\nfunc (tm *TaskManager) SetTaskStatus(taskName string, status string, metadata map[string]string) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\ttask, exists := tm.tasks[taskName]\n\tif !exists {\n\t\ttask = &Task{\n\t\t\tName:     taskName,\n\t\t\tStatus:   \"initialized\", // A neutral status before first \"pending\"\n\t\t\tMetadata: make(map[string]string),\n\t\t}\n\t\t// Populate metadata if provided\n\t\tfor k, v := range metadata { // Removed unnecessary nil check, range over nil map is a no-op\n\t\t\ttask.Metadata[k] = v\n\t\t}\n\t\ttm.tasks[taskName] = task\n\t}\n\n\t// Only send update if status actually changed\n\tif task.Status == status {\n\t\treturn\n\t}\n\n\t// Update status in memory\n\ttask.Status = status\n\tlog.Printf(\"TASK | %s | Status updated to: %s\", taskName, status)\n\n\t// Send update to the channel for backend sync\n\tselect {\n\tcase tm.statusCh <- TaskStatusUpdate{TaskName: taskName, NewStatus: status}:\n\t\t// Successfully sent\n\tdefault:\n\t\t// Channel is full, log and drop. In a production system, one might implement\n\t\t// retry logic, a larger buffer, or block the sender.\n\t\tlog.Printf(\"WARN | Channel full | Task %s status %s. Dropping update.\", taskName, status)\n\t}\n}\n\n// DeleteTaskStatus removes a task's status from the in-memory cache.\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\tif _, exists := tm.tasks[taskName]; exists {\n\t\tdelete(tm.tasks, taskName)\n\t\tlog.Printf(\"CACHE | %s | Deleted from cache due to sync failure.\", taskName)\n\t}\n}\n\n// GetTaskStatus retrieves the current status of a task.\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (string, bool) {\n\ttm.mu.RLock()\n\tdefer tm.mu.RUnlock()\n\n\ttask, exists := tm.tasks[taskName]\n\tif !exists {\n\t\treturn \"\", false\n\t}\n\treturn task.Status, true\n}\n\n// syncBatchWorker continuously processes status updates from the channel.\nfunc (tm *TaskManager) syncBatchWorker() {\n\tdefer tm.wg.Done()\n\tlog.Println(\"SYNC | Worker started.\")\n\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusCh:\n\t\t\ttm.processUpdate(update)\n\t\tcase <-tm.stopCh:\n\t\t\t// Stop signal received, drain remaining updates from the channel\n\t\t\tlog.Println(\"SYNC | Stop signal received. Draining channel...\")\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusCh:\n\t\t\t\t\ttm.processUpdate(update) // Process updates during drain\n\t\t\t\tdefault:\n\t\t\t\t\tlog.Println(\"SYNC | Channel drained. Shutting down.\")\n\t\t\t\t\treturn // Channel is empty, exit goroutine\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// processUpdate handles a single TaskStatusUpdate, simulating backend sync and potential failure.\nfunc (tm *TaskManager) processUpdate(update TaskStatusUpdate) {\n\tlog.Printf(\"SYNC | Attempting to sync Task %s status: %s\", update.TaskName, update.NewStatus)\n\n\tif rand.Float64() < syncFailureRate { // rand.Float64() from math/rand/v2\n\t\tlog.Printf(\"SYNC | FAILED to sync Task %s status: %s. Deleting from cache.\", update.TaskName, update.NewStatus)\n\t\ttm.DeleteTaskStatus(update.TaskName) // Clean up cache on failure\n\t\ttm.counterMu.Lock()\n\t\ttm.failedSyncCount++\n\t\ttm.counterMu.Unlock()\n\t} else {\n\t\tlog.Printf(\"SYNC | SUCCESSFULLY synced Task %s status: %s\", update.TaskName, update.NewStatus)\n\t\ttm.counterMu.Lock()\n\t\ttm.syncedCount++\n\t\ttm.counterMu.Unlock()\n\t}\n}\n\n// Shutdown gracefully stops the TaskManager's background goroutines.\nfunc (tm *TaskManager) Shutdown() {\n\tlog.Println(\"MANAGER | Shutting down...\")\n\tclose(tm.stopCh)  // Signal the sync worker to stop\n\ttm.wg.Wait()      // Wait for the sync worker to finish\n\tclose(tm.statusCh) // Close the status channel after the worker has stopped consuming\n\tlog.Println(\"MANAGER | Shutdown complete.\")\n}\n\n// GetStats returns current synchronization statistics.\nfunc (tm *TaskManager) GetStats() (synced int64, failed int64) {\n\ttm.counterMu.Lock()\n\tdefer tm.counterMu.Unlock()\n\treturn tm.syncedCount, tm.failedSyncCount\n}\n\nfunc main() {\n\t// rand.Seed is deprecated and not needed with math/rand/v2\n\tlog.SetFlags(log.Ldate | log.Ltime | log.Lmicroseconds)\n\n\ttaskManager := NewTaskManager()\n\tdefer taskManager.Shutdown()\n\n\tvar workerWg sync.WaitGroup\n\n\t// Start worker goroutines\n\tfor i := 0; i < workerCount; i++ {\n\t\tworkerWg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer workerWg.Done()\n\t\t\tnumTasks := rand.IntN(3) + 3 // Each worker updates 3 to 5 tasks (rand.IntN from math/rand/v2)\n\t\t\ttaskNames := make([]string, numTasks)\n\t\t\tfor t := 0; t < numTasks; t++ {\n\t\t\t\ttaskNames[t] = fmt.Sprintf(\"worker%d_task%d\", workerID, t)\n\t\t\t\t// Initial creation with \"pending\" status\n\t\t\t\ttaskManager.SetTaskStatus(taskNames[t], \"pending\", map[string]string{\"createdBy\": fmt.Sprintf(\"worker%d\", workerID)})\n\t\t\t\ttime.Sleep(time.Duration(rand.IntN(50)+10) * time.Millisecond) // Small delay (rand.IntN from math/rand/v2)\n\t\t\t}\n\n\t\t\t// Update tasks through several state transitions\n\t\t\tfor updateIter := 0; updateIter < rand.IntN(5)+5; updateIter++ { // Each task gets 5-9 updates on average (rand.IntN from math/rand/v2)\n\t\t\t\ttaskIdx := rand.IntN(numTasks) // rand.IntN from math/rand/v2\n\t\t\t\ttaskName := taskNames[taskIdx]\n\n\t\t\t\t// Simulate state transitions: pending -> running -> completed/failed\n\t\t\t\tvar newStatus string\n\t\t\t\tcurrentStatus, _ := taskManager.GetTaskStatus(taskName) // Get current status to make transitions realistic\n\n\t\t\t\tswitch currentStatus {\n\t\t\t\tcase \"pending\", \"initialized\":\n\t\t\t\t\tnewStatus = \"running\"\n\t\t\t\tcase \"running\":\n\t\t\t\t\tif rand.Float64() < 0.7 { // 70% chance to complete from running\n\t\t\t\t\t\tnewStatus = \"completed\"\n\t\t\t\t\t} else { // 30% chance to fail\n\t\t\t\t\t\tnewStatus = \"failed\"\n\t\t\t\t\t}\n\t\t\t\tcase \"completed\", \"failed\":\n\t\t\t\t\t// Once completed or failed, tasks typically don't change status,\n\t\t\t\t\t// but for simulation, we can re-activate or just make it stable.\n\t\t\t\t\t// For now, if already completed/failed, try to set to pending again with a low probability\n\t\t\t\t\tif rand.Float64() < 0.1 {\n\t\t\t\t\t\tnewStatus = \"pending\" // Simulate a re-run\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnewStatus = currentStatus // Stay in current terminal state\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tnewStatus = \"pending\" // Fallback\n\t\t\t\t}\n\n\t\t\t\tif newStatus != currentStatus { // Only update if it's a real status change\n\t\t\t\t\ttaskManager.SetTaskStatus(taskName, newStatus, nil) // metadata not changing typically\n\t\t\t\t}\n\t\t\t\ttime.Sleep(time.Duration(rand.IntN(100)+20) * time.Millisecond) // Simulate work (rand.IntN from math/rand/v2)\n\t\t\t}\n\t\t\tlog.Printf(\"WORKER | %d | Finished its updates.\", workerID)\n\t\t}(i)\n\t}\n\n\t// Goroutine to print periodic statistics\n\tstatsTicker := time.NewTicker(1 * time.Second)\n\tdefer statsTicker.Stop()\n\tstopStats := make(chan struct{})\n\tstatsWg := sync.WaitGroup{}\n\tstatsWg.Add(1)\n\tgo func() {\n\t\tdefer statsWg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-statsTicker.C:\n\t\t\t\tsynced, failed := taskManager.GetStats()\n\t\t\t\ttaskManager.mu.RLock()\n\t\t\t\tcurrentTasks := len(taskManager.tasks)\n\t\t\t\ttaskManager.mu.RUnlock()\n\t\t\t\tlog.Printf(\"STATS | Total Synced: %d, Failed Syncs: %d, Cached Tasks: %d\", synced, failed, currentTasks)\n\t\t\tcase <-stopStats:\n\t\t\t\tlog.Println(\"STATS | Stats printer stopped.\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for the simulation duration\n\tlog.Printf(\"MAIN | Running simulation for %v...\", simulationDuration)\n\ttime.Sleep(simulationDuration)\n\tlog.Println(\"MAIN | Simulation duration elapsed. Signaling workers to finish and manager to shutdown.\")\n\n\tworkerWg.Wait() // Wait for all worker goroutines to finish their updates\n\tlog.Println(\"MAIN | All worker goroutines finished their updates.\")\n\n\t// Stop stats goroutine\n\tclose(stopStats)\n\tstatsWg.Wait() // Wait for the stats goroutine to stop\n\n\tlog.Println(\"MAIN | Program exiting.\")\n\t// The deferred taskManager.Shutdown() will now execute\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:238\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    237: \t\t\t\t}\n  > 238: \t\t\t\ttime.Sleep(time.Duration(rand.IntN(100)+20) * time.Millisecond) // Simulate work (rand.IntN from math/rand/v2)\n    239: \t\t\t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:226\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    225: \t\t\t\t\t// For now, if already completed/failed, try to set to pending again with a low probability\n  > 226: \t\t\t\t\tif rand.Float64() < 0.1 {\n    227: \t\t\t\t\t\tnewStatus = \"pending\" // Simulate a re-run\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:217\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    216: \t\t\t\tcase \"running\":\n  > 217: \t\t\t\t\tif rand.Float64() < 0.7 { // 70% chance to complete from running\n    218: \t\t\t\t\t\tnewStatus = \"completed\"\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:206\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    205: \t\t\tfor updateIter := 0; updateIter < rand.IntN(5)+5; updateIter++ { // Each task gets 5-9 updates on average (rand.IntN from math/rand/v2)\n  > 206: \t\t\t\ttaskIdx := rand.IntN(numTasks) // rand.IntN from math/rand/v2\n    207: \t\t\t\ttaskName := taskNames[taskIdx]\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:205\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    204: \t\t\t// Update tasks through several state transitions\n  > 205: \t\t\tfor updateIter := 0; updateIter < rand.IntN(5)+5; updateIter++ { // Each task gets 5-9 updates on average (rand.IntN from math/rand/v2)\n    206: \t\t\t\ttaskIdx := rand.IntN(numTasks) // rand.IntN from math/rand/v2\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:201\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    200: \t\t\t\ttaskManager.SetTaskStatus(taskNames[t], \"pending\", map[string]string{\"createdBy\": fmt.Sprintf(\"worker%d\", workerID)})\n  > 201: \t\t\t\ttime.Sleep(time.Duration(rand.IntN(50)+10) * time.Millisecond) // Small delay (rand.IntN from math/rand/v2)\n    202: \t\t\t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:195\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    194: \t\t\tdefer workerWg.Done()\n  > 195: \t\t\tnumTasks := rand.IntN(3) + 3 // Each worker updates 3 to 5 tasks (rand.IntN from math/rand/v2)\n    196: \t\t\ttaskNames := make([]string, numTasks)\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:151\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    150: \n  > 151: \tif rand.Float64() < syncFailureRate { // rand.Float64() from math/rand/v2\n    152: \t\tlog.Printf(\"SYNC | FAILED to sync Task %s status: %s. Deleting from cache.\", update.TaskName, update.NewStatus)\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 281\n  Nosec  : 0\n  Issues : \u001b[1;31m8\u001b[0m\n\n[gosec] 2025/12/01 13:02:44 Including rules: default\n[gosec] 2025/12/01 13:02:44 Excluding rules: default\n[gosec] 2025/12/01 13:02:44 Including analyzers: default\n[gosec] 2025/12/01 13:02:44 Excluding analyzers: default\n[gosec] 2025/12/01 13:02:44 Import directory: /tmp/go_synthesis_izhbsekr\n[gosec] 2025/12/01 13:02:44 Checking package: main\n[gosec] 2025/12/01 13:02:44 Checking file: /tmp/go_synthesis_izhbsekr/main.go"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"math/rand/v2\" // Use v2 for improved random number generation and no explicit seeding\n\t\"sync\"\n\t\"time\"\n)\n\n// Task represents a task with its status and metadata.\ntype Task struct {\n\tName     string\n\tStatus   string\n\tMetadata map[string]string\n}\n\n// TaskStatusUpdate is sent through the channel to signal a status change.\ntype TaskStatusUpdate struct {\n\tTaskName  string\n\tNewStatus string\n}\n\n// TaskManager manages tasks, their status, and synchronization.\ntype TaskManager struct {\n\ttasks           map[string]*Task\n\tmu              sync.RWMutex // Protects the tasks map\n\tstatusCh        chan TaskStatusUpdate\n\tstopCh          chan struct{}\n\twg              sync.WaitGroup // For waiting on background goroutines\n\tsyncedCount     int64\n\tfailedSyncCount int64\n\tcounterMu       sync.Mutex // Protects syncedCount and failedSyncCount\n}\n\nconst (\n\tstatusChannelCapacity = 5\n\tsyncFailureRate       = 0.2 // 20% chance of sync failure\n\tworkerCount           = 10\n\tsimulationDuration    = 7 * time.Second\n)\n\n// NewTaskManager creates and initializes a new TaskManager.\nfunc NewTaskManager() *TaskManager {\n\ttm := &TaskManager{\n\t\ttasks:    make(map[string]*Task),\n\t\tstatusCh: make(chan TaskStatusUpdate, statusChannelCapacity),\n\t\tstopCh:   make(chan struct{}),\n\t}\n\n\t// Start the background goroutine for syncing\n\ttm.wg.Add(1)\n\tgo tm.syncBatchWorker()\n\n\treturn tm\n}\n\n// SetTaskStatus updates a task's status in memory and sends an update for synchronization.\n// It creates the task if it doesn't exist.\n// If metadata is provided, it updates the task's metadata upon creation.\nfunc (tm *TaskManager) SetTaskStatus(taskName string, status string, metadata map[string]string) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\ttask, exists := tm.tasks[taskName]\n\tif !exists {\n\t\ttask = &Task{\n\t\t\tName:     taskName,\n\t\t\tStatus:   \"initialized\", // A neutral status before first \"pending\"\n\t\t\tMetadata: make(map[string]string),\n\t\t}\n\t\t// Populate metadata if provided\n\t\tfor k, v := range metadata { // range over nil map is a no-op, no explicit nil check needed\n\t\t\ttask.Metadata[k] = v\n\t\t}\n\t\ttm.tasks[taskName] = task\n\t}\n\n\t// Only send update if status actually changed\n\tif task.Status == status {\n\t\treturn\n\t}\n\n\t// Update status in memory\n\ttask.Status = status\n\tlog.Printf(\"TASK | %s | Status updated to: %s\", taskName, status)\n\n\t// Send update to the channel for backend sync\n\tselect {\n\tcase tm.statusCh <- TaskStatusUpdate{TaskName: taskName, NewStatus: status}:\n\t\t// Successfully sent\n\tdefault:\n\t\t// Channel is full, log and drop. In a production system, one might implement\n\t\t// retry logic, a larger buffer, or block the sender.\n\t\tlog.Printf(\"WARN | Channel full | Task %s status %s. Dropping update.\", taskName, status)\n\t}\n}\n\n// DeleteTaskStatus removes a task's status from the in-memory cache.\nfunc (tm *TaskManager) DeleteTaskStatus(taskName string) {\n\ttm.mu.Lock()\n\tdefer tm.mu.Unlock()\n\n\tif _, exists := tm.tasks[taskName]; exists {\n\t\tdelete(tm.tasks, taskName)\n\t\tlog.Printf(\"CACHE | %s | Deleted from cache due to sync failure.\", taskName)\n\t}\n}\n\n// GetTaskStatus retrieves the current status of a task.\nfunc (tm *TaskManager) GetTaskStatus(taskName string) (string, bool) {\n\ttm.mu.RLock()\n\tdefer tm.mu.RUnlock()\n\n\ttask, exists := tm.tasks[taskName]\n\tif !exists {\n\t\treturn \"\", false\n\t}\n\treturn task.Status, true\n}\n\n// syncBatchWorker continuously processes status updates from the channel.\nfunc (tm *TaskManager) syncBatchWorker() {\n\tdefer tm.wg.Done()\n\tlog.Println(\"SYNC | Worker started.\")\n\n\tfor {\n\t\tselect {\n\t\tcase update := <-tm.statusCh:\n\t\t\ttm.processUpdate(update)\n\t\tcase <-tm.stopCh:\n\t\t\t// Stop signal received, drain remaining updates from the channel\n\t\t\tlog.Println(\"SYNC | Stop signal received. Draining channel...\")\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase update := <-tm.statusCh:\n\t\t\t\t\ttm.processUpdate(update) // Process updates during drain\n\t\t\t\tdefault:\n\t\t\t\t\tlog.Println(\"SYNC | Channel drained. Shutting down.\")\n\t\t\t\t\treturn // Channel is empty, exit goroutine\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// processUpdate handles a single TaskStatusUpdate, simulating backend sync and potential failure.\nfunc (tm *TaskManager) processUpdate(update TaskStatusUpdate) {\n\tlog.Printf(\"SYNC | Attempting to sync Task %s status: %s\", update.TaskName, update.NewStatus)\n\n\tif rand.Float64() < syncFailureRate { // rand.Float64() from math/rand/v2\n\t\tlog.Printf(\"SYNC | FAILED to sync Task %s status: %s. Deleting from cache.\", update.TaskName, update.NewStatus)\n\t\ttm.DeleteTaskStatus(update.TaskName) // Clean up cache on failure\n\t\ttm.counterMu.Lock()\n\t\ttm.failedSyncCount++\n\t\ttm.counterMu.Unlock()\n\t} else {\n\t\tlog.Printf(\"SYNC | SUCCESSFULLY synced Task %s status: %s\", update.TaskName, update.NewStatus)\n\t\ttm.counterMu.Lock()\n\t\ttm.syncedCount++\n\t\ttm.counterMu.Unlock()\n\t}\n}\n\n// Shutdown gracefully stops the TaskManager's background goroutines.\nfunc (tm *TaskManager) Shutdown() {\n\tlog.Println(\"MANAGER | Shutting down...\")\n\tclose(tm.stopCh)  // Signal the sync worker to stop\n\ttm.wg.Wait()      // Wait for the sync worker to finish\n\tclose(tm.statusCh) // Close the status channel after the worker has stopped consuming\n\tlog.Println(\"MANAGER | Shutdown complete.\")\n}\n\n// GetStats returns current synchronization statistics.\nfunc (tm *TaskManager) GetStats() (synced int64, failed int64) {\n\ttm.counterMu.Lock()\n\tdefer tm.counterMu.Unlock()\n\treturn tm.syncedCount, tm.failedSyncCount\n}\n\nfunc main() {\n\t// rand.Seed is deprecated and not needed with math/rand/v2\n\tlog.SetFlags(log.Ldate | log.Ltime | log.Lmicroseconds)\n\n\ttaskManager := NewTaskManager()\n\tdefer taskManager.Shutdown()\n\n\tvar workerWg sync.WaitGroup\n\n\t// Start worker goroutines\n\tfor i := 0; i < workerCount; i++ {\n\t\tworkerWg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer workerWg.Done()\n\t\t\tnumTasks := rand.IntN(3) + 3 // Each worker updates 3 to 5 tasks (rand.IntN from math/rand/v2)\n\t\t\ttaskNames := make([]string, numTasks)\n\t\t\tfor t := 0; t < numTasks; t++ {\n\t\t\t\ttaskNames[t] = fmt.Sprintf(\"worker%d_task%d\", workerID, t)\n\t\t\t\t// Initial creation with \"pending\" status\n\t\t\t\ttaskManager.SetTaskStatus(taskNames[t], \"pending\", map[string]string{\"createdBy\": fmt.Sprintf(\"worker%d\", workerID)})\n\t\t\t\ttime.Sleep(time.Duration(rand.IntN(50)+10) * time.Millisecond) // Small delay (rand.IntN from math/rand/v2)\n\t\t\t}\n\n\t\t\t// Update tasks through several state transitions\n\t\t\tfor updateIter := 0; updateIter < rand.IntN(5)+5; updateIter++ { // Each task gets 5-9 updates on average (rand.IntN from math/rand/v2)\n\t\t\t\ttaskIdx := rand.IntN(numTasks) // rand.IntN from math/rand/v2\n\t\t\t\ttaskName := taskNames[taskIdx]\n\n\t\t\t\t// Simulate state transitions: pending -> running -> completed/failed\n\t\t\t\tvar newStatus string\n\t\t\t\tcurrentStatus, _ := taskManager.GetTaskStatus(taskName) // Get current status to make transitions realistic\n\n\t\t\t\tswitch currentStatus {\n\t\t\t\tcase \"pending\", \"initialized\":\n\t\t\t\t\tnewStatus = \"running\"\n\t\t\t\tcase \"running\":\n\t\t\t\t\tif rand.Float64() < 0.7 { // 70% chance to complete from running\n\t\t\t\t\t\tnewStatus = \"completed\"\n\t\t\t\t\t} else { // 30% chance to fail\n\t\t\t\t\t\tnewStatus = \"failed\"\n\t\t\t\t\t}\n\t\t\t\tcase \"completed\", \"failed\":\n\t\t\t\t\t// Once completed or failed, tasks typically don't change status,\n\t\t\t\t\t// but for simulation, we can re-activate or just make it stable.\n\t\t\t\t\t// For now, if already completed/failed, try to set to pending again with a low probability\n\t\t\t\t\tif rand.Float64() < 0.1 {\n\t\t\t\t\t\tnewStatus = \"pending\" // Simulate a re-run\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnewStatus = currentStatus // Stay in current terminal state\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\tnewStatus = \"pending\" // Fallback\n\t\t\t\t}\n\n\t\t\t\tif newStatus != currentStatus { // Only update if it's a real status change\n\t\t\t\t\ttaskManager.SetTaskStatus(taskName, newStatus, nil) // metadata not changing typically\n\t\t\t\t}\n\t\t\t\ttime.Sleep(time.Duration(rand.IntN(100)+20) * time.Millisecond) // Simulate work (rand.IntN from math/rand/v2)\n\t\t\t}\n\t\t\tlog.Printf(\"WORKER | %d | Finished its updates.\", workerID)\n\t\t}(i)\n\t}\n\n\t// Goroutine to print periodic statistics\n\tstatsTicker := time.NewTicker(1 * time.Second)\n\tdefer statsTicker.Stop()\n\tstopStats := make(chan struct{})\n\tstatsWg := sync.WaitGroup{}\n\tstatsWg.Add(1)\n\tgo func() {\n\t\tdefer statsWg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-statsTicker.C:\n\t\t\t\tsynced, failed := taskManager.GetStats()\n\t\t\t\ttaskManager.mu.RLock()\n\t\t\t\tcurrentTasks := len(taskManager.tasks)\n\t\t\t\ttaskManager.mu.RUnlock()\n\t\t\t\tlog.Printf(\"STATS | Total Synced: %d, Failed Syncs: %d, Cached Tasks: %d\", synced, failed, currentTasks)\n\t\t\tcase <-stopStats:\n\t\t\t\tlog.Println(\"STATS | Stats printer stopped.\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for the simulation duration\n\tlog.Printf(\"MAIN | Running simulation for %v...\", simulationDuration)\n\ttime.Sleep(simulationDuration)\n\tlog.Println(\"MAIN | Simulation duration elapsed. Signaling workers to finish and manager to shutdown.\")\n\n\tworkerWg.Wait() // Wait for all worker goroutines to finish their updates\n\tlog.Println(\"MAIN | All worker goroutines finished their updates.\")\n\n\t// Stop stats goroutine\n\tclose(stopStats)\n\tstatsWg.Wait() // Wait for the stats goroutine to stop\n\n\tlog.Println(\"MAIN | Program exiting.\")\n\t// The deferred taskManager.Shutdown() will now execute\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:238\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    237: \t\t\t\t}\n  > 238: \t\t\t\ttime.Sleep(time.Duration(rand.IntN(100)+20) * time.Millisecond) // Simulate work (rand.IntN from math/rand/v2)\n    239: \t\t\t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:226\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    225: \t\t\t\t\t// For now, if already completed/failed, try to set to pending again with a low probability\n  > 226: \t\t\t\t\tif rand.Float64() < 0.1 {\n    227: \t\t\t\t\t\tnewStatus = \"pending\" // Simulate a re-run\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:217\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    216: \t\t\t\tcase \"running\":\n  > 217: \t\t\t\t\tif rand.Float64() < 0.7 { // 70% chance to complete from running\n    218: \t\t\t\t\t\tnewStatus = \"completed\"\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:206\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    205: \t\t\tfor updateIter := 0; updateIter < rand.IntN(5)+5; updateIter++ { // Each task gets 5-9 updates on average (rand.IntN from math/rand/v2)\n  > 206: \t\t\t\ttaskIdx := rand.IntN(numTasks) // rand.IntN from math/rand/v2\n    207: \t\t\t\ttaskName := taskNames[taskIdx]\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:205\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    204: \t\t\t// Update tasks through several state transitions\n  > 205: \t\t\tfor updateIter := 0; updateIter < rand.IntN(5)+5; updateIter++ { // Each task gets 5-9 updates on average (rand.IntN from math/rand/v2)\n    206: \t\t\t\ttaskIdx := rand.IntN(numTasks) // rand.IntN from math/rand/v2\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:201\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    200: \t\t\t\ttaskManager.SetTaskStatus(taskNames[t], \"pending\", map[string]string{\"createdBy\": fmt.Sprintf(\"worker%d\", workerID)})\n  > 201: \t\t\t\ttime.Sleep(time.Duration(rand.IntN(50)+10) * time.Millisecond) // Small delay (rand.IntN from math/rand/v2)\n    202: \t\t\t}\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:195\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    194: \t\t\tdefer workerWg.Done()\n  > 195: \t\t\tnumTasks := rand.IntN(3) + 3 // Each worker updates 3 to 5 tasks (rand.IntN from math/rand/v2)\n    196: \t\t\ttaskNames := make([]string, numTasks)\n\nAutofix: \n\n[\u001b[97;41m/tmp/go_synthesis_izhbsekr/main.go:151\u001b[0m] - G404 (CWE-338): Use of weak random number generator (math/rand or math/rand/v2 instead of crypto/rand) (Confidence: MEDIUM, Severity: HIGH)\n    150: \n  > 151: \tif rand.Float64() < syncFailureRate { // rand.Float64() from math/rand/v2\n    152: \t\tlog.Printf(\"SYNC | FAILED to sync Task %s status: %s. Deleting from cache.\", update.TaskName, update.NewStatus)\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 281\n  Nosec  : 0\n  Issues : \u001b[1;31m8\u001b[0m\n\n[gosec] 2025/12/01 13:03:00 Including rules: default\n[gosec] 2025/12/01 13:03:00 Excluding rules: default\n[gosec] 2025/12/01 13:03:00 Including analyzers: default\n[gosec] 2025/12/01 13:03:00 Excluding analyzers: default\n[gosec] 2025/12/01 13:03:00 Import directory: /tmp/go_synthesis_izhbsekr\n[gosec] 2025/12/01 13:03:00 Checking package: main\n[gosec] 2025/12/01 13:03:00 Checking file: /tmp/go_synthesis_izhbsekr/main.go"
                }
            ]
        }
    ],
    "passed": false
}