{
    "task": "8baaa06cce9e58548f60e6e9b21c9af3d42580bb.txt",
    "provider": "gemini",
    "model": "gemini-2.5-flash",
    "rounds": [
        {
            "code": "package main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\n// ConnectionPoolManager manages server endpoints and maintains a single active connection.\ntype ConnectionPoolManager struct {\n\tmu          sync.Mutex\n\taddresses   map[string]struct{} // Set of all known addresses\n\tactiveAddr  string              // The currently pinned address, empty if none\n\n\tnotifyCh    chan []string       // For consumers to get updates (buffered 1)\n\n\tisReady     bool                // True if an activeAddr is pinned\n\treadyCond   *sync.Cond          // Condition variable for WaitForReady\n\n\tcloseOnce   sync.Once\n\tclosed      chan struct{}       // Signal for internal goroutines to shut down\n\twg          sync.WaitGroup      // WaitGroup for tracking internal goroutines\n}\n\n// NewConnectionPoolManager creates and initializes a new manager.\nfunc NewConnectionPoolManager(initialAddrs []string) *ConnectionPoolManager {\n\tmgr := &ConnectionPoolManager{\n\t\taddresses: make(map[string]struct{}),\n\t\tnotifyCh:  make(chan []string, 1), // Buffered to hold the latest state\n\t\tclosed:    make(chan struct{}),\n\t}\n\tmgr.readyCond = sync.NewCond(&mgr.mu)\n\n\t// Use UpdateAddresses for initial population and notification\n\tmgr.UpdateAddresses(initialAddrs)\n\treturn mgr\n}\n\n// UpdateAddresses dynamically updates the list of available server addresses.\nfunc (c *ConnectionPoolManager) UpdateAddresses(newAddrs []string) {\n\tselect {\n\tcase <-c.closed:\n\t\treturn // Manager is closed, ignore updates\n\tdefault:\n\t\t// continue\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tnewAddrMap := make(map[string]struct{})\n\tfor _, addr := range newAddrs {\n\t\tnewAddrMap[addr] = struct{}{}\n\t}\n\n\t// Check if the current active address is still valid\n\toldActiveAddr := c.activeAddr\n\t_, activeStillExists := newAddrMap[oldActiveAddr]\n\n\tif oldActiveAddr != \"\" && !activeStillExists {\n\t\t// Pinned connection is no longer in the list, unpin it.\n\t\tc.activeAddr = \"\"\n\t\tc.isReady = false\n\t\tc.readyCond.Broadcast() // Notify waiters that state changed (now not ready)\n\t}\n\n\tc.addresses = newAddrMap\n\n\t// Send notification based on current state\n\tif c.activeAddr != \"\" {\n\t\tc.sendNotification([]string{c.activeAddr})\n\t} else {\n\t\tc.sendNotification(c.getAvailableAddressesLocked())\n\t}\n}\n\n// getAvailableAddressesLocked returns the current list of available addresses.\n// It assumes the mutex is already held.\nfunc (c *ConnectionPoolManager) getAvailableAddressesLocked() []string {\n\taddrs := make([]string, 0, len(c.addresses))\n\tfor addr := range c.addresses {\n\t\taddrs = append(addrs, addr)\n\t}\n\treturn addrs\n}\n\n// Up is called when a connection to an address is established.\n// It returns a cleanup function to be called when the connection goes down.\nfunc (c *ConnectionPoolManager) Up(addr string) (func(error), error) {\n\tselect {\n\tcase <-c.closed:\n\t\treturn func(error) {}, errors.New(\"manager is closed, cannot establish new connections\")\n\tdefault:\n\t\t// continue\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif _, ok := c.addresses[addr]; !ok {\n\t\treturn func(error) {}, fmt.Errorf(\"address %s is not a known endpoint\", addr)\n\t}\n\n\tisInitialPin := false\n\tif c.activeAddr == \"\" {\n\t\tc.activeAddr = addr\n\t\tc.isReady = true\n\t\tc.readyCond.Broadcast() // Notify waiters that a connection is pinned\n\t\tisInitialPin = true\n\t}\n\n\tif isInitialPin {\n\t\tc.sendNotification([]string{addr}) // Notify with pinned address\n\t}\n\n\t// Return a cleanup function for this specific connection\n\treturn func(err error) {\n\t\tc.mu.Lock()\n\t\tdefer c.mu.Unlock()\n\n\t\tif c.activeAddr == addr {\n\t\t\t// This was the pinned connection, so unpin it\n\t\t\tc.activeAddr = \"\"\n\t\t\tc.isReady = false\n\t\t\tc.readyCond.Broadcast() // Notify waiters that state changed (now not ready)\n\t\t\tc.sendNotification(c.getAvailableAddressesLocked()) // Notify with full list\n\t\t}\n\t}, nil\n}\n\n// Notify returns a channel for receiving address updates.\nfunc (c *ConnectionPoolManager) Notify() <-chan []string {\n\treturn c.notifyCh\n}\n\n// WaitForReady blocks until at least one connection is established or timeout occurs.\nfunc (c *ConnectionPoolManager) WaitForReady(timeout time.Duration) error {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.isReady {\n\t\treturn nil\n\t}\n\n\t// Check for shutdown before waiting\n\tselect {\n\tcase <-c.closed:\n\t\treturn errors.New(\"connection pool manager closed\")\n\tdefault:\n\t\t// continue\n\t}\n\n\tdeadline := time.Now().Add(timeout)\n\tfor {\n\t\tif c.isReady {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Check for manager closure while waiting\n\t\tselect {\n\t\tcase <-c.closed:\n\t\t\treturn errors.New(\"connection pool manager closed\")\n\t\tdefault:\n\t\t\t// not closed, continue\n\t\t}\n\n\t\tif time.Now().After(deadline) {\n\t\t\treturn context.DeadlineExceeded\n\t\t}\n\n\t\tc.readyCond.Wait() // Releases mutex, waits, then re-acquires mutex\n\t}\n}\n\n// Close gracefully shuts down the manager. It can be called multiple times safely.\nfunc (c *ConnectionPoolManager) Close() error {\n\tvar err error\n\tc.closeOnce.Do(func() {\n\t\t// Signal all internal goroutines to stop\n\t\tclose(c.closed)\n\n\t\tc.mu.Lock()\n\t\tc.activeAddr = \"\"\n\t\tc.isReady = false\n\t\tc.readyCond.Broadcast() // Wake up any WaitForReady calls\n\t\tc.mu.Unlock()\n\n\t\t// Wait for all internal goroutines started by sendNotification to finish\n\t\tc.wg.Wait()\n\n\t\t// Close the notification channel after all potential senders have stopped\n\t\tclose(c.notifyCh)\n\t})\n\treturn err\n}\n\n// sendNotification attempts to send a notification to the notifyCh.\n// It runs in a separate goroutine to avoid blocking the caller.\n// It is non-blocking with respect to the manager's state changes.\nfunc (c *ConnectionPoolManager) sendNotification(addrs []string) {\n\tc.wg.Add(1)\n\tgo func() {\n\t\tdefer c.wg.Done()\n\t\tselect {\n\t\tcase <-c.closed:\n\t\t\t// Manager is shutting down, don't send notifications\n\t\t\treturn\n\t\tcase c.notifyCh <- addrs:\n\t\t\t// Notification sent\n\t\tdefault:\n\t\t\t// Channel full (buffer of 1), drop notification.\n\t\t\t// This ensures only the latest state is held for slow consumers.\n\t\t}\n\t}()\n}\n\n// ------------------------------------------------------------------------------------------\n// Test Scenario Implementation (adapted to be runnable in main)\n// ------------------------------------------------------------------------------------------\n\n// fakeEndpoint simulates a server endpoint that accepts connections and immediately closes them.\nfunc fakeEndpoint(addr string, wg *sync.WaitGroup, stopCh <-chan struct{}) (*net.Listener, error) {\n\tlistener, err := net.Listen(\"tcp\", addr)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to listen on %s: %w\", addr, err)\n\t}\n\tfmt.Printf(\"Fake server listening on %s\\n\", addr)\n\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-stopCh:\n\t\t\t\tlistener.Close()\n\t\t\t\tfmt.Printf(\"Fake server on %s shutting down.\\n\", addr)\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\t// Set a deadline for Accept to be interruptible by stopCh\n\t\t\t\tlistener.(*net.TCPListener).SetDeadline(time.Now().Add(100 * time.Millisecond))\n\t\t\t\tconn, err := listener.Accept()\n\t\t\t\tif err != nil {\n\t\t\t\t\tif opErr, ok := err.(*net.OpError); ok && opErr.Timeout() {\n\t\t\t\t\t\tcontinue // Timeout, check stopCh again\n\t\t\t\t\t}\n\t\t\t\t\tif !errors.Is(err, net.ErrClosed) {\n\t\t\t\t\t\tfmt.Printf(\"Fake server on %s accept error: %v\\n\", addr, err)\n\t\t\t\t\t}\n\t\t\t\t\tcontinue // Listener likely closed or other non-fatal error\n\t\t\t\t}\n\t\t\t\tconn.Close() // Immediately close the connection\n\t\t\t}\n\t\t}\n\t}()\n\treturn &listener, nil\n}\n\n// simulateClient connects to endpoints and interacts with the ConnectionPoolManager.\nfunc simulateClient(mgr *ConnectionPoolManager, addrs []string, clientWG *sync.WaitGroup, stopCh <-chan struct{}, clientID int) {\n\tdefer clientWG.Done()\n\n\tfmt.Printf(\"Client %d starting...\\n\", clientID)\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"Client %d stopping.\\n\", clientID)\n\t\t\treturn\n\t\tdefault:\n\t\t\taddr := addrs[clientID%len(addrs)] // Simple way to pick an address\n\n\t\t\tcleanup, err := mgr.Up(addr)\n\t\t\tif err != nil {\n\t\t\t\ttime.Sleep(10 * time.Millisecond) // Wait a bit before retrying\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\ttime.Sleep(50 * time.Millisecond) // Simulate short active duration\n\n\t\t\tcleanup(nil) // Simulate connection going down\n\t\t\ttime.Sleep(50 * time.Millisecond) // Wait before trying another connection\n\t\t}\n\t}\n}\n\n// notificationListener listens for notifications from the manager.\nfunc notificationListener(mgr *ConnectionPoolManager, listenerWG *sync.WaitGroup, stopCh <-chan struct{}, listenerID int) {\n\tdefer listenerWG.Done()\n\tfmt.Printf(\"Notification Listener %d starting...\\n\", listenerID)\n\tnotifyCh := mgr.Notify()\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"Notification Listener %d stopping.\\n\", listenerID)\n\t\t\treturn\n\t\tcase _, ok := <-notifyCh:\n\t\t\tif !ok {\n\t\t\t\tfmt.Printf(\"Notification Listener %d: Notify channel closed.\\n\", listenerID)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// fmt.Printf(\"Notification Listener %d received update: %v\\n\", listenerID, addrs)\n\t\t}\n\t}\n}\n\n// waitForReadyCaller continuously calls WaitForReady.\nfunc waitForReadyCaller(mgr *ConnectionPoolManager, callerWG *sync.WaitGroup, stopCh <-chan struct{}, callerID int) {\n\tdefer callerWG.Done()\n\tfmt.Printf(\"WaitForReady Caller %d starting...\\n\", callerID)\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"WaitForReady Caller %d stopping.\\n\", callerID)\n\t\t\treturn\n\t\tdefault:\n\t\t\ttime.Sleep(10 * time.Millisecond) // Small sleep to prevent busy-looping excessively\n\n\t\t\terr := mgr.WaitForReady(100 * time.Millisecond) // Short timeout for polling\n\t\t\tif err == nil {\n\t\t\t\ttime.Sleep(50 * time.Millisecond) // Wait if ready\n\t\t\t} else if errors.Is(err, context.DeadlineExceeded) {\n\t\t\t\t// fmt.Printf(\"WaitForReady Caller %d: Timeout waiting for ready.\\n\", callerID)\n\t\t\t} else {\n\t\t\t\tfmt.Printf(\"WaitForReady Caller %d: WaitForReady error: %v\\n\", callerID, err)\n\t\t\t\tif errors.Is(err, errors.New(\"connection pool manager closed\")) {\n\t\t\t\t\treturn // Manager closed, exit\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc main() {\n\tnumTestRuns := 3 // How many times to create/close the manager rapidly\n\tnumServers := 2\n\tnumClients := 5\n\tnumNotifyListeners := 2\n\tnumWaitForReadyCallers := 3\n\tcloseTimeout := 3 * time.Second\n\n\t// Use an atomic counter to simulate test failures\n\tvar failures atomic.Int32\n\n\tfor i := 0; i < numTestRuns; i++ {\n\t\tfmt.Printf(\"\\n--- Test Run %d/%d ---\\n\", i+1, numTestRuns)\n\n\t\t// 1. Setup fake servers\n\t\tvar serverWG sync.WaitGroup\n\t\tserverStopCh := make(chan struct{})\n\t\tserverAddresses := make([]string, numServers)\n\t\tvar listeners []*net.Listener\n\n\t\tfor s := 0; s < numServers; s++ {\n\t\t\taddr := fmt.Sprintf(\"127.0.0.1:%d\", 8080+s+(i*10)) // Use different ports for different runs\n\t\t\tlistener, err := fakeEndpoint(addr, &serverWG, serverStopCh)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Failed to start fake server %s: %v\\n\", addr, err)\n\t\t\t\tfailures.Add(1)\n\t\t\t\tgoto cleanupAndContinue // Skip to cleanup for this run\n\t\t\t}\n\t\t\tserverAddresses[s] = addr\n\t\t\tlisteners = append(listeners, listener)\n\t\t}\n\n\t\t// 2. Create ConnectionPoolManager\n\t\tmgr := NewConnectionPoolManager(serverAddresses)\n\n\t\t// 3. Start concurrent clients, notification listeners, and WaitForReady callers\n\t\tvar clientWG sync.WaitGroup\n\t\tclientStopCh := make(chan struct{}) // Shared stop channel for all concurrent ops\n\n\t\tfor c := 0; c < numClients; c++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo simulateClient(mgr, serverAddresses, &clientWG, clientStopCh, c)\n\t\t}\n\t\tfor n := 0; n < numNotifyListeners; n++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo notificationListener(mgr, &clientWG, clientStopCh, n)\n\t\t}\n\t\tfor w := 0; w < numWaitForReadyCallers; w++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo waitForReadyCaller(mgr, &clientWG, clientStopCh, w)\n\t\t}\n\n\t\t// 4. Let the system run for a bit\n\t\tfmt.Println(\"Running simulation for 500ms...\")\n\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t// 5. Graceful Shutdown\n\t\tfmt.Println(\"Calling mgr.Close()...\")\n\t\tcloseDone := make(chan struct{})\n\t\tgo func() {\n\t\t\tdefer close(closeDone)\n\t\t\tif err := mgr.Close(); err != nil {\n\t\t\t\tfmt.Printf(\"Manager Close() returned an error: %v\\n\", err)\n\t\t\t\tfailures.Add(1)\n\t\t\t}\n\t\t}()\n\n\t\tselect {\n\t\tcase <-time.After(closeTimeout):\n\t\t\tfmt.Printf(\"ERROR: mgr.Close() did not complete within %v\\n\", closeTimeout)\n\t\t\tfailures.Add(1)\n\t\tcase <-closeDone:\n\t\t\tfmt.Println(\"mgr.Close() completed successfully.\")\n\t\t}\n\n\t\t// Signal all concurrent client goroutines to stop\n\t\tclose(clientStopCh)\n\t\tclientWG.Wait() // Wait for all clients to finish\n\t\tfmt.Println(\"All client-side goroutines stopped.\")\n\n\tcleanupAndContinue:\n\t\t// 6. Cleanup fake servers\n\t\tif len(listeners) > 0 {\n\t\t\tclose(serverStopCh) // Signal fake servers to stop\n\t\t\tserverWG.Wait()     // Wait for fake servers to finish\n\t\t\tfor _, l := range listeners {\n\t\t\t\tif *l != nil {\n\t\t\t\t\t(*l).Close()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif failures.Load() > 0 {\n\t\tfmt.Printf(\"\\n--- TEST FAILED with %d errors ---\\n\", failures.Load())\n\t} else {\n\t\tfmt.Println(\"\\n--- ALL TESTS PASSED ---\")\n\t}\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "go build -race",
                    "error": "# command-line-arguments\n./main.go:363:10: goto cleanupAndContinue jumps over declaration of closeDone at ./main.go:395:13"
                },
                {
                    "verifier": "gosec",
                    "error": "Results:\n\nGolang errors in file: []:\n\n  > [line 0 : column 0] - # command-line-arguments\n./main.go:363:10: goto cleanupAndContinue jumps over declaration of closeDone at ./main.go:395:13\n\n\nGolang errors in file: [/tmp/go_synthesis_e_or1ilw/main.go]:\n\n  > [line 363 : column 10] - goto cleanupAndContinue jumps over variable declaration at line 395\n\n\n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:424\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    423: \t\t\t\tif *l != nil {\n  > 424: \t\t\t\t\t(*l).Close()\n    425: \t\t\t\t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:255\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    254: \t\t\t\t}\n  > 255: \t\t\t\tconn.Close() // Immediately close the connection\n    256: \t\t\t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:244\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    243: \t\t\t\t// Set a deadline for Accept to be interruptible by stopCh\n  > 244: \t\t\t\tlistener.(*net.TCPListener).SetDeadline(time.Now().Add(100 * time.Millisecond))\n    245: \t\t\t\tconn, err := listener.Accept()\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:239\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    238: \t\t\tcase <-stopCh:\n  > 239: \t\t\t\tlistener.Close()\n    240: \t\t\t\tfmt.Printf(\"Fake server on %s shutting down.\\n\", addr)\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 435\n  Nosec  : 0\n  Issues : \u001b[1;31m4\u001b[0m\n\n[gosec] 2025/12/01 13:13:42 Including rules: default\n[gosec] 2025/12/01 13:13:42 Excluding rules: default\n[gosec] 2025/12/01 13:13:42 Including analyzers: default\n[gosec] 2025/12/01 13:13:42 Excluding analyzers: default\n[gosec] 2025/12/01 13:13:42 Import directory: /tmp/go_synthesis_e_or1ilw\n[gosec] 2025/12/01 13:13:42 Checking package: main\n[gosec] 2025/12/01 13:13:42 Checking file: /tmp/go_synthesis_e_or1ilw/main.go"
                },
                {
                    "verifier": "govulncheck",
                    "error": "govulncheck: loading packages: \nThere are errors with the provided package patterns:\n\n/tmp/go_synthesis_e_or1ilw/main.go:363:10: goto cleanupAndContinue jumps over variable declaration at line 395\n\nFor details on package patterns, see https://pkg.go.dev/cmd/go#hdr-Package_lists_and_patterns."
                },
                {
                    "verifier": "errcheck",
                    "error": "error: failed to check packages: errors while loading package command-line-arguments: [-: # command-line-arguments\n./main.go:363:10: goto cleanupAndContinue jumps over declaration of closeDone at ./main.go:395:13 /tmp/go_synthesis_e_or1ilw/main.go:363:10: goto cleanupAndContinue jumps over variable declaration at line 395]"
                },
                {
                    "verifier": "go-errorlint",
                    "error": "-: # command-line-arguments\n./main.go:363:10: goto cleanupAndContinue jumps over declaration of closeDone at ./main.go:395:13\n/tmp/go_synthesis_e_or1ilw/main.go:363:10: goto cleanupAndContinue jumps over variable declaration at line 395\nerrorlint: analysis skipped due to errors in package"
                },
                {
                    "verifier": "staticcheck",
                    "error": "-: # command-line-arguments\n./main.go:363:10: goto cleanupAndContinue jumps over declaration of closeDone at ./main.go:395:13 (compile)"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\n// ConnectionPoolManager manages server endpoints and maintains a single active connection.\ntype ConnectionPoolManager struct {\n\tmu          sync.Mutex\n\taddresses   map[string]struct{} // Set of all known addresses\n\tactiveAddr  string              // The currently pinned address, empty if none\n\n\tnotifyCh    chan []string       // For consumers to get updates (buffered 1)\n\n\tisReady     bool                // True if an activeAddr is pinned\n\treadyCond   *sync.Cond          // Condition variable for WaitForReady\n\n\tcloseOnce   sync.Once\n\tclosed      chan struct{}       // Signal for internal goroutines to shut down\n\twg          sync.WaitGroup      // WaitGroup for tracking internal goroutines\n}\n\n// NewConnectionPoolManager creates and initializes a new manager.\nfunc NewConnectionPoolManager(initialAddrs []string) *ConnectionPoolManager {\n\tmgr := &ConnectionPoolManager{\n\t\taddresses: make(map[string]struct{}),\n\t\tnotifyCh:  make(chan []string, 1), // Buffered to hold the latest state\n\t\tclosed:    make(chan struct{}),\n\t}\n\tmgr.readyCond = sync.NewCond(&mgr.mu)\n\n\t// Use UpdateAddresses for initial population and notification\n\tmgr.UpdateAddresses(initialAddrs)\n\treturn mgr\n}\n\n// UpdateAddresses dynamically updates the list of available server addresses.\nfunc (c *ConnectionPoolManager) UpdateAddresses(newAddrs []string) {\n\tselect {\n\tcase <-c.closed:\n\t\treturn // Manager is closed, ignore updates\n\tdefault:\n\t\t// continue\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tnewAddrMap := make(map[string]struct{})\n\tfor _, addr := range newAddrs {\n\t\tnewAddrMap[addr] = struct{}{}\n\t}\n\n\t// Check if the current active address is still valid\n\toldActiveAddr := c.activeAddr\n\t_, activeStillExists := newAddrMap[oldActiveAddr]\n\n\tif oldActiveAddr != \"\" && !activeStillExists {\n\t\t// Pinned connection is no longer in the list, unpin it.\n\t\tc.activeAddr = \"\"\n\t\tc.isReady = false\n\t\tc.readyCond.Broadcast() // Notify waiters that state changed (now not ready)\n\t}\n\n\tc.addresses = newAddrMap\n\n\t// Send notification based on current state\n\tif c.activeAddr != \"\" {\n\t\tc.sendNotification([]string{c.activeAddr})\n\t} else {\n\t\tc.sendNotification(c.getAvailableAddressesLocked())\n\t}\n}\n\n// getAvailableAddressesLocked returns the current list of available addresses.\n// It assumes the mutex is already held.\nfunc (c *ConnectionPoolManager) getAvailableAddressesLocked() []string {\n\taddrs := make([]string, 0, len(c.addresses))\n\tfor addr := range c.addresses {\n\t\taddrs = append(addrs, addr)\n\t}\n\treturn addrs\n}\n\n// Up is called when a connection to an address is established.\n// It returns a cleanup function to be called when the connection goes down.\nfunc (c *ConnectionPoolManager) Up(addr string) (func(error), error) {\n\tselect {\n\tcase <-c.closed:\n\t\treturn func(error) {}, errors.New(\"manager is closed, cannot establish new connections\")\n\tdefault:\n\t\t// continue\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif _, ok := c.addresses[addr]; !ok {\n\t\treturn func(error) {}, fmt.Errorf(\"address %s is not a known endpoint\", addr)\n\t}\n\n\tisInitialPin := false\n\tif c.activeAddr == \"\" {\n\t\tc.activeAddr = addr\n\t\tc.isReady = true\n\t\tc.readyCond.Broadcast() // Notify waiters that a connection is pinned\n\t\tisInitialPin = true\n\t}\n\n\tif isInitialPin {\n\t\tc.sendNotification([]string{addr}) // Notify with pinned address\n\t}\n\n\t// Return a cleanup function for this specific connection\n\treturn func(err error) {\n\t\tc.mu.Lock()\n\t\tdefer c.mu.Unlock()\n\n\t\tif c.activeAddr == addr {\n\t\t\t// This was the pinned connection, so unpin it\n\t\t\tc.activeAddr = \"\"\n\t\t\tc.isReady = false\n\t\t\tc.readyCond.Broadcast() // Notify waiters that state changed (now not ready)\n\t\t\tc.sendNotification(c.getAvailableAddressesLocked()) // Notify with full list\n\t\t}\n\t}, nil\n}\n\n// Notify returns a channel for receiving address updates.\nfunc (c *ConnectionPoolManager) Notify() <-chan []string {\n\treturn c.notifyCh\n}\n\n// WaitForReady blocks until at least one connection is established or timeout occurs.\nfunc (c *ConnectionPoolManager) WaitForReady(timeout time.Duration) error {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.isReady {\n\t\treturn nil\n\t}\n\n\t// Check for shutdown before waiting\n\tselect {\n\tcase <-c.closed:\n\t\treturn errors.New(\"connection pool manager closed\")\n\tdefault:\n\t\t// continue\n\t}\n\n\tdeadline := time.Now().Add(timeout)\n\tfor {\n\t\tif c.isReady {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Check for manager closure while waiting\n\t\tselect {\n\t\tcase <-c.closed:\n\t\t\treturn errors.New(\"connection pool manager closed\")\n\t\tdefault:\n\t\t\t// not closed, continue\n\t\t}\n\n\t\tif time.Now().After(deadline) {\n\t\t\treturn context.DeadlineExceeded\n\t\t}\n\n\t\tc.readyCond.Wait() // Releases mutex, waits, then re-acquires mutex\n\t}\n}\n\n// Close gracefully shuts down the manager. It can be called multiple times safely.\nfunc (c *ConnectionPoolManager) Close() error {\n\tvar err error\n\tc.closeOnce.Do(func() {\n\t\t// Signal all internal goroutines to stop\n\t\tclose(c.closed)\n\n\t\tc.mu.Lock()\n\t\tc.activeAddr = \"\"\n\t\tc.isReady = false\n\t\tc.readyCond.Broadcast() // Wake up any WaitForReady calls\n\t\tc.mu.Unlock()\n\n\t\t// Wait for all internal goroutines started by sendNotification to finish\n\t\tc.wg.Wait()\n\n\t\t// Close the notification channel after all potential senders have stopped\n\t\tclose(c.notifyCh)\n\t})\n\treturn err\n}\n\n// sendNotification attempts to send a notification to the notifyCh.\n// It runs in a separate goroutine to avoid blocking the caller.\n// It is non-blocking with respect to the manager's state changes.\nfunc (c *ConnectionPoolManager) sendNotification(addrs []string) {\n\tc.wg.Add(1)\n\tgo func() {\n\t\tdefer c.wg.Done()\n\t\tselect {\n\t\tcase <-c.closed:\n\t\t\t// Manager is shutting down, don't send notifications\n\t\t\treturn\n\t\tcase c.notifyCh <- addrs:\n\t\t\t// Notification sent\n\t\tdefault:\n\t\t\t// Channel full (buffer of 1), drop notification.\n\t\t\t// This ensures only the latest state is held for slow consumers.\n\t\t}\n\t}()\n}\n\n// ------------------------------------------------------------------------------------------\n// Test Scenario Implementation (adapted to be runnable in main)\n// ------------------------------------------------------------------------------------------\n\n// fakeEndpoint simulates a server endpoint that accepts connections and immediately closes them.\nfunc fakeEndpoint(addr string, wg *sync.WaitGroup, stopCh <-chan struct{}) (*net.Listener, error) {\n\tlistener, err := net.Listen(\"tcp\", addr)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to listen on %s: %w\", addr, err)\n\t}\n\tfmt.Printf(\"Fake server listening on %s\\n\", addr)\n\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-stopCh:\n\t\t\t\tlistener.Close()\n\t\t\t\tfmt.Printf(\"Fake server on %s shutting down.\\n\", addr)\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\t// Set a deadline for Accept to be interruptible by stopCh\n\t\t\t\tlistener.(*net.TCPListener).SetDeadline(time.Now().Add(100 * time.Millisecond))\n\t\t\t\tconn, err := listener.Accept()\n\t\t\t\tif err != nil {\n\t\t\t\t\tif opErr, ok := err.(*net.OpError); ok && opErr.Timeout() {\n\t\t\t\t\t\tcontinue // Timeout, check stopCh again\n\t\t\t\t\t}\n\t\t\t\t\tif !errors.Is(err, net.ErrClosed) {\n\t\t\t\t\t\tfmt.Printf(\"Fake server on %s accept error: %v\\n\", addr, err)\n\t\t\t\t\t}\n\t\t\t\t\tcontinue // Listener likely closed or other non-fatal error\n\t\t\t\t}\n\t\t\t\tconn.Close() // Immediately close the connection\n\t\t\t}\n\t\t}\n\t}()\n\treturn &listener, nil\n}\n\n// simulateClient connects to endpoints and interacts with the ConnectionPoolManager.\nfunc simulateClient(mgr *ConnectionPoolManager, addrs []string, clientWG *sync.WaitGroup, stopCh <-chan struct{}, clientID int) {\n\tdefer clientWG.Done()\n\n\tfmt.Printf(\"Client %d starting...\\n\", clientID)\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"Client %d stopping.\\n\", clientID)\n\t\t\treturn\n\t\tdefault:\n\t\t\taddr := addrs[clientID%len(addrs)] // Simple way to pick an address\n\n\t\t\tcleanup, err := mgr.Up(addr)\n\t\t\tif err != nil {\n\t\t\t\ttime.Sleep(10 * time.Millisecond) // Wait a bit before retrying\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\ttime.Sleep(50 * time.Millisecond) // Simulate short active duration\n\n\t\t\tcleanup(nil) // Simulate connection going down\n\t\t\ttime.Sleep(50 * time.Millisecond) // Wait before trying another connection\n\t\t}\n\t}\n}\n\n// notificationListener listens for notifications from the manager.\nfunc notificationListener(mgr *ConnectionPoolManager, listenerWG *sync.WaitGroup, stopCh <-chan struct{}, listenerID int) {\n\tdefer listenerWG.Done()\n\tfmt.Printf(\"Notification Listener %d starting...\\n\", listenerID)\n\tnotifyCh := mgr.Notify()\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"Notification Listener %d stopping.\\n\", listenerID)\n\t\t\treturn\n\t\tcase _, ok := <-notifyCh:\n\t\t\tif !ok {\n\t\t\t\tfmt.Printf(\"Notification Listener %d: Notify channel closed.\\n\", listenerID)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// fmt.Printf(\"Notification Listener %d received update: %v\\n\", listenerID, addrs)\n\t\t}\n\t}\n}\n\n// waitForReadyCaller continuously calls WaitForReady.\nfunc waitForReadyCaller(mgr *ConnectionPoolManager, callerWG *sync.WaitGroup, stopCh <-chan struct{}, callerID int) {\n\tdefer callerWG.Done()\n\tfmt.Printf(\"WaitForReady Caller %d starting...\\n\", callerID)\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"WaitForReady Caller %d stopping.\\n\", callerID)\n\t\t\treturn\n\t\tdefault:\n\t\t\ttime.Sleep(10 * time.Millisecond) // Small sleep to prevent busy-looping excessively\n\n\t\t\terr := mgr.WaitForReady(100 * time.Millisecond) // Short timeout for polling\n\t\t\tif err == nil {\n\t\t\t\ttime.Sleep(50 * time.Millisecond) // Wait if ready\n\t\t\t} else if errors.Is(err, context.DeadlineExceeded) {\n\t\t\t\t// fmt.Printf(\"WaitForReady Caller %d: Timeout waiting for ready.\\n\", callerID)\n\t\t\t} else {\n\t\t\t\tfmt.Printf(\"WaitForReady Caller %d: WaitForReady error: %v\\n\", callerID, err)\n\t\t\t\tif errors.Is(err, errors.New(\"connection pool manager closed\")) {\n\t\t\t\t\treturn // Manager closed, exit\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc main() {\n\tnumTestRuns := 3 // How many times to create/close the manager rapidly\n\tnumServers := 2\n\tnumClients := 5\n\tnumNotifyListeners := 2\n\tnumWaitForReadyCallers := 3\n\tcloseTimeout := 3 * time.Second\n\n\t// Use an atomic counter to simulate test failures\n\tvar failures atomic.Int32\n\n\tfor i := 0; i < numTestRuns; i++ {\n\t\tfmt.Printf(\"\\n--- Test Run %d/%d ---\\n\", i+1, numTestRuns)\n\n\t\t// Declare all loop-scoped variables at the top to avoid 'goto jumps over declaration' errors\n\t\tvar mgr *ConnectionPoolManager\n\t\tcloseDone := make(chan struct{}) // For mgr.Close() completion\n\t\tvar serverWG sync.WaitGroup\n\t\tserverStopCh := make(chan struct{}) // For fake servers\n\t\tserverAddresses := make([]string, numServers)\n\t\tvar listeners []*net.Listener\n\t\tvar clientWG sync.WaitGroup // Shared waitgroup for all client-side goroutines\n\t\tclientStopCh := make(chan struct{}) // Shared stop channel for all client-side goroutines\n\n\t\t// 1. Setup fake servers\n\t\thasServerErrors := false\n\t\tfor s := 0; s < numServers; s++ {\n\t\t\taddr := fmt.Sprintf(\"127.0.0.1:%d\", 8080+s+(i*10)) // Use different ports for different runs\n\t\t\tlistener, err := fakeEndpoint(addr, &serverWG, serverStopCh)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Failed to start fake server %s: %v\\n\", addr, err)\n\t\t\t\tfailures.Add(1)\n\t\t\t\thasServerErrors = true\n\t\t\t\tbreak // Stop trying to start servers for this run\n\t\t\t}\n\t\t\tserverAddresses[s] = addr\n\t\t\tlisteners = append(listeners, listener)\n\t\t}\n\n\t\t// If server setup failed, jump to cleanup directly\n\t\tif hasServerErrors {\n\t\t\tgoto cleanupAndContinue\n\t\t}\n\n\t\t// 2. Create ConnectionPoolManager (only if servers are up)\n\t\tmgr = NewConnectionPoolManager(serverAddresses)\n\n\t\t// 3. Start concurrent clients, notification listeners, and WaitForReady callers\n\t\tfor c := 0; c < numClients; c++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo simulateClient(mgr, serverAddresses, &clientWG, clientStopCh, c)\n\t\t}\n\t\tfor n := 0; n < numNotifyListeners; n++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo notificationListener(mgr, &clientWG, clientStopCh, n)\n\t\t}\n\t\tfor w := 0; w < numWaitForReadyCallers; w++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo waitForReadyCaller(mgr, &clientWG, clientStopCh, w)\n\t\t}\n\n\t\t// 4. Let the system run for a bit\n\t\tfmt.Println(\"Running simulation for 500ms...\")\n\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t// 5. Graceful Shutdown\n\t\tfmt.Println(\"Calling mgr.Close()...\")\n\t\tgo func() {\n\t\t\tdefer close(closeDone)\n\t\t\tif err := mgr.Close(); err != nil {\n\t\t\t\tfmt.Printf(\"Manager Close() returned an error: %v\\n\", err)\n\t\t\t\tfailures.Add(1)\n\t\t\t}\n\t\t}()\n\n\t\tselect {\n\t\tcase <-time.After(closeTimeout):\n\t\t\tfmt.Printf(\"ERROR: mgr.Close() did not complete within %v\\n\", closeTimeout)\n\t\t\tfailures.Add(1)\n\t\tcase <-closeDone:\n\t\t\tfmt.Println(\"mgr.Close() completed successfully.\")\n\t\t}\n\n\t\t// Signal all concurrent client goroutines to stop\n\t\tclose(clientStopCh)\n\t\tclientWG.Wait() // Wait for all clients to finish\n\t\tfmt.Println(\"All client-side goroutines stopped.\")\n\n\tcleanupAndContinue:\n\t\t// 6. Cleanup fake servers\n\t\tif len(listeners) > 0 { // Only close serverStopCh if listeners were successfully created\n\t\t\tclose(serverStopCh) // Signal fake servers to stop\n\t\t\tserverWG.Wait()     // Wait for fake servers to finish\n\t\t\tfor _, l := range listeners {\n\t\t\t\tif *l != nil {\n\t\t\t\t\tif err := (*l).Close(); err != nil { // Handle error from Close()\n\t\t\t\t\t\t// It's common for Accept to return net.ErrClosed after listener.Close() is called.\n\t\t\t\t\t\t// Only print actual unexpected errors.\n\t\t\t\t\t\tif !errors.Is(err, net.ErrClosed) {\n\t\t\t\t\t\t\tfmt.Printf(\"Warning: error closing listener on %s: %v\\n\", (*l).Addr().String(), err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif failures.Load() > 0 {\n\t\tfmt.Printf(\"\\n--- TEST FAILED with %d errors ---\\n\", failures.Load())\n\t} else {\n\t\tfmt.Println(\"\\n--- ALL TESTS PASSED ---\")\n\t}\n}",
            "passed": false,
            "errors": [
                {
                    "verifier": "gosec",
                    "error": "Results:\n\n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:255\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    254: \t\t\t\t}\n  > 255: \t\t\t\tconn.Close() // Immediately close the connection\n    256: \t\t\t}\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:244\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    243: \t\t\t\t// Set a deadline for Accept to be interruptible by stopCh\n  > 244: \t\t\t\tlistener.(*net.TCPListener).SetDeadline(time.Now().Add(100 * time.Millisecond))\n    245: \t\t\t\tconn, err := listener.Accept()\n\nAutofix: \n\n[\u001b[37;40m/tmp/go_synthesis_e_or1ilw/main.go:239\u001b[0m] - G104 (CWE-703): Errors unhandled (Confidence: HIGH, Severity: LOW)\n    238: \t\t\tcase <-stopCh:\n  > 239: \t\t\t\tlistener.Close()\n    240: \t\t\t\tfmt.Printf(\"Fake server on %s shutting down.\\n\", addr)\n\nAutofix: \n\n\u001b[1;36mSummary:\u001b[0m\n  Gosec  : dev\n  Files  : 1\n  Lines  : 449\n  Nosec  : 0\n  Issues : \u001b[1;31m3\u001b[0m\n\n[gosec] 2025/12/01 13:14:14 Including rules: default\n[gosec] 2025/12/01 13:14:14 Excluding rules: default\n[gosec] 2025/12/01 13:14:14 Including analyzers: default\n[gosec] 2025/12/01 13:14:14 Excluding analyzers: default\n[gosec] 2025/12/01 13:14:14 Import directory: /tmp/go_synthesis_e_or1ilw\n[gosec] 2025/12/01 13:14:14 Checking package: main\n[gosec] 2025/12/01 13:14:14 Checking file: /tmp/go_synthesis_e_or1ilw/main.go"
                },
                {
                    "verifier": "errcheck",
                    "error": "main.go:239:19:\tlistener.Close()\nmain.go:244:44:\tlistener.(*net.TCPListener).SetDeadline(time.Now().Add(100 * time.Millisecond))\nmain.go:255:15:\tconn.Close() // Immediately close the connection"
                },
                {
                    "verifier": "go-errorlint",
                    "error": "/tmp/go_synthesis_e_or1ilw/main.go:247:22: type assertion on error will fail on wrapped errors. Use errors.As to check for specific errors"
                }
            ]
        },
        {
            "code": "package main\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\n// ConnectionPoolManager manages server endpoints and maintains a single active connection.\ntype ConnectionPoolManager struct {\n\tmu          sync.Mutex\n\taddresses   map[string]struct{} // Set of all known addresses\n\tactiveAddr  string              // The currently pinned address, empty if none\n\n\tnotifyCh    chan []string       // For consumers to get updates (buffered 1)\n\n\tisReady     bool                // True if an activeAddr is pinned\n\treadyCond   *sync.Cond          // Condition variable for WaitForReady\n\n\tcloseOnce   sync.Once\n\tclosed      chan struct{}       // Signal for internal goroutines to shut down\n\twg          sync.WaitGroup      // WaitGroup for tracking internal goroutines\n}\n\n// NewConnectionPoolManager creates and initializes a new manager.\nfunc NewConnectionPoolManager(initialAddrs []string) *ConnectionPoolManager {\n\tmgr := &ConnectionPoolManager{\n\t\taddresses: make(map[string]struct{}),\n\t\tnotifyCh:  make(chan []string, 1), // Buffered to hold the latest state\n\t\tclosed:    make(chan struct{}),\n\t}\n\tmgr.readyCond = sync.NewCond(&mgr.mu)\n\n\t// Use UpdateAddresses for initial population and notification\n\tmgr.UpdateAddresses(initialAddrs)\n\treturn mgr\n}\n\n// UpdateAddresses dynamically updates the list of available server addresses.\nfunc (c *ConnectionPoolManager) UpdateAddresses(newAddrs []string) {\n\tselect {\n\tcase <-c.closed:\n\t\treturn // Manager is closed, ignore updates\n\tdefault:\n\t\t// continue\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tnewAddrMap := make(map[string]struct{})\n\tfor _, addr := range newAddrs {\n\t\tnewAddrMap[addr] = struct{}{}\n\t}\n\n\t// Check if the current active address is still valid\n\toldActiveAddr := c.activeAddr\n\t_, activeStillExists := newAddrMap[oldActiveAddr]\n\n\tif oldActiveAddr != \"\" && !activeStillExists {\n\t\t// Pinned connection is no longer in the list, unpin it.\n\t\tc.activeAddr = \"\"\n\t\tc.isReady = false\n\t\tc.readyCond.Broadcast() // Notify waiters that state changed (now not ready)\n\t}\n\n\tc.addresses = newAddrMap\n\n\t// Send notification based on current state\n\tif c.activeAddr != \"\" {\n\t\tc.sendNotification([]string{c.activeAddr})\n\t} else {\n\t\tc.sendNotification(c.getAvailableAddressesLocked())\n\t}\n}\n\n// getAvailableAddressesLocked returns the current list of available addresses.\n// It assumes the mutex is already held.\nfunc (c *ConnectionPoolManager) getAvailableAddressesLocked() []string {\n\taddrs := make([]string, 0, len(c.addresses))\n\tfor addr := range c.addresses {\n\t\taddrs = append(addrs, addr)\n\t}\n\treturn addrs\n}\n\n// Up is called when a connection to an address is established.\n// It returns a cleanup function to be called when the connection goes down.\nfunc (c *ConnectionPoolManager) Up(addr string) (func(error), error) {\n\tselect {\n\tcase <-c.closed:\n\t\treturn func(error) {}, errors.New(\"manager is closed, cannot establish new connections\")\n\tdefault:\n\t\t// continue\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif _, ok := c.addresses[addr]; !ok {\n\t\treturn func(error) {}, fmt.Errorf(\"address %s is not a known endpoint\", addr)\n\t}\n\n\tisInitialPin := false\n\tif c.activeAddr == \"\" {\n\t\tc.activeAddr = addr\n\t\tc.isReady = true\n\t\tc.readyCond.Broadcast() // Notify waiters that a connection is pinned\n\t\tisInitialPin = true\n\t}\n\n\tif isInitialPin {\n\t\tc.sendNotification([]string{addr}) // Notify with pinned address\n\t}\n\n\t// Return a cleanup function for this specific connection\n\treturn func(err error) {\n\t\tc.mu.Lock()\n\t\tdefer c.mu.Unlock()\n\n\t\tif c.activeAddr == addr {\n\t\t\t// This was the pinned connection, so unpin it\n\t\t\tc.activeAddr = \"\"\n\t\t\tc.isReady = false\n\t\t\tc.readyCond.Broadcast() // Notify waiters that state changed (now not ready)\n\t\t\tc.sendNotification(c.getAvailableAddressesLocked()) // Notify with full list\n\t\t}\n\t}, nil\n}\n\n// Notify returns a channel for receiving address updates.\nfunc (c *ConnectionPoolManager) Notify() <-chan []string {\n\treturn c.notifyCh\n}\n\n// WaitForReady blocks until at least one connection is established or timeout occurs.\nfunc (c *ConnectionPoolManager) WaitForReady(timeout time.Duration) error {\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\n\tif c.isReady {\n\t\treturn nil\n\t}\n\n\t// Check for shutdown before waiting\n\tselect {\n\tcase <-c.closed:\n\t\treturn errors.New(\"connection pool manager closed\")\n\tdefault:\n\t\t// continue\n\t}\n\n\tdeadline := time.Now().Add(timeout)\n\tfor {\n\t\tif c.isReady {\n\t\t\treturn nil\n\t\t}\n\n\t\t// Check for manager closure while waiting\n\t\tselect {\n\t\tcase <-c.closed:\n\t\t\treturn errors.New(\"connection pool manager closed\")\n\t\tdefault:\n\t\t\t// not closed, continue\n\t\t}\n\n\t\tif time.Now().After(deadline) {\n\t\t\treturn context.DeadlineExceeded\n\t\t}\n\n\t\tc.readyCond.Wait() // Releases mutex, waits, then re-acquires mutex\n\t}\n}\n\n// Close gracefully shuts down the manager. It can be called multiple times safely.\nfunc (c *ConnectionPoolManager) Close() error {\n\tvar err error\n\tc.closeOnce.Do(func() {\n\t\t// Signal all internal goroutines to stop\n\t\tclose(c.closed)\n\n\t\tc.mu.Lock()\n\t\tc.activeAddr = \"\"\n\t\tc.isReady = false\n\t\tc.readyCond.Broadcast() // Wake up any WaitForReady calls\n\t\tc.mu.Unlock()\n\n\t\t// Wait for all internal goroutines started by sendNotification to finish\n\t\tc.wg.Wait()\n\n\t\t// Close the notification channel after all potential senders have stopped\n\t\tclose(c.notifyCh)\n\t})\n\treturn err\n}\n\n// sendNotification attempts to send a notification to the notifyCh.\n// It runs in a separate goroutine to avoid blocking the caller.\n// It is non-blocking with respect to the manager's state changes.\nfunc (c *ConnectionPoolManager) sendNotification(addrs []string) {\n\tc.wg.Add(1)\n\tgo func() {\n\t\tdefer c.wg.Done()\n\t\tselect {\n\t\tcase <-c.closed:\n\t\t\t// Manager is shutting down, don't send notifications\n\t\t\treturn\n\t\tcase c.notifyCh <- addrs:\n\t\t\t// Notification sent\n\t\tdefault:\n\t\t\t// Channel full (buffer of 1), drop notification.\n\t\t\t// This ensures only the latest state is held for slow consumers.\n\t\t}\n\t}()\n}\n\n// ------------------------------------------------------------------------------------------\n// Test Scenario Implementation (adapted to be runnable in main)\n// ------------------------------------------------------------------------------------------\n\n// fakeEndpoint simulates a server endpoint that accepts connections and immediately closes them.\nfunc fakeEndpoint(addr string, wg *sync.WaitGroup, stopCh <-chan struct{}) (*net.Listener, error) {\n\tlistener, err := net.Listen(\"tcp\", addr)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to listen on %s: %w\", addr, err)\n\t}\n\tfmt.Printf(\"Fake server listening on %s\\n\", addr)\n\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\ttcpListener, isTCP := listener.(*net.TCPListener) // Perform type assertion once\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-stopCh:\n\t\t\t\tif err := listener.Close(); err != nil && !errors.Is(err, net.ErrClosed) {\n\t\t\t\t\tfmt.Printf(\"Fake server on %s error closing listener: %v\\n\", addr, err)\n\t\t\t\t}\n\t\t\t\tfmt.Printf(\"Fake server on %s shutting down.\\n\", addr)\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\t// Set a deadline for Accept to be interruptible by stopCh\n\t\t\t\tif isTCP {\n\t\t\t\t\tif err := tcpListener.SetDeadline(time.Now().Add(100 * time.Millisecond)); err != nil {\n\t\t\t\t\t\tif errors.Is(err, net.ErrClosed) {\n\t\t\t\t\t\t\t// Listener was already closed, exit goroutine\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfmt.Printf(\"Fake server on %s error setting deadline: %v\\n\", addr, err)\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tconn, err := listener.Accept()\n\t\t\t\tif err != nil {\n\t\t\t\t\tvar opErr *net.OpError\n\t\t\t\t\tif errors.As(err, &opErr) && opErr.Timeout() {\n\t\t\t\t\t\tcontinue // Timeout, check stopCh again\n\t\t\t\t\t}\n\t\t\t\t\tif errors.Is(err, net.ErrClosed) {\n\t\t\t\t\t\t// Listener closed, exit goroutine\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Other errors\n\t\t\t\t\tfmt.Printf(\"Fake server on %s accept error: %v\\n\", addr, err)\n\t\t\t\t\tcontinue // Continue to try accepting or check stopCh\n\t\t\t\t}\n\t\t\t\tif err := conn.Close(); err != nil {\n\t\t\t\t\tfmt.Printf(\"Fake server on %s error closing connection: %v\\n\", addr, err)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\treturn &listener, nil\n}\n\n// simulateClient connects to endpoints and interacts with the ConnectionPoolManager.\nfunc simulateClient(mgr *ConnectionPoolManager, addrs []string, clientWG *sync.WaitGroup, stopCh <-chan struct{}, clientID int) {\n\tdefer clientWG.Done()\n\n\tfmt.Printf(\"Client %d starting...\\n\", clientID)\n\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"Client %d stopping.\\n\", clientID)\n\t\t\treturn\n\t\tdefault:\n\t\t\taddr := addrs[clientID%len(addrs)] // Simple way to pick an address\n\n\t\t\tcleanup, err := mgr.Up(addr)\n\t\t\tif err != nil {\n\t\t\t\ttime.Sleep(10 * time.Millisecond) // Wait a bit before retrying\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\ttime.Sleep(50 * time.Millisecond) // Simulate short active duration\n\n\t\t\tcleanup(nil) // Simulate connection going down\n\t\t\ttime.Sleep(50 * time.Millisecond) // Wait before trying another connection\n\t\t}\n\t}\n}\n\n// notificationListener listens for notifications from the manager.\nfunc notificationListener(mgr *ConnectionPoolManager, listenerWG *sync.WaitGroup, stopCh <-chan struct{}, listenerID int) {\n\tdefer listenerWG.Done()\n\tfmt.Printf(\"Notification Listener %d starting...\\n\", listenerID)\n\tnotifyCh := mgr.Notify()\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"Notification Listener %d stopping.\\n\", listenerID)\n\t\t\treturn\n\t\tcase _, ok := <-notifyCh:\n\t\t\tif !ok {\n\t\t\t\tfmt.Printf(\"Notification Listener %d: Notify channel closed.\\n\", listenerID)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// fmt.Printf(\"Notification Listener %d received update: %v\\n\", listenerID, addrs)\n\t\t}\n\t}\n}\n\n// waitForReadyCaller continuously calls WaitForReady.\nfunc waitForReadyCaller(mgr *ConnectionPoolManager, callerWG *sync.WaitGroup, stopCh <-chan struct{}, callerID int) {\n\tdefer callerWG.Done()\n\tfmt.Printf(\"WaitForReady Caller %d starting...\\n\", callerID)\n\tfor {\n\t\tselect {\n\t\tcase <-stopCh:\n\t\t\tfmt.Printf(\"WaitForReady Caller %d stopping.\\n\", callerID)\n\t\t\treturn\n\t\tdefault:\n\t\t\ttime.Sleep(10 * time.Millisecond) // Small sleep to prevent busy-looping excessively\n\n\t\t\terr := mgr.WaitForReady(100 * time.Millisecond) // Short timeout for polling\n\t\t\tif err == nil {\n\t\t\t\ttime.Sleep(50 * time.Millisecond) // Wait if ready\n\t\t\t} else if errors.Is(err, context.DeadlineExceeded) {\n\t\t\t\t// fmt.Printf(\"WaitForReady Caller %d: Timeout waiting for ready.\\n\", callerID)\n\t\t\t} else {\n\t\t\t\tfmt.Printf(\"WaitForReady Caller %d: WaitForReady error: %v\\n\", callerID, err)\n\t\t\t\tif errors.Is(err, errors.New(\"connection pool manager closed\")) {\n\t\t\t\t\treturn // Manager closed, exit\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc main() {\n\tnumTestRuns := 3 // How many times to create/close the manager rapidly\n\tnumServers := 2\n\tnumClients := 5\n\tnumNotifyListeners := 2\n\tnumWaitForReadyCallers := 3\n\tcloseTimeout := 3 * time.Second\n\n\t// Use an atomic counter to simulate test failures\n\tvar failures atomic.Int32\n\n\tfor i := 0; i < numTestRuns; i++ {\n\t\tfmt.Printf(\"\\n--- Test Run %d/%d ---\\n\", i+1, numTestRuns)\n\n\t\t// Declare all loop-scoped variables at the top to avoid 'goto jumps over declaration' errors\n\t\tvar mgr *ConnectionPoolManager\n\t\tcloseDone := make(chan struct{}) // For mgr.Close() completion\n\t\tvar serverWG sync.WaitGroup\n\t\tserverStopCh := make(chan struct{}) // For fake servers\n\t\tserverAddresses := make([]string, numServers)\n\t\tvar listeners []*net.Listener\n\t\tvar clientWG sync.WaitGroup // Shared waitgroup for all client-side goroutines\n\t\tclientStopCh := make(chan struct{}) // Shared stop channel for all client-side goroutines\n\n\t\t// 1. Setup fake servers\n\t\thasServerErrors := false\n\t\tfor s := 0; s < numServers; s++ {\n\t\t\taddr := fmt.Sprintf(\"127.0.0.1:%d\", 8080+s+(i*10)) // Use different ports for different runs\n\t\t\tlistener, err := fakeEndpoint(addr, &serverWG, serverStopCh)\n\t\t\tif err != nil {\n\t\t\t\tfmt.Printf(\"Failed to start fake server %s: %v\\n\", addr, err)\n\t\t\t\tfailures.Add(1)\n\t\t\t\thasServerErrors = true\n\t\t\t\tbreak // Stop trying to start servers for this run\n\t\t\t}\n\t\t\tserverAddresses[s] = addr\n\t\t\tlisteners = append(listeners, listener)\n\t\t}\n\n\t\t// If server setup failed, jump to cleanup directly\n\t\tif hasServerErrors {\n\t\t\tgoto cleanupAndContinue\n\t\t}\n\n\t\t// 2. Create ConnectionPoolManager (only if servers are up)\n\t\tmgr = NewConnectionPoolManager(serverAddresses)\n\n\t\t// 3. Start concurrent clients, notification listeners, and WaitForReady callers\n\t\tfor c := 0; c < numClients; c++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo simulateClient(mgr, serverAddresses, &clientWG, clientStopCh, c)\n\t\t}\n\t\tfor n := 0; n < numNotifyListeners; n++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo notificationListener(mgr, &clientWG, clientStopCh, n)\n\t\t}\n\t\tfor w := 0; w < numWaitForReadyCallers; w++ {\n\t\t\tclientWG.Add(1)\n\t\t\tgo waitForReadyCaller(mgr, &clientWG, clientStopCh, w)\n\t\t}\n\n\t\t// 4. Let the system run for a bit\n\t\tfmt.Println(\"Running simulation for 500ms...\")\n\t\ttime.Sleep(500 * time.Millisecond)\n\n\t\t// 5. Graceful Shutdown\n\t\tfmt.Println(\"Calling mgr.Close()...\")\n\t\tgo func() {\n\t\t\tdefer close(closeDone)\n\t\t\tif err := mgr.Close(); err != nil {\n\t\t\t\tfmt.Printf(\"Manager Close() returned an error: %v\\n\", err)\n\t\t\t\tfailures.Add(1)\n\t\t\t}\n\t\t}()\n\n\t\tselect {\n\t\tcase <-time.After(closeTimeout):\n\t\t\tfmt.Printf(\"ERROR: mgr.Close() did not complete within %v\\n\", closeTimeout)\n\t\t\tfailures.Add(1)\n\t\tcase <-closeDone:\n\t\t\tfmt.Println(\"mgr.Close() completed successfully.\")\n\t\t}\n\n\t\t// Signal all concurrent client goroutines to stop\n\t\tclose(clientStopCh)\n\t\tclientWG.Wait() // Wait for all clients to finish\n\t\tfmt.Println(\"All client-side goroutines stopped.\")\n\n\tcleanupAndContinue:\n\t\t// 6. Cleanup fake servers\n\t\tif len(listeners) > 0 { // Only close serverStopCh if listeners were successfully created\n\t\t\tclose(serverStopCh) // Signal fake servers to stop\n\t\t\tserverWG.Wait()     // Wait for fake servers to finish\n\t\t\tfor _, l := range listeners {\n\t\t\t\tif *l != nil {\n\t\t\t\t\tif err := (*l).Close(); err != nil { // Handle error from Close()\n\t\t\t\t\t\t// It's common for Accept to return net.ErrClosed after listener.Close() is called.\n\t\t\t\t\t\t// Only print actual unexpected errors.\n\t\t\t\t\t\tif !errors.Is(err, net.ErrClosed) {\n\t\t\t\t\t\t\tfmt.Printf(\"Warning: error closing listener on %s: %v\\n\", (*l).Addr().String(), err)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif failures.Load() > 0 {\n\t\tfmt.Printf(\"\\n--- TEST FAILED with %d errors ---\\n\", failures.Load())\n\t} else {\n\t\tfmt.Println(\"\\n--- ALL TESTS PASSED ---\")\n\t}\n}",
            "passed": true,
            "errors": []
        }
    ],
    "passed": true
}